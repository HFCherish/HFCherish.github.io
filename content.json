[{"title":"jave core(advanced topics)","date":"2023-05-12T00:04:51.000Z","path":"2023/05/12/java-core-advanced/","excerpt":"JVM is the heart of Java programming language. When we execute a Java program, JVM is responsible for converting the byte code to the machine-specific code. JVM is also platform-dependent and provides core java functions such as memory management, garbage collection, security, etc. Garbage Collection In Java, a developer does not need to explicitly allocate and deallocate memory – the JVM and more specifically the Garbage Collector – has the duty of handling memory allocation so that the developer doesn’t have to. For most collectors GC related pauses are proportional to size of their heaps","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"java multithread programming","date":"2023-05-09T08:53:26.000Z","path":"2023/05/09/java-thread/","excerpt":"Core Java Tutorial | DigitalOcean Thread Process: A process is a self contained execution environment and it can be seen as a program or application. Thread: lightweight. * a process can have multiple threads running, and at least one main thread. * all thread share parent process code and data * thread creation, context switching, and intercommunication are less expensive * Multithreading: In multi-core system, more than one thread can run at the exactly same time. In single-core system, more than one thread can run parallel using OS feature time slicing to shar","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"react native components & apis","date":"2022-10-22T06:22:52.351Z","path":"2022/10/22/react-native-components-apis/","excerpt":"react 提倡组件化开发，以促进复用。react native 也一样是组件化开发思想。不同的是，react 中是使用原生的 html 组件作为基本组件（div、a…），而 react native 使用的是另一些原生组件。用户的自定义组件也是基于这些原生组件。 所以要用 react native，必须了解这些原生组件（就跟学 html 组件差不多） general props 一些所有组件或大部分组件都有的属性。 style 所有的核心组件都接受名为 style 的属性（类似 html 标签中的 html）。这些样式名基本上是遵循了 web 上的 CSS 的命名，只是按照 JS 的语法要求使用了驼峰命名法，例如将 background-color 改为backgroundColor。 实际开发中组件的样式会越来越复杂，我们建议使用StyleSheet.create来集中定义组件的样式。常见的做法是按顺序声明和使用 style 属性，以借鉴 CSS 中的“层叠”做法（即后声明的属性会覆盖先声明的同名属性）。比如像下面这样： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import React, { Component } from 'react';","tags":[{"name":"react native","slug":"react-native","permalink":"http://hfcherish.github.io/tags/react-native/"},{"name":"react","slug":"react","permalink":"http://hfcherish.github.io/tags/react/"}]},{"title":"java stream parallel 有时比 sequential 还慢？","date":"2022-10-22T06:22:52.338Z","path":"2022/10/22/java-stream-parallel/","excerpt":"为什么 java stream parallel 有时比 sequential 执行还慢？ 场景 考虑下边的代码，并行执行不一定比顺序执行快，甚至很多时候都是更慢的。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @Test public void should_not_sure_if_without_warm_up() { String[] array = new String[1000000]; Arrays.fill(array, \"AbabagalamagA\"); System.out.println(\"Benchmark...\"); for (int i = 0; i < 5; ++i) { System.out.printf(\"Run %d: sequential %s - parallel %s\\n\", i, test(() -> sequential(array)), test(() -> parallel(array))); }","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"fantastic tools","date":"2022-10-22T06:22:52.335Z","path":"2022/10/22/fantastic-tools/","excerpt":"设计 收集一些在设计时可能会用到的比较好的网站 * icons from the fontawesome: 各种各样的图标，很漂亮 * ant design：里边有 UI 设计价值观及图标资源等，还有前端组件库 前端组件库 组件库大合集 组件库大合集2 mac 强迫症的Mac设置指南 terminal [10 Must know terminal commands and tips for productivity](10 Must know terminal commands and tips for productivity) 介绍了一些，简单罗列下： 1. iterm2：是一种 terminal，将其作为默认 terminal，可以方便的分屏等 2. oh my zsh：管理 zsh 配置的，使用 ~/.zshrc，利用 source ~/.zshrc 可以是配置立马生效 1. oh my zsh 插件 3. cat：不打开文件的情况下查看内容 4. imgcat：同上，不过查看的是图片 5. less [filename]：同 cat，不过如果长文件，可以用这个，仅显示一部分 6. pbcopy < [filename]：复制文件内容到 clipboard 7. touch：创建各种各样的文件，可以同时创建","tags":[{"name":"tool","slug":"tool","permalink":"http://hfcherish.github.io/tags/tool/"}]},{"title":"","date":"2021-05-17T02:14:44.000Z","path":"2021/05/17/kerberos/","excerpt":"图解Kerberos协议原理 内网渗透之kerberos协议分析 kerberos 协议 Kerberos Concepts - Principals, Keytabs and Delegation Tokens A user in Kerberos is called a principal, which is made up of three distinct components: the primary, instance, and realm. A Kerberos principal is used in a Kerberos-secured system to represent a unique identity. The first component of the principal is called the primary, or sometimes the user component. The primary component is an arbitrary string and may be the operating system username of the user or the name of a service. The primary component is followed by an optional section ca","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"},{"name":"security","slug":"security","permalink":"http://hfcherish.github.io/tags/security/"}]},{"title":"presto","date":"2021-05-10T03:10:15.000Z","path":"2021/05/10/presto/","excerpt":"背景 Facebook的数据仓库存储在少量大型Hadoop/HDFS集群。Hive是Facebook在几年前专为Hadoop打造的一款数据仓库工具。在以前，Facebook的科学家和分析师一直依靠Hive来做数据分析。但Hive使用MapReduce作为底层计算框架，是专为批处理设计的。但随着数据越来越多，使用Hive进行一个简单的数据查询可能要花费几分到几小时，显然不能满足交互式查询的需求。Facebook也调研了其他比Hive更快的工具，但它们要么在功能有所限制要么就太简单，以至于无法操作Facebook庞大的数据仓库。 2012年开始试用的一些外部项目都不合适，他们决定自己开发，这就是Presto。2012年秋季开始开发，目前该项目已经在超过 1000名Facebook雇员中使用，运行超过30000个查询，每日数据在1PB级别。Facebook称Presto的性能比Hive要好上10倍多。2013年Facebook正式宣布开源Presto。 定位 presto 官网 Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"}]},{"title":"clean mac other storage","date":"2021-02-24T11:43:25.000Z","path":"2021/02/24/clean-mac-other-storage/","excerpt":"other storage 主要是存的系统的一些缓存、日志等数据。有时会占特别大空间，可以按下列步骤清理 1. 暂时关闭 SIP，以能查看和删除系统文件（解决 not permitted 问题） 1. 以 recover mode 重启电脑：启动时，按 command + R 即可 2. 选择 Utilities -> Terminal 3. 在 Terminal 中输入 csrutil disable 关闭 SIP 4. 重启电脑 在完成 clean 后，应该重复 1、2，并在 terminal 中输入 csrutil enable 来启动 SIP 重启电脑后，可以通过 csrutil status 来查看 SIP 服务是否启动（清理完成后应该启动） 2. 按 size 查找大文件 1 2 $ cd / $ sudo du -sh -- *| sort -hr 3. 常见的大文件 ~/Libraray/Caches 和 /Library/Caches ~/Libraray 和 /Library 下的 Caches 和 logs 等都是可以安全删除的。可以查看一下大小，把自己不用的 cache 删掉。 当然也可以查看 Library 下的所有大文件，确认是否可以删除 docker Docker 的 images、volumes 等可能占很大空间，可以查","tags":[]},{"title":"多维数据模型","date":"2020-12-21T08:05:11.000Z","path":"2020/12/21/多维数据模型/","excerpt":"数据仓库建模 数据仓库的多维数据模型 数据仓库的多维数据模型 – 非常好的一系列文章 Kimball 维度建模 维度建模就是时刻考虑如何能够提供简单性，以业务为驱动，以用户理解性和查询性能为目标 kimball维度建模详解 维度建模分为两种表：事实表和维度表 1. 事实表：必然存在的一些数据，像采集的日志文件，订单表，都可以作为事实表 特征：是一堆主键的集合，每个主键对应维度表中的一条记录，客观存在的，根据主题确定出需要使用的数据 1. 维度表：维度就是所分析的数据的一个量，维度表就是以合适的角度来创建的表，分析问题的一个角度：时间、地域、终端、用户等角度 多维数据模型的定义和作用 多维数据模型是为了满足用户从多角度多层次进行数据查询和分析的需要而建立起来的基于事实和维的数据库模型，其基本的应用是为了实现OLAP（Online Analytical Processing）。 当然，通过多维数据模型的数据展示、查询和获取就是其作用的展现，但其真的作用的实现在于，通过数据仓库可以根据不同的数据需求建立起各类多维模型，并组成数据集市开放给不同的用户群体使用，也就是根据需求定制的各类数据商品摆放在数据集市中供不同的数据消费者进行采购。 多维数据模型最大的优点就是其基于分析优化的数据组织和存储模式。 主题建模 多维分析仓库构建-面向主题的建模 构成 主题建模是对","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"design","slug":"design","permalink":"http://hfcherish.github.io/tags/design/"},{"name":"data modeling","slug":"data-modeling","permalink":"http://hfcherish.github.io/tags/data-modeling/"}]},{"title":"airflow","date":"2020-12-18T07:33:14.000Z","path":"2020/12/18/airflow/","excerpt":"install quickstart Airflow is published as apache-airflow package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open and applications usually pin them, but we should do neither and both at the same time. We decided to keep our dependencies as open as possible (in setup.cfg and setup.py) so users can install different version of libraries if needed. This means that from time to time plain pip install apache-airflow will not work or will produce unusable Airflow installation. In ord","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"schedule","slug":"schedule","permalink":"http://hfcherish.github.io/tags/schedule/"}]},{"title":"HDP install (offline using ambari)","date":"2020-11-23T07:36:44.000Z","path":"2020/11/23/ambari-install-offline/","excerpt":"reference 官方安装指导 Preparation 除非说明，默认以下操作都是在所有节点上执行 修改 host 1 2 3 4 5 6 7 [root@master ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.105.137 master 192.168.105.191 slave1 192.168.105.13 slave2 修改 network config 1 2 3 4 5 6 7 8 9 10 11 12 [root@master ~]# vi /etc/sysconfig/network # Created by anaconda NETWORKING=yes HOSTNAME=master [root@master ~]# hostnamectl set-hostname master [root@master ~]# hostname master # ping 各个节点，查看是否可连通 [root@maste","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"}]},{"title":"atlas","date":"2020-11-13T02:04:33.000Z","path":"2020/11/13/atlas/","excerpt":"Architecture Install install steps Access Apache Atlas UI using a browser: http://localhost:21000 You can also access the rest api http://localhost:21000/api/atlas/v2 默认的用户名密码是 (admin, admin) Atlas Features 定义元模型，规范元数据 atlas 可以维护（增删改查） metadata types，支持 * 创建多种类型的 metadata types * businessmetadatadef：业务元数据的元模型 * classificationdef：标签数据的元模型 * entitydef：一般元数据的元模型 * enumdef * relationshipdef：关系元数据的元模型 * structdef * 元模型支持定义属性约束、索引、唯一性等 * 按 id/typename/query 来检索 相关 API 定义 typedef request schema object 1 2 # DELETE/GET/POST/PUT /v2/types/typedef 约束 * type","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"}]},{"title":"数据导入 hive","date":"2020-11-11T03:58:27.000Z","path":"2020/11/11/import-data-to-hive/","excerpt":"ftp .csv 文件导入 可以先将文件弄到 HDFS，然后创建/更新 hive 表来关联到 HDFS 文件。 将文件弄到 HDFS有以下一些方法： 1. ftp -> local -> hdfs: 将文件先下载到本地，再通过 hdfs 命令拷贝到 hdfs 中 2. ftp -> hdfs: 直接连接 FTP，将文件拷到 hdfs 中，省却本地拷贝 3. 已有的数据采集工具：使用实时数据流处理系统，来实现不同系统之间的流通 一、ftp -> local ->hdfs 几种方案： 1. hadoop fs -get ftp://uid:password@server_url/file_path temp_file | hadoop fs -moveFromLocal tmp_file hadoop_path/dest_file 2. 参照这个实现用 python 包从 ftp 中读，然后用 hdfs 命令写到 hdfs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from urllib.request import urlopen","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"}]},{"title":"sqoop","date":"2020-11-10T05:39:30.000Z","path":"2020/11/10/sqoop/","excerpt":"Concept Sqoop: sq are the first two of “sql”, oop are the last three of “hadoop”. It transfers bulk data between hdfs and relational database servers. It supports: * Full Load * Incremental Load * Parallel Import/Export (throught mapper jobs) * Compression * Kerberos Security Integration * Data loading directly to HIVE Sqoop cannot import .csv files into hdfs/hive. It only support databases / mainframe datasets import. Architecture Sqoop provides CLI, thus you can use a simple command to conduct import/export. The import/export are executes in fact through map tasks. When Import f","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"}]},{"title":"MPP (Massively Parallel Processing)","date":"2020-11-10T02:12:56.000Z","path":"2020/11/10/MPP/","excerpt":"Concept 5分钟了解MPP数据库 MPP (Massively Parallel Processing)，即大规模并行处理。简单来说，MPP是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果(与Hadoop相似，但主要针对大规模关系型数据的分析计算)。 MPP架构特征 * 任务并行执行; * 数据分布式存储(本地化); * 分布式计算; * 私有资源; * 横向扩展; * Shared Nothing架构。 MPPDB v.s. Hadoop 知乎-为什么说HADOOP扩展性优于MPP架构的关系型数据库？ hadoop 和 MPPDB 最大的区别在于：对数据管理理念的不同。 1. HDFS/Hadoop 对于数据管理是粗放型管理，以一个文件系统的模式，让用户根据文件夹层级，把文件直接塞到池子里。处理也以批处理为主，就是拼命 scan。如果想在一大堆数据里找符合条件的数据，hadoop 就是粗暴的把所有文件从头到尾 scan 一遍，因为对于这些文件他没有索引、分类等，他管的少，知道的也少，用的时候每次就要全 scan。 2. 数据库的本质在于数据管理，对外提供在线访问、增删改查等一系列操作。数据库的内存管理比较精细，有一套很完善的数据管理和分布体系。如果想在一大堆数据里找符合条件的数据，他可以根据","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"}]},{"title":"data lake","date":"2020-11-09T05:54:11.000Z","path":"2020/11/09/data-lake/","excerpt":"Concept 数据湖 数据湖是： 1. 装有一些便于提取、分析、搜索、挖掘的设备（本身不具备分析能力，是其他分析工具可以方便的在湖上运行，而不需要把湖的数据挪出去再分析） 2. 存放各种数据（格式不统一，原始数据）：结构、半结构、非结构化 3. 来源各种各样，能很方便的导入到数据湖 数据湖就是原始数据保存区. 虽然这个概念国内谈的少，但绝大部分互联网公司都已经有了。国内一般把整个HDFS叫做数据仓库（广义），即存放所有数据的地方，而国外一般叫数据湖（data lake）。把需要的数据导入到数据湖，如果你想结合来自数据湖的信息和客户关系管理系统（CRM）里面的信息，我们就进行连接，只有需要时才执行这番数据结合。 数据湖是多结构数据的系统或存储库，它们以原始格式和模式存储，通常作为对象“blob”或文件存储。数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。数据湖中的数据包括结构化数据（关系数据库数据），半结构化数据（CSV、XML、JSON等），非结构化数据（电子邮件，文档，PDF）和二进制数据（图像、音频、视频），从而形成一个容纳所有形式数据的集中式数据存储。 数据湖从本质上来讲，是一种企业数据架构方法，物理实现上则是一个数据存储平台，用来集中化存储企业内海量的、多来源，","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"}]},{"title":"az data engineer certificate","date":"2020-10-10T09:03:32.000Z","path":"2020/10/10/az-data-engineer-certificate/","excerpt":"learning paths On-premises Env vs Cloud link The term total cost of ownership (TCO) describes the final cost of owning a given technology. In on-premises systems, TCO includes the following costs: * Hardware * Software licensing * Labor (installation, upgrades, maintenance) * Datacenter overhead (power, telecommunications, building, heating and cooling) Cloud systems like Azure track costs by subscriptions. A subscription can be based on usage that’s measured in compute units, hours, or transactions. The cost includes hardware, software, disk storage, and labor. Because of economies of","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"cloud","slug":"cloud","permalink":"http://hfcherish.github.io/tags/cloud/"},{"name":"certificate","slug":"certificate","permalink":"http://hfcherish.github.io/tags/certificate/"}]},{"title":"network","date":"2020-07-23T06:10:25.000Z","path":"2020/07/23/network/","excerpt":"Eli the computer guy Introduction the whole picture Speed & storage unit Physical & logical modem * t1 * dsl: * no faster than 12Mb/s * Asynchronous: download faster than upload * cabel * satellite Router firewall * block the internet to get into your network VPN * enable the internet to get into your network * client-server Switch * Connect everything together TCP/IP ip: internet protocol. How to find a computer. tcp: transmission control protocol. How to communicate between 2 computers suite: tcp protocol, ip protocol, etc tcp windowing components: * i","tags":[{"name":"network","slug":"network","permalink":"http://hfcherish.github.io/tags/network/"}]},{"title":"Cache Memory","date":"2020-06-28T07:29:22.000Z","path":"2020/06/28/Cache-Memory/","excerpt":"General Concept CPU Core Caching Cache Lines Cache Memory Associative Memory Direct-Mapped Memory Set Associative Memory Cache Read/Write Policies cache coherency MESI protocol: (Modified, Exclusive, Shared, Invalid) * Invalid lines are cache lines that are either not present in the cache, or whose contents are known to be stale. For the purposes of caching, these are ignored. Once a cache line is invalidated, it’s as if it wasn’t in the cache in the first place. * Shared lines are clean copies of the contents of main memory. Cache line","tags":[{"name":"cache","slug":"cache","permalink":"http://hfcherish.github.io/tags/cache/"}]},{"title":"Cache - MicroService","date":"2020-06-03T02:42:53.000Z","path":"2020/06/03/Cache-MicroService/","excerpt":"Where is my cache for a service [Architectural Patterns for Caching Microservices](Architectural Patterns for Caching Microservices) Patterns: 1. embedded: save cache in the service 2. client-server: a completely separate cache server 3. reverse-proxy: put the cache in front of each service 4. Sidecar: put the cache as a sidecar container that belongs to the service How does cache work? The application receives the request and checks if the same request was already executed (and stored in the cache) Embedded Embedded Distributed Cache Why distributed? 1. Same requests happen on diff","tags":[{"name":"cache","slug":"cache","permalink":"http://hfcherish.github.io/tags/cache/"}]},{"title":"MSSQL: multiple cascade paths","date":"2020-05-27T03:10:25.000Z","path":"2020/05/27/MSSQL-multiple-cascade-paths/","excerpt":"Symptoms You may receive the following error message when you create a FOREIGN KEY constraint: (microsoft report) 1 Server: Msg 1785, Level 16, State 1, Line 1 Introducing FOREIGN KEY constraint 'fk_two' on table 'table2' may cause cycles or multiple cascade paths. Specify ON DELETE NO ACTION or ON UPDATE NO ACTION, or modify other FOREIGN KEY constraints. Server: Msg 1750, Level 16, State 1, Line 1 Could not create constraint. See previous errors. For example, the table definition is like this: 1 2 3 4 5 6 7 8 Table t1: Id: primaryKey Table t2: Id: primaryKey parent: Fore","tags":[{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"}]},{"title":"NHibernate: inverse, cascade","date":"2020-05-25T03:38:17.000Z","path":"2020/05/25/NHibernate-inverse-cascade/","excerpt":"playing-nhibernate-inverse-and-cascade, nhibernate-inverse bidirectional associations In database, there may be biodirectional relationships, e.g. Parent has multiple child, and Child has a parent. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #### class definition class Parent: - String id - IList childs class Child: - String id - Parent parent #### db definition table Parent: - id table Child: - id - parentId Inverse Inverse focus on the association. It defines which side is responsible of the association maintenance (create, update, delete), that is, the","tags":[{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"}]},{"title":"VPN","date":"2020-05-06T11:14:51.000Z","path":"2020/05/06/VPN/","excerpt":"the great video History Why does we create internet? It’s created from the need of American military, to protect the communication in the wars. The old communication system, phone, connects Lily with Tom through fixed central offices. If some of the central offices are destroyed by nuclear, then the rerouting of the communication line is difficult, that the commnunication will fail. Ta Da.. Internet comes. It communicates through millions of routers. Even half of the routers are destroyed, there may still be the way to communicate. Why does we create VPN (virtual private network)? Interne","tags":[{"name":"network","slug":"network","permalink":"http://hfcherish.github.io/tags/network/"}]},{"title":"nginx 502 vs 504","date":"2020-04-27T05:57:12.000Z","path":"2020/04/27/502-vs-504/","excerpt":"nginx 502 和 504 超时演示 502 Bad Gateway: The server was acting as a gateway or proxy and received an invalid response from the upstream server. 504: he server was acting as a gateway or proxy and did not receive a timely response from the upstream server. Conclusion 504 是 nginx 没有及时从上游服务获取响应，超时了： * 上游服务响应慢，读取 response / 发送 request 超时（upstream timed out (110: Operation timed out) **while** reading response header from upstream） * 某些请求处理就是慢。此时就应该调大 proxy_read_timeout (默认 60s) * 上游服务压力太大，响应变慢。此时可以增加上游服务的响应能力，也可以适当提升 proxy_send_timeout, proxy_read_timeout * 连接上游服务超时。可能是上游服务已经断了，但由于","tags":[]},{"title":"c# contextual keywords: yield","date":"2020-03-10T03:19:31.000Z","path":"2020/03/10/yield/","excerpt":"yield is a contextual keywords. When it shows in a statement, it means the method or get accessor in which it appears is an iterator. Thus it provides a simple way to define an iterator, rather than a class that implements IEnumerable or IEnumerator. When you use the yield contextual keyword in a statement, you indicate that the method, operator, or get accessor in which it appears is an iterator. Using yield to define an iterator removes the need for an explicit extra class (the class that holds the state for an enumeration, see IEnumerator for an example) when you implement the IEnumerable","tags":[{"name":"c#","slug":"c","permalink":"http://hfcherish.github.io/tags/c/"}]},{"title":"FluentValidator","date":"2020-01-17T08:36:43.000Z","path":"2020/01/17/FluentValidator/","excerpt":"FluentValidation Knowledge * The RuleFor method create a validation rule. To specify a validation rule for a particular property, call the RuleFor method, passing a lambda expression that indicates the property that you wish to validate. * Rules are run syncronously By default, all rules in FluentValidation are separate and cannot influence one another. This is intentional and necessary for asynchronous validation to work. * Must, NotNull…. are built-in validators. WithMessage is a method on a validator. When defines condition for validator(s). * Append multiple validators on a same","tags":[{"name":"DTO validator","slug":"DTO-validator","permalink":"http://hfcherish.github.io/tags/DTO-validator/"}]},{"title":"c# basic","date":"2019-06-05T09:11:28.000Z","path":"2019/06/05/c-sharp-basic/","excerpt":".net, asp.net, c# c# is like java language specification; .net is like jdk/javase/javaee asp.net: is like springboot 1. default, as, is 2. sln: solution ——> csproj: c sharp project ——> files .sln vs .csproj Key concepts ref: c# concepts * solution: a complete application, similar to maven project. It contains several c# project like frontend, backend, library to compose a complete application. * project: similar to maven module. It can be a web project, a library, a windows program, etc. * assembly: similar to maven jar. A c# project is corresponding to an assembly. An assembly can b","tags":[{"name":"c#","slug":"c","permalink":"http://hfcherish.github.io/tags/c/"}]},{"title":"rest and hateoas","date":"2019-06-04T07:47:32.000Z","path":"2019/06/04/rest-and-hateoas/","excerpt":"why REST? The World Wide Web is arguably the world’s largest distributed application. Understanding the key architectural principles underlying the Web can help explain its technical success and may lead to improvements in other distributed applications, particularly those that are amenable to the same or similar methods of interaction. REST contributes both the rationale behind the modern Web’s software architecture and a significant lesson in how software engineering principles can be systematically applied in the design and evaluation of a real software system. —- Fielding’s REST dissertati","tags":[{"name":"rest","slug":"rest","permalink":"http://hfcherish.github.io/tags/rest/"},{"name":"design","slug":"design","permalink":"http://hfcherish.github.io/tags/design/"}]},{"title":"python basic","date":"2019-03-26T04:43:42.000Z","path":"2019/03/26/python-basic/","excerpt":"great free learning website with quiz great online editor & debugger Python学习资料/文章/指南整理 Zen of Python By typing import this, you can see the zen of python. Some need more explanation: Explicit is better than implicit: It is best to spell out exactly what your code is doing. This is why adding a numeric string to an integer requires explicit conversion, rather than having it happen behind the scenes, as it does in other languages. Flat is better than nested: Heavily nested structures (lists of lists, of lists, and on and on…) should be avoided. Errors should never pass silently: In general,","tags":[{"name":"python","slug":"python","permalink":"http://hfcherish.github.io/tags/python/"}]},{"title":"k8s general","date":"2019-03-01T07:41:12.000Z","path":"2019/03/01/k8s/","excerpt":"k8s is a platform to manage containerized workloads and services. Concepts kubernetes Objects 1. Kubernetes abstract a desired state of cluster as objects. 2. an object configuration includes: 1. spec: describe the desired state 1. apiVersion: the api version of kubernetes 2. metadata: the name & namespace 3. spec: the desired state definition. 2. status: describe the actual state of the object 3. cluster state (understanding kubernetes objects): 1. what containerized applications are running and where they’re running 2. how many reso","tags":[{"name":"container","slug":"container","permalink":"http://hfcherish.github.io/tags/container/"}]},{"title":"obs","date":"2019-03-01T02:50:15.000Z","path":"2019/03/01/obs/","excerpt":"Huawei Obs is an object storage service on cloud. Concepts Object 1. The real complete file or byte stream to save 2. object name is the unique id in a bucket 1. it’s used as part of url path. The naming restrictions are fit to url path naming restrictions. 3. Access(based on version in fact) 1. Object ACL: 1. general control to object: read object, read/write object ACL, only users in the same account 2. Object policy 1. fine-grained control to object: fine-grained actions(put,delete…) on object, all users 4. multi-versions 1.","tags":[{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"},{"name":"cloud","slug":"cloud","permalink":"http://hfcherish.github.io/tags/cloud/"}]},{"title":"pipeline process: beam","date":"2019-01-30T09:02:49.000Z","path":"2019/01/30/pipeline-process-beam/","excerpt":"What’s beam beam is a open-source, unified model for defining both batched & streaming data-parallel processing pipelines. * open-source (apache v2 license) * to define data-parallel processing pipelines * an unified model to define pipelines. The real processing is run by the underlying runner (eg. spark, apache apex, etc.). all available runners * can process both batched (bounded datasets) & streaming (unbounded datasets) datasets Use it See the wordcount examples, wordcount src Now we define a simple pipeline and run it. Transform, Count are all built-in atom operations to define t","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"distributed computing","slug":"distributed-computing","permalink":"http://hfcherish.github.io/tags/distributed-computing/"}]},{"title":"linux commands","date":"2019-01-23T07:32:13.000Z","path":"2019/01/23/linux-command/","excerpt":"chmod, chown understanding linux file permissions File permissions are defined by permission group and permission type 1. permission group * owner(u) * group(g) * all other users(a) 2. permission type * read (r - 4) * write(w - 2) * execute(x - 1) permission presentation The permission in the command line is displayed as _rwxrwxrwx 1 owner:group * the first character (underscore _ here) is the special permission flag that can vary. * the following three groups of rwx represent permission of owner, group and all other users respectively. If the ow","tags":[{"name":"linux","slug":"linux","permalink":"http://hfcherish.github.io/tags/linux/"}]},{"title":"lombok","date":"2019-01-17T02:24:55.000Z","path":"2019/01/17/lombok/","excerpt":"lombok is a library to help your write java cleaner and more efficiently. It’s plugged into the editor and build tool, which works at compile time. Essentially, it modifies the byte-codes by operating AST (abstract semantic tree) at compile time, which is allowed by javac. This is, in fact, a way to modify java grammar. Usage To use it, 1. install lombok plugin in intellij 2. add package dependency in project (to use its annotations) 1 2 3 4 5 6 org.projectlombok lombok 1.16.18 provided","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"automatic drive","date":"2019-01-15T01:57:10.000Z","path":"2019/01/15/automatic-drive/","excerpt":"reference: * coco: one format for data labelling","tags":[{"name":"automatic drive","slug":"automatic-drive","permalink":"http://hfcherish.github.io/tags/automatic-drive/"}]},{"title":"spark","date":"2019-01-10T02:17:01.000Z","path":"2019/01/10/spark/","excerpt":"Concept spark is a fast and general-purpose cluster computing system like Hadoop Map-reduce. It runs on the clusters. Spark Ecosystem The components of Apache Spark Ecosystem * spark core: cluster computing system. Provide API to write computing functions. * Spark SQL. SQL for data processing, like hive? * MLlib for machine learning. * GraphX for graph processing * Spark Streaming. Spark Core Spark Core is the fundamental unit of the whole Spark project. Its key features are: * It is in charge of essential I/O functionalities. * Provide API to defines and manipulate the RDDs. Sign","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"},{"name":"distributed computing","slug":"distributed-computing","permalink":"http://hfcherish.github.io/tags/distributed-computing/"}]},{"title":"yarn","date":"2019-01-09T09:27:05.000Z","path":"2019/01/09/yarn/","excerpt":"yarn architecture Yarn is used to manage/allocate cluster resource & schedule/moniter jobs. These parts – resource manager – are split up from hadoop framework. Yarn has two main components: * Schedular: manage resources (cpu, memory, network, disk, etc.) and allocate it the applications. * node manager will tell Schedular the node resource info (node status) * application master will ask Schedular for resources. * When partitioning resources among various queues, applications, Schedular supports pluggable policies. For example: * CapacityScheduler allocate resources by tenant req","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"},{"name":"distributed computing","slug":"distributed-computing","permalink":"http://hfcherish.github.io/tags/distributed-computing/"},{"name":"resource manager","slug":"resource-manager","permalink":"http://hfcherish.github.io/tags/resource-manager/"}]},{"title":"hadoop","date":"2019-01-07T06:46:44.000Z","path":"2019/01/07/hadoop/","excerpt":"Hadoop is a framework of distributed storage & computing. * distributed storage: hadoop use HDFS to save large amount of data in cluster. * distributed computing: hadoop use map-reduce framework to conduct fast data analysis (query & writing) over data in HDFS. * resource manager & job schedular: hadoop use yarn to manage/allocate cluster resources (memory, cpu, etc.) and to schedule and moniter job executing. Architecture cluster architecture request processing Fault Tolerance Use rack aware so that your replicas will be saved into different racks, which can solve the rack failure","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"},{"name":"distributed computing","slug":"distributed-computing","permalink":"http://hfcherish.github.io/tags/distributed-computing/"}]},{"title":"hdfs","date":"2019-01-07T06:05:16.000Z","path":"2019/01/07/hdfs/","excerpt":"hdfs architecture HDFS 集群以 master-slave 模型运行。其中有两种节点： * namenode: master node. know where the files are to find in hdfs * datanode: slave node: have the data of the files namenode 参见 namenode and datanode Namenode 管理着文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块 (block) 所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。 每个 file 有多个 block 构成，这些 block 分散的存储在各个 datanode 上（并且根据 replication factor，有冗余副本），而 namenode 知道如何一个 file 有哪些 block (file 的","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"}]},{"title":"hive introduction","date":"2019-01-03T06:50:02.000Z","path":"2019/01/03/hive-introduction/","excerpt":"apache hive 是一个 data warehouse 应用。支持分布式存储的大数据读、写和管理，并且支持使用标准的 SQL 语法查询。Hive is not a database. This is to make use of SQL capabilities by defining a metadata to the files in HDFS. Long story short, it brings the possibility to query the hdfs file. hive 并没有固定的数据存储方式。自带的是 csv（comma-separated value）和 tsv (tab-separated values) connectors，也可以使用 connector for other formats。 database v.s. warehouse 参见 the difference between database and data warehouse database： 存储具体的业务数据，完善支持 concurrent transaction 操作（CRUD）。 database contains highly detailed data as well as a detailed relational views. Tables ar","tags":[{"name":"big data","slug":"big-data","permalink":"http://hfcherish.github.io/tags/big-data/"},{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"},{"name":"hadoop","slug":"hadoop","permalink":"http://hfcherish.github.io/tags/hadoop/"}]},{"title":"responsive-web-design","date":"2018-12-13T09:57:19.000Z","path":"2018/12/13/responsive-web-design/","excerpt":"自适应一般是设定基准值，宽、高、字体大小都指定为基准值的百分比。当基准值改变时，页面元素、宽高也会按比例变化。 自适应宽度 不使用绝对宽度 网页宽度默认等于屏幕宽度。所以大部分时候只要不适用绝对宽度即可实现自适应宽度： 1 2 3 4 body: { width: 100%; // or width: auto; } 如果元素是图片，也可以使用 max-width 属性，参见responsive web design: image 1 2 3 4 img { max-width: 100%; height: auto; } 使用 media 这适用于需要针对不同的屏幕，显示不同的排版。利用 @media 的 css 规则，可实现根据一个或多个基于设备类型、具体特点和环境的媒体查询来应用样式。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* Media query */ @media screen and (min-width: 900px) { article { padding: 1rem 3rem; } } /* Nested media query */ @supports (display: flex) { @media screen and (min-width: 90","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"},{"name":"css","slug":"css","permalink":"http://hfcherish.github.io/tags/css/"}]},{"title":"duplicate rows in postgresql","date":"2018-11-06T06:53:28.000Z","path":"2018/11/06/duplicate-rows-in-postgresql/","excerpt":"参见 Search and destroy duplicate rows in PostgreSQL Find duplicates using group 1 2 3 4 5 6 7 8 9 SELECT firstname, lastname, count(*) FROM people GROUP BY firstname, lastname HAVING count(*) > 1; using partition 1 2 3 4 5 6 7 8 9 SELECT * FROM (SELECT *, count(*) OVER (PARTITION BY firstname, lastname ) AS count FROM people) tableWithCount WHERE tableWithCount.count > 1; Using not strict distinct 利用 not strict distinct DISTINCT ON 找到唯一的那些条，剩余的就是重复的，可以修改或删除 1 2 3 4 5 6 7 8 9 DELETE FROM people WHERE people.id NOT IN (SELECT id FROM ( SELECT","tags":[{"name":"sql","slug":"sql","permalink":"http://hfcherish.github.io/tags/sql/"}]},{"title":"performance for io in java","date":"2018-10-15T04:59:58.000Z","path":"2018/10/15/performance-of-io-in-java/","excerpt":"java.io.ByteArrayOutputStream 这一般在用到字节流是会用到。 java performance tuning guide 这篇文章不建议在 performance-criticted 代码中使用 ByteArrayOutputStream： 1. 同步写入，效率低 ByteArrayOutputStream allows you to write anything to an internal expandable byte array and use that array as a single piece of output afterwards. Default buffer size is 32 bytes, so if you expect to write something longer, provide an explicit buffer size in the ByteArrayOutputStream(int) constructor 注： 1. ByteArrayOutputStream 内部是一个可变长度的 byte[]（通过扩充实现可变）。它有个初始长度（默认 32），可以在 constructor 中指定. 2. ByteArrayOutputStream 是同步写入，比较影响效率 2. toByteA","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"test principles","date":"2018-10-12T05:49:16.000Z","path":"2018/10/12/test-principles/","excerpt":"three reasons why we should not use inheritance in tests 大概意思是： 1. 很多测试里的继承用的不合适。测试也是代码，必须符合继承的原则。 The point of inheritance is to take advantage of polymorphic behavior NOT to reuse code, and people miss that, they see inheritance as a cheap way to add behavior to a class. When I design code I like to think about options. When I inherit, I reduce my options. I am now sub-class of that class and cannot be a sub-class of something else. I have permanently fixed my construction to that of the superclass, and I am at a mercy of the super-class changing APIs. My freedom to change is fixed at co","tags":[{"name":"test","slug":"test","permalink":"http://hfcherish.github.io/tags/test/"}]},{"title":"gradle test performance","date":"2018-10-12T02:41:39.000Z","path":"2018/10/12/gradle-test-performance/","excerpt":"gradle test configurations one sample config ways to improve performance of gradle build common used properties: * jvmArgs: jvm 参数。通常会配置堆栈大小，保证测试对内存的要求。 * '-Xms128m', '-Xmx1024m', '-XX:MaxMetaspaceSize=128m'。-Xms 是初始堆大小，-Xmx 是最大堆大小，-XX:MaxMetaspaceSize 是 class metadata 可占用的最大本地内存（默认是 unlimited）。具体 jvm 参数参考 java doc. * forkEvery: 每个 test process 里跑的 test classes 的最大个数。当次数达到限制后，会自动重启。这定义了一个测试线程什么时候回重启，与并发无关。默认是 0，即无最大限制，就是可以一直跑 * maxParalleForks: 能并发跑的最大 test processes 数目 * systemProperty: 系统属性 * environment：系统环境变量 * include: 具体执行的测试。可以通过这个配置不同的测试级别（单元测试、集成测试、functional 测试……）","tags":[{"name":"test","slug":"test","permalink":"http://hfcherish.github.io/tags/test/"}]},{"title":"python subprocess","date":"2018-09-03T06:06:24.000Z","path":"2018/09/03/python-subprocess/","excerpt":"Synchronous vs multiprocessing vs multithreading vs async Concurrency vs Parralism. asyncio & threading can run multiple I/O operations at the same time. Async runs one block of code at a time while threading just one line of code at a time. With async, we have better control of when the execution is given to other block of code but we have to release the execution ourselves. * IO bound problems: use async if your libraries support it and if not, use threading. * CPU bound problems: use multi-processing. * None above is a problem:you are probably just fine with synchronous code. You may","tags":[{"name":"python","slug":"python","permalink":"http://hfcherish.github.io/tags/python/"}]},{"title":"guice","date":"2018-08-17T08:53:25.000Z","path":"2018/08/17/guice/","excerpt":"TEST with GUICE","tags":[]},{"title":"构建","date":"2018-08-16T03:26:13.000Z","path":"2018/08/16/gradle-build/","excerpt":"Problems? 1. manifest 是干什么用的？ 2. 代码运行时，如何找到 dependency 的包 3. java -jar 时，classpath 指定？ classpath classpath 指定的是 java 类所在的目录（包括当前项目的类、依赖的类等）。应该是当打 jar 包的时候，默认会加上当前目录(.)到 classpath，这样就包含了 jar 内部的类？ Thin jar gradle lean This plugin depends on JavaPlugin and ApplicationPlugin. * for installDist, jars under install/$PROJECT_NAME$/lib/ * for distZip, jars under /lib/ inside package 1 2 3 4 5 6 7 8 9 10 11 plugins { id 'java' // Apply the application plugin to add support for building a CLI application. id 'application' id 'scala' id 'com.github.maiflai.scalatest' ve","tags":[{"name":"gradle","slug":"gradle","permalink":"http://hfcherish.github.io/tags/gradle/"},{"name":"build","slug":"build","permalink":"http://hfcherish.github.io/tags/build/"},{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"java.lang.UnsatisfiedLinkError: no xxx in java.library.path","date":"2018-08-14T02:07:06.000Z","path":"2018/08/14/java-application-using-third-party-lib/","excerpt":"背景 1. 项目需要引入 local 第三方包 2. 该第三方包只有 window/linux license，而开发在 macos 3. 开发时，通过 gradle dependency compile files('path/to/thejar.jar') 来引入包 问题 运行时，报错误 java.lang.UnsatisfiedLinkError: no thejar in java.library.path 原因 引入 .dll 或 .so 失败造成。 solution 1. 把 thejar 加入到 path 中 ————— not work 2. 加入 path，并 loadLibrary ——————— not work 3. should work（配置 .dll 或 .so 路径）： 1. 配置 PATH 2. 或 jar 包启动时，设置 ‘-Djava.library.path’ 有关 PATH, -classpath, java.library.path 的区别，再 google。java 在使用这三个 path 时： 1. PATH：用来寻找 java, javac 等 command 并执行 2. classpath：jvm 在执行时用来寻找 java class。classpath 一般指向 jar","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"mybatis 工作原理","date":"2018-08-10T01:58:24.000Z","path":"2018/08/10/mybatis-工作原理/","excerpt":"几个核心类 参见: * java api * 入门 - 介绍核心使用组件和最佳实践 SqlSessionFactory mybatis 应用以一个 sqlSessionFactory 实例为核心，即一个应用中有一个单例 SqlSessionFactory，所以数据库 session 都从这里获得。 SqlSessionFactory 可以通过 SqlSessionFactoryBuilder 获得，builder 负责从 xml 配置或 java configuration 类获得。xml (或相应的 java configuration 类) 配置了 datasource（数据库连接信息）、mappers 等信息 SqlSessionFactoryBuilder 它主要就是用来获取 SqlSessionFactory，可以从 xml 或 Java Configuration 类加载配置并构建。提供如下几种方式来获取（参见java api）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 从 xml 获取，其中配置了 environment，datasource，mappers SqlSessionFactory build(InputStream inputStream); // 从 xml 获取，但当 xml 配置了多个 env","tags":[{"name":"orm","slug":"orm","permalink":"http://hfcherish.github.io/tags/orm/"}]},{"title":"oauth","date":"2018-07-27T07:24:55.000Z","path":"2018/07/27/oauth/","excerpt":"traditional authentication 传统认证使用 session: 1. client 发送 username、password 给 server 2. server 查数据库，检查信息，是否正确。正确就把用户登录信息(即用户状态)写到 session 里（即服务器内存中），并将 sessionId 返回给 client。 3. client 在请求 api 时，在 cookie 中传递 sessionId。server 端根据 sessionId 获取用户登录信息，如果已认证，返回正常响应；反之，401 这种方式有个缺陷：如果做分布式服务部署，那么需要每个服务器都要同步相同的登录信息，这不是一个好的方式。所以一般 rest 微服务都要求的是 stateless，即 server 端不保存任何用户信息，请求中包含所有需要的信息。 oauth oauth 是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 OAuth允许用户提供一个 令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权","tags":[{"name":"security","slug":"security","permalink":"http://hfcherish.github.io/tags/security/"},{"name":"oauth","slug":"oauth","permalink":"http://hfcherish.github.io/tags/oauth/"}]},{"title":"azure storage","date":"2018-07-27T03:09:53.000Z","path":"2018/07/27/asure-storage/","excerpt":"asure storage 提供四种存储支持（asure storage overview (youtube)）： * blob (binary large object)：二进制数据存储。有两种：page blog（只能新增/删除/向已有数据附加数据，不能修改数据）；block blog（可以更新） * table：存储表格（nosql） * queue：有库，有 rest api * files：好像主要用在文件共享的时候，就是类似于 windows server 上的文件共享（smb(server message block)），实现和使用方式都和 windows server 的文件共享一样。所以需要支持例如按 /servername/filename 等方式来 share 和 使用文件。它是构建于 blob 之上的。 几个概念 Storage Accout storage 的所有存储都必须在一个 storage account 内发生。这有点类似于一个 database。 安全也是在这里实现： 1. key：创建 account 时，就会生成俩 key，primary key 就是你用来登录访问数据的 key。不过这种方式对于有 client 时不太方便，因为可能不能 share key 2. saas token：就是可以登录认证获得 token，然","tags":[{"name":"cloud","slug":"cloud","permalink":"http://hfcherish.github.io/tags/cloud/"},{"name":"azure","slug":"azure","permalink":"http://hfcherish.github.io/tags/azure/"}]},{"title":"mime type","date":"2018-07-26T09:52:01.000Z","path":"2018/07/26/mime-type/","excerpt":"[MIME 类型](MIME 类型) 是用一种标准化的方式来表示文档的性质和格式。浏览器一般通过 MIME 类型（而不是文档扩展名）来确定如何处理文档。因此服务器传输数据时，必须设置正确的 MIME 类型。 通用结构 1 type/subtype 1. 不允许空格 2. 大小写不敏感，一般都是小写 独立类型 type 可以是独立类型，表示文件的分类，可以是如下值： 类型描述典型示例text表明文件是普通文本，理论上是可读的语言text/plain, text/html, text/css, text/javascriptimage表明是某种图像。不包括视频，但是动态图（比如动态gif）也使用image类型image/gif, image/png, image/jpeg, image/bmp, image/webpaudio表明是某种音频文件audio/midi, audio/mpeg, audio/webm, audio/ogg, audio/wavvideo表明是某种视频文件video/webm, video/oggapplication表明是某种二进制数据application/octet-stream, application/pkcs12, application/vnd.mspowerpoint, application/xhtml+xml, appli","tags":[{"name":"rest","slug":"rest","permalink":"http://hfcherish.github.io/tags/rest/"}]},{"title":"code license","date":"2018-07-26T01:47:13.000Z","path":"2018/07/26/code-license/","excerpt":"license 是软件的授权许可。 对于开源软件来说，虽然别人可以用，但是用的时候希望别人遵循一些要求，比如，使用时必须标明原作者是谁、可以做怎样的修改、软件被用作不正规用途原作者是否要负责……这些其实就是一个协议。 对于作者来说，自己为开源代码写符合法律条规的繁冗的 license 太麻烦，所以就可以采用广为流传的开源协议（eg. MIT, CC…），在 license 文件中标明 “Licnse under the MIT license” 快速选择 详细的协议选择可以从 github choose license 项目中选。下边列些常用的（参见 如何为你的代码选择一个开源协议） MIT 协议 宽松但覆盖一般要点。此协议允许别人以任何方式使用你的代码同时署名原作者，但原作者不承担代码使用后的风险，当然也没有技术支持的义务。jQuery和Rails就是MIT协议 apache 协议 作品涉及到专利，可以考虑这个。也比较宽松，但考虑了专利，简单指明了作品归属者对用户专利上的一些授权（我的理解是软件作品中含有专利，但它授权你可以免费使用）。Apache服务器，SVN还有NuGet等是使用的Apache协议。 GPL 对作品的传播和修改有约束的，可以使用这个。GPL（V2或V3）是一种版本自由的协议（可以参照copy right来理解，后者是版本保留，那copyleft便是","tags":[]},{"title":"java api lib for excel","date":"2018-07-25T02:37:45.000Z","path":"2018/07/25/java-api-for-excel/","excerpt":"可选 lib * apache POI：java 中最大众的 ，支持 xls、xlsx，提供接口来创建、读写 excel文件。 * apache openOffice uno： * JExcel app：这个功能强大，什么都可以做，但是可能收费，而且仅基于 windows + installed excel * JExcelAPI：轻量更易用，但似乎仅支持 xls，而且不支持复杂的 range、formular 计算操作。 * javascript 版本的 JExcel * docx4j：其中 xlsx4j 是处理 excel 的，不过看起来功能比较简单。 * microsoft excel VBA：vba 是微软写的操作 office 软件的语言。所以可以利用这个 vba 写代码…… ref doc: * baeldung example for apache POI & JExcelAPI * Mkyong example for JExcelAPI 核心操作支持 libapache poiapache openofficeJExcelAPIread/create excel file✔️可以✔️read/write excel cells1. 获取 sheet，遍历 row，遍历 row cells；可以1. 获取 sheet，可以根据 cell 行列","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"},{"name":"excel","slug":"excel","permalink":"http://hfcherish.github.io/tags/excel/"}]},{"title":"hikariCP configuration","date":"2018-07-19T02:56:58.000Z","path":"2018/07/19/hikariCP-configuration/","excerpt":"hikariCP 是一个轻量级的数据库连接池。引用 数据库连接池性能对比 的说法（我并没有测试过）： 1. 性能方面 hikariCP>druid>tomcat-jdbc>dbcp>c3p0 。hikariCP的高性能得益于最大限度的避免锁竞争。 2. druid功能最为全面，sql拦截等功能，统计数据较为全面，具有良好的扩展性。 3. 综合性能，扩展性等方面，可考虑使用druid或者hikariCP连接池。 4. 可开启prepareStatement缓存，对性能会有大概20%的提升。 在使用 spring jpa 时，默认使用的连接池是 hikariCP，所以最终采用了这个连接池。 使用过程中出现了一些坑，总结一下。 java.sql.SQLTransientConnectionException 仅使用默认配置，在运行所有测试时，会出现如下异常信息： 这是因为默认的连接池数量是 10，而并行运行测试时，连接池数量不够了。通过设置 maximumPoolSize 解决。 1 spring.datasource.hikari.maximum-pool-size:1000 too many connections 在进行上述设置后，启动应用，连接数据库，发现数据库无法连接，报 too many connections。 fixed-size poo","tags":[{"name":"storage","slug":"storage","permalink":"http://hfcherish.github.io/tags/storage/"}]},{"title":"bean validator","date":"2018-07-18T02:30:57.000Z","path":"2018/07/18/bean-validator/","excerpt":"bean validator 主要是验证一个 bean 的各字段是否满足一些约束，例如 @NotNull bean validation 有个规范 jsr 380，里边定义了一堆 api。有很多规范的实现，最常用的是 hibernate validator，jersey 出的 jersey-bean-validation 也是基于 hibernate validator 做的。 bean validator 一般是应用在 web 框架（如 spring、jersey）上，框架在反序列化 rest 请求到 bean 对象时，框架会调用 validator 根据 bean 对象的 annotation 对 bean 进行验证。 这个过程也可以手动进行。可参考 hibernate validator: get started。 引入依赖 1 2 3 4 5 6 7 8 // jsr 380 api compile \"javax.validation:validation-api:2.0.1.Final\" // hibernate vaidator 实现 testCompile \"org.hibernate.validator:hibernate-validator:6.0.10.Final\" // hibernate validator 依赖的 JSR 341 实现 te","tags":[{"name":"rest","slug":"rest","permalink":"http://hfcherish.github.io/tags/rest/"}]},{"title":"java reflection","date":"2018-07-14T14:37:43.000Z","path":"2018/07/14/java-reflection/","excerpt":"泛型 java 泛型存在类型擦除（参见 java 泛型） 1 2 3 4 List l1 = new ArrayList(); List l2 = new ArrayList(); System.out.println(l1.getClass() == l2.getClass()); // return true, 两个都是 List.class 获取运行时泛型类型 类型擦除使得根据类定义获取 runtime 泛型类型是不可能的，一般有几种方法(参见 stackoverflow)： 1. 根据类对象实例获取，可参见 handle java generic types with reflection * eg. Class tClass = (Class) ReflectionUtil.getClass(ReflectionUtil.getParameterizedTypes(this)[0]); 2. 从父类中获取（要求父类有相同的泛型参数） * eg. Class tClass = (Class) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments","tags":[{"name":"java","slug":"java","permalink":"http://hfcherish.github.io/tags/java/"}]},{"title":"在 aws lambda 中应用 jersey","date":"2018-06-27T02:39:55.000Z","path":"2018/06/27/aws-lambda-jersey/","excerpt":"使用 aws serverless java container 实现。 使用 aws cli 创建项目，配置 aws cli 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 利用原型创建项目 $ mvn archetype:generate -DgroupId=my.service -DartifactId=my-service -Dversion=1.0-SNAPSHOT \\ -DarchetypeGroupId=com.amazonaws.serverless.archetypes \\ -DarchetypeArtifactId=aws-serverless-jersey-archetype \\ -DarchetypeVersion=1.1.3 # 安装 aws cli $ pip install awscli # 配置 credentials $ aws configure AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [None]: us-west-","tags":[{"name":"cloud","slug":"cloud","permalink":"http://hfcherish.github.io/tags/cloud/"},{"name":"aws","slug":"aws","permalink":"http://hfcherish.github.io/tags/aws/"}]},{"title":"js global namespace","date":"2018-06-21T04:27:25.000Z","path":"2018/06/21/js-global-namespace/","excerpt":"1. 最基本的js写法是：不管是要调用项目内或项目外的其他文件的方法，都是直接在当前文件中调用，然后在web文件（html）中利用 按依赖顺序引入所有的内部和外部文件。 * 即每个js中声明的变量都是全局变量。js采用{}来定义变量生命周期（或者说namespace），除了被｛｝所包围的其余变量都是全局变量。有个god object即window－－－全局对象，所有的变量、函数都是这个god object的member－－－全局变量。 * 利用 引入所有js，效果类似于将所有文件的内容组装到一个大文件运行。因此声明顺序受依赖关系约束。 * 即browser运行每个html时，它始终是将这个html中的所有script作为一个文件来运行。即browser是个解释执行器，它总是执行一个文件。而不同于后端（例如java 的jre）的编译运行， 2. 前后端的区别： * browser是解释执行器，而js不提供import机制，所以只能人工解决依赖确定关系－－－需要提供一种机制来解决这中依赖确定关系 * 后端执行是通过server来执行的，服务器会下载所需lib存放到服务器容器中，运行是直接从容器拿。而前端执行是通过browser来执行，每次执行js browser都需要下载依赖的各种js文件，然后将这","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"}]},{"title":"js ecosystem","date":"2018-06-21T03:54:38.000Z","path":"2018/06/21/js-ecosystem-md/","excerpt":"我在学习 react，一直在使用 create-react-app 创建项目。create-react-app 其实包括两个核心： * create-react-app：主要提供了 command-line 工具，方便用户创建 react 项目 * react-scripts：这才是核心。它封装了所有开发 react 项目的配置，使得用户可以零配置直接开始开发 react。用户基本不需要更新 create-react-app，因为它总是拉最新的 react-scripts，而 react-scripts 才是简化用户配置的核心。 问题来了，我在写测试的时候发现，react-scripts 默认配置使用的是 jest，而且版本较低（可运行 npm ls jest 查看依赖树，结果如下图所示）。而我需要使用的 data-driven-test 依赖包 jest-each 是 jest 23.0.0 以后的版本才有的。 所以我需要在测试的时候不使用默认安装的 jest， 读了一个 post: 自己写样板，不使用 create-react-app，受它的启发要自己配置 react 项目，那么就有必要了解 js ecosystem 中的一些工程。 webpack 一个开源的前端打包工具，支持用户进行模块化开发（即用户开发很多 module，然后不同的 mudule 之间可通","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"}]},{"title":"js export","date":"2018-06-15T10:17:19.000Z","path":"2018/06/15/js-export/","excerpt":"export 用于从 module 中导出函数、对象、原始值，在其他地方通过 import 使用这些函数、对象、原始值。 有两种导出方式：命名导出（export），默认导出（export default） 1. 命名导出 就是导出 module 中有命名的函数、对象、原始值。相应的，import 时，必须使用相同的命名引入（当然可以使用 import a as b from './module' 来修改名称） 举例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 定义时导出 export const a = 'a', b = 'b'; // 适用于 var, let export function a(){}; export class a{}; // 定义后导出 const a = 'a', b = 'b'; export {a, b as newB}; // 导出其他模块的导出。 // 此时仅会导出 otherModule 的命名导出，所以这里最终导出的还是有命名的 export * from 'otherModule'; export {a, b as newB} from 'otherModule'; // 如果需要导出 otherModule 的默认导出，只能这样写： import defaultExport","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"}]},{"title":"hexo+next 设置","date":"2018-06-14T11:19:44.000Z","path":"2018/06/14/hexo-next/","excerpt":"使用 hexo + github 部署博客 介绍了怎么部署自己的博客，然后就开始无休止的调整主题。 我选定的主题是 next：有目录，也有集成搜索的文档，这是一个 example，参照 第三方集成 集成搜索等功能. next 优化配置可参考 这篇文章 配置文件 我是采用两个配置文件的写法，即在 source/_data/next.yml 中写 next 相关的配置。 code highlight 配置 按照主题设定教程，我设置的是 scheme: Mist。默认的代码 highlight 是用 tomorrow theme，按照 代码高亮设置教程，可以有五种选项。但是很多 code grammar 高亮显示无效，比如 jsx。所以想换一个 highlight 主题。 找了个 code highlight theme 配置教程 ，开始动手。","tags":[{"name":"tool","slug":"tool","permalink":"http://hfcherish.github.io/tags/tool/"}]},{"title":"react setState 的坑","date":"2018-06-05T11:39:46.000Z","path":"2018/06/05/react-set-state/","excerpt":"问题 setState 是更新 state 的 API，进而会引发组件的重新渲染。然而在使用过程中发现有些坑(参见 react 正确使用状态)： 1. setState(obj)一般情况下不会立即更新 state 的值； 2. 同一 cycle 的多次 setState 调用可能会合并（性能考虑） 对于第一点，引用下边的例子： 1 2 3 4 5 function incrementMultiple() { this.setState({count: this.state.count + 1}); this.setState({count: this.state.count + 1}); this.setState({count: this.state.count + 1}); } 代码运行时，虽然是对 state 加了三次，但是每次加操作都是针对初始的 state，所以最终相当于仅加了一次。即上述代码等同于下边的代码： 1 2 3 4 5 6 Object.assign( previousState, {quantity: state.quantity + 1}, {quantity: state.quantity + 1}, ... ) 为什么 从 react 生命周期看 setState 生效时间 要理解其原因，我们要先","tags":[{"name":"react","slug":"react","permalink":"http://hfcherish.github.io/tags/react/"}]},{"title":"javascript 表达式和操作符","date":"2018-05-31T08:09:58.000Z","path":"2018/05/31/javascript-spread-operator/","excerpt":"... 扩展运算符 ...obj 是 js 的扩展运算符，可以将一个可迭代的对象在函数调用的位置展开成为多个参数,或者在数组字面量中展开成多个数组元素。(其他可参见运算符介绍，运算符和表达式清单 reference) eg. 1 2 3 4 5 6 7 8 9 10 // 在数组字面量中展开 // 利用扩展运算符，实现了数组合并 var parts = ['shoulder', 'knees']; var lyrics = ['head', ...parts, 'and', 'toes']; // 在函数调用处展开 function f(x, y, z) { } var args = [0, 1, 2]; f(...args); `` template literals template literals sql template strings","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"}]},{"title":"javascript 解构语法","date":"2018-05-31T03:24:20.000Z","path":"2018/05/31/javascript-destructuring-assignment/","excerpt":"具体可参见 mdn javascript 解构语法. 这里简单总结一下。 解构是做什么的 解构就是一种方便变量赋值的语法，由编译器完成真正的变量赋值 数组解构 * 将数组元素赋值给变量 * 赋值依据是元素顺序 * 指定变量名时，可以提供默认值，以避免 undefined 赋值 * 支持忽略一些元素（添加 , ，但不提供变量名） * 支持rest 数组赋值 eg. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // 基本赋值 var a, b, rest; [a, b] = [10, 20]; console.log(a); // 10 console.log(b); // 20 // 默认值 var a, b; [a=5, b=7] = [1]; console.log(a); // 1 console.log(b); // 7 // 忽略某些元素 function f() { return [1, 2, 3]; } var [a, , b] = f(); console.log(a); // 1 console.log(b); // 3 // rest 赋值 [a, b, ...rest] = [10, 20, 30, 40","tags":[{"name":"javascript","slug":"javascript","permalink":"http://hfcherish.github.io/tags/javascript/"}]},{"title":"redux","date":"2018-05-29T02:46:35.000Z","path":"2018/05/29/redux/","excerpt":"what’s redux redux 中文文档 中的几个关键特性： 1. 状态容器，提供可预测化的状态管理 2. 跨平台，客户端、服务端、原生应用都能用 3. 易于测试 4. 轻量，支持 react 等界面库 其中第一点，讲明了 redux 的主要用途：状态容器 以 react 为例，页面是渲染状态树得到，有静态状态（props）、动态状态（state）。通过在代码中 setState 来修改状态树，状态自上而下传递到各个子组件，最终触发组件树的重新渲染。 使用 redux，我们就将状态的定义和允许的迁移 function 挪出去，放到一个状态容器里，来有效管理组件的所有状态和状态迁移。此时 component 代码中就没有 state、setState 等定义了。 counter without redux 以计数器为例，不使用 redux，即直接在本地定义 state，并通过 setState 修改状态，以实现重现渲染。(源代码) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import React, {Component} from 'react'; export default class Counter extends Componen","tags":[{"name":"react","slug":"react","permalink":"http://hfcherish.github.io/tags/react/"}]},{"title":"使用 hexo + github 部署博客","date":"2018-05-14T11:19:44.000Z","path":"2018/05/14/build-blog-using-hexo/","excerpt":"安装部署 hexo 参考这个知乎文章安装 1. 准备 node、git 2. 安装 hexo-cli 1 $ npm i -g hexo-cli 3. 创建一个网站 1 $ hexo init xxx.github.io 4. 部署服务器 这里选择部署到 git 上。 1. 首先安装 git deployer 2. 然后修改配置文件，选择部署方式为 git 并配置 repo。 3. hexo 部署 4. 访问 xxx.github.io 即可 1 2 3 4 5 6 7 8 9 10 $ npm install hexo-deployer-git --save $ vi _config.yml deploy: type: git repo: branch: [branch] message: [message] $ npm d 注意： 1. 可以选择同时部署到多个服务器。多写几个 ‘deploy’ 配置即可 2. git 用户名、网站用户名（xxx.github.io 中的 xxx）必须相同。因为它相当于使用 github 服务器 设置 theme 在 官方 themes 里挑。我比较喜欢以下几款： 1. 带目录结构的： 2. cactus：英文的，有几种颜色可以选，带目录，可以","tags":[{"name":"tool","slug":"tool","permalink":"http://hfcherish.github.io/tags/tool/"}]}]