<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[react native components & apis]]></title>
    <url>%2F2022%2F10%2F22%2Freact-native-components-apis%2F</url>
    <content type="text"><![CDATA[react 提倡组件化开发，以促进复用。react native 也一样是组件化开发思想。不同的是，react 中是使用原生的 html 组件作为基本组件（div、a…），而 react native 使用的是另一些原生组件。用户的自定义组件也是基于这些原生组件。 所以要用 react native，必须了解这些原生组件（就跟学 html 组件差不多） general props一些所有组件或大部分组件都有的属性。 style所有的核心组件都接受名为 style 的属性（类似 html 标签中的 html）。这些样式名基本上是遵循了 web 上的 CSS 的命名，只是按照 JS 的语法要求使用了驼峰命名法，例如将 background-color 改为backgroundColor。 实际开发中组件的样式会越来越复杂，我们建议使用StyleSheet.create来集中定义组件的样式。常见的做法是按顺序声明和使用 style 属性，以借鉴 CSS 中的“层叠”做法（即后声明的属性会覆盖先声明的同名属性）。比如像下面这样： 123456789101112131415161718192021222324252627282930import React, &#123; Component &#125; from 'react';import &#123; AppRegistry, StyleSheet, Text, View &#125; from 'react-native';export default class LotsOfStyles extends Component &#123; render() &#123; return ( &lt;View&gt; &lt;Text style=&#123;styles.red&#125;&gt;just red&lt;/Text&gt; &lt;Text style=&#123;styles.bigblue&#125;&gt;just bigblue&lt;/Text&gt; &lt;Text style=&#123;[styles.bigblue, styles.red]&#125;&gt;bigblue, then red&lt;/Text&gt; &lt;Text style=&#123;[styles.red, styles.bigblue]&#125;&gt;red, then bigblue&lt;/Text&gt; &lt;/View&gt; ); &#125;&#125;const styles = StyleSheet.create(&#123; bigblue: &#123; color: 'blue', fontWeight: 'bold', fontSize: 30, &#125;, red: &#123; color: 'red', &#125;,&#125;);// 注册应用(registerComponent)后才能正确渲染// 注意：只把应用作为一个整体注册一次，而不是每个组件/模块都注册AppRegistry.registerComponent('LotsOfStyles', () =&gt; LotsOfStyles); 组件宽度高度有两种设定方法： 1. 指定宽高 最简单的给组件设定尺寸的方式就是在样式中指定固定的 width和 height。React Native中的尺寸都是无单位的，表示的是与设备像素密度无关的逻辑像素点。 2. 弹性宽高 在组件样式中使用 flex 可以使其在可利用的空间中动态地扩张或收缩。一般而言我们会使用 flex:1 来指定某个组件扩张以撑满所有剩余的空间。如果有多个并列的子组件使用了 flex:1，则这些子组件会平分父容器中剩余的空间。如果这些并列的子组件的 flex 值不一样，则谁的值更大，谁占据剩余空间的比例就更大（即占据剩余空间的比等于并列组件间 flex 值的比）。 组件能够撑满剩余空间的前提是其父容器的尺寸不为零。如果父容器既没有固定的 width 和 height，也没有设定 flex，则父容器的尺寸为零。其子组件如果使用了 flex，也是无法显示的。 1234567891011121314151617181920import React, &#123; Component &#125; from 'react';import &#123; AppRegistry, View &#125; from 'react-native';export default class FlexDimensionsBasics extends Component &#123; render() &#123; return ( // 试试去掉父View中的`flex: 1`。 // 则父View不再具有尺寸，因此子组件也无法再撑开。 &lt;View style=&#123;&#123;flex: 1&#125;&#125;&gt; // 这里设定的是固定宽高 &lt;View style=&#123;&#123;width: 50, height: 50, backgroundColor: 'powderblue'&#125;&#125; /&gt; // 弹性宽高 &lt;View style=&#123;&#123;flex: 2, backgroundColor: 'skyblue'&#125;&#125; /&gt; &lt;View style=&#123;&#123;flex: 3, backgroundColor: 'steelblue'&#125;&#125; /&gt; &lt;/View&gt; ); &#125;&#125;;AppRegistry.registerComponent('AwesomeProject', () =&gt; FlexDimensionsBasics); flexbox 布局一般用以下三个属性就可以完成布局的要求了，完整的布局样式属性参照 这篇文章（也可以参考图解布局模式）： flexDirection: 定义布局主轴。row（水平轴）、column（default） justifyContent: 定义主轴上元素排列的方式。flex-start、center、flex-end、space-around（元素在主轴上均匀分布）、space-between(元素间距均匀分布) alignItems: 定义子元素在次轴（与主轴垂直）上的排列方式。flex-start、flex-end、center、stretch（要使 stretch 选项生效的话，子元素在次轴方向上不能有固定的尺寸） 核心组件 componentsreact native 提供了很多内置的 components，社区也有很多开发者提供有很多实用的 components（可以直接从 npm 搜 react native，或者查看 awesome react native 里总结的一些列表） 大部分组件，最终都是基于下边这些基本组件写的。components &amp; apis 总结了 basic components、用于 ui 渲染的、list 的、ios specific 的、android specific 的（学习这些相当于是学习 html 基本组件的过程） ViewView 常用作其他组件的容器，来帮助控制布局和样式（类似于 div） Text写文本的 TextInput是一个允许用户输入文本的基础组件 Image放图片的 SrollViewScrollView 是一个通用的可滚动的容器，你可以在其中放入多个组件和视图，而且这些组件并不需要是同类型的。ScrollView 不仅可以垂直滚动，还能水平滚动（通过 horizontal 属性来设置）。 StyleSheet这其实是一个接口 StyleSheet提供了一种类似CSS样式表的抽象 FlatListreact native 有几种用于显示长列表的组件：flatlist、sectionlist、scrollview 等。 FlatList 组件用于显示一个垂直的滚动列表，其中的元素之间结构近似而仅数据不同，且元素个数可以增删。和 ScrollView 不同的是，FlatList 并不立即渲染所有元素，而是优先渲染屏幕上可见的元素。而那些已经渲染好了但移动到了屏幕之外的元素，则会从原生视图结构中移除（以提高性能）. FlatList 组件必须的两个属性是 data 和 renderItem。data 是列表的数据源，而 renderItem 则从数据源中逐个解析数据，然后返回一个设定好格式的组件来渲染。 123456789101112131415161718192021export default class FlatListBasics extends Component &#123; render() &#123; return ( &lt;View style=&#123;styles.container&#125;&gt; &lt;FlatList data=&#123;[ &#123;key: 'Devin'&#125;, &#123;key: 'Jackson'&#125;, &#123;key: 'James'&#125;, &#123;key: 'Joel'&#125;, &#123;key: 'John'&#125;, &#123;key: 'Jillian'&#125;, &#123;key: 'Jimmy'&#125;, &#123;key: 'Julie'&#125;, ]&#125; renderItem=&#123;(&#123;item&#125;) =&gt; &lt;Text style=&#123;styles.item&#125;&gt;&#123;item.key&#125;&lt;/Text&gt;&#125; /&gt; &lt;/View&gt; ); &#125;&#125; SectionList如果要渲染的是一组需要分组的数据，也许还带有分组标签的，那么 SectionList 将是个不错的选择 1234567891011121314render() &#123; return ( &lt;View style=&#123;styles.container&#125;&gt; &lt;SectionList sections=&#123;[ &#123;title: 'D', data: ['Devin']&#125;, &#123;title: 'J', data: ['Jackson', 'James', 'Jillian', 'Jimmy', 'Joel', 'John', 'Julie']&#125;, ]&#125; renderItem=&#123;(&#123;item&#125;) =&gt; &lt;Text style=&#123;styles.item&#125;&gt;&#123;item&#125;&lt;/Text&gt;&#125; renderSectionHeader=&#123;(&#123;section&#125;) =&gt; &lt;Text style=&#123;styles.sectionHeader&#125;&gt;&#123;section.title&#125;&lt;/Text&gt;&#125; /&gt; &lt;/View&gt; ); &#125; API接口其实是仅提供接口功能的简单组件。这些组件可能没有渲染功能。]]></content>
      <tags>
        <tag>react</tag>
        <tag>react native</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java stream parallel 有时比 sequential 还慢？]]></title>
    <url>%2F2022%2F10%2F22%2Fjava-stream-parallel%2F</url>
    <content type="text"><![CDATA[为什么 java stream parallel 有时比 sequential 执行还慢？场景考虑下边的代码，并行执行不一定比顺序执行快，甚至很多时候都是更慢的。 1234567891011121314151617181920212223242526@Test public void should_not_sure_if_without_warm_up() &#123; String[] array = new String[1000000]; Arrays.fill(array, "AbabagalamagA"); System.out.println("Benchmark..."); for (int i = 0; i &lt; 5; ++i) &#123; System.out.printf("Run %d: sequential %s - parallel %s\n", i, test(() -&gt; sequential(array)), test(() -&gt; parallel(array))); &#125; &#125; private static void sequential(String[] array) &#123; Arrays.stream(array).map(String::toLowerCase).collect(Collectors.toList()); &#125; private static void parallel(String[] array) &#123; Arrays.stream(array).parallel().map(String::toLowerCase).collect(Collectors.toList()); &#125; private static String test(Runnable runnable) &#123; long start = System.currentTimeMillis(); runnable.run(); long elapsed = System.currentTimeMillis() - start; return String.format("%4.2fs", elapsed / 1000.0); &#125; 为什么？有几个原因（stackoverflow）： stream 的并行执行比串行执行要做更多的事。并行执行需要拆分程序，使得程序可以并行执行，最后要合并结果。例如，上述并行执行涉及到 new 线程池、分配线程执行特定的 string 操作并加到一个 list、最终合并 list。这个程序本身已经执行很快，此时，这些额外开销比本身执行的时间可能还要长，就影响了它最终带来的性能。 编译器、jvm、GC 等会影响代码执行效率，因此对 java 做这些基准测试很微妙。例如 JIT compiler、GC 等就会很大程度的影响测试结果。 测试很大程度受 JIT compiler 执行的影响 在 JIT compiler 完成之前，可能测试已经跑完了。此时顺序执行和并行执行哪个 JIT compiler 先跑完，可能测试就会跑的更快一些 而且 JIT compiler 什么时候开始跑也不确定。 并且 JIT compiler 会做一些运行时优化，比如有些代码，其输出没有在任何地方被使用，JIT compiler 会直接消除这些代码的执行。这种情况还是非常容易发生的。此时，你这些测试衡量就更微妙了，因为可能最终执行的测试并不是你所写的测试，而是优化之后的。 如果在测试执行之前，加上一些预热，就可以保证程序都已经再编译完成，此时评估的就是同等条件下的程序执行效率了（参见下边的 code）。 GC 会影响执行效率，不同的代码会产生不同的 eliminated objects stream、并行运行等会涉及到很多中间变量的构建、copy 等，比如中间 string、list 等，这时 GC 执行工作量就比较大，会影响最终的测试执行时间，使得测试结果也不可信。 对 java 做这些基准测试，有时结果会比较 confusing，所以建议采用专门的 benchmark 框架来做基准测试，比如 JMH，这框架执行过程中，可以看到很多 java 额外执行的一些操作时间等，就可以更好的观察测试结果了。 1234567891011121314151617181920/*** 更改测试，加上预热，保证 JIT 编译已完成，此时基本是在同等条件下测试，测试结果相对更可信一些*/@Testpublic void should_parallel_faster_if_has_warm_up() &#123; String[] array = new String[1000000]; Arrays.fill(array, "AbabagalamagA"); System.out.println("Warmup..."); for (int i = 0; i &lt; 100; ++i) &#123; sequential(array); parallel(array); &#125; System.out.println("Benchmark..."); for (int i = 0; i &lt; 5; ++i) &#123; System.out.printf("Run %d: sequential %s - parallel %s\n", i, test(() -&gt; sequential(array)), test(() -&gt; parallel(array))); &#125;&#125; 什么是 JIT compilerJIT (just-in-time) compiler 指在运行时执行的编译器。 (1) java 是编译成字节码，然后在运行时解释执行的 c、C++ 等编程语言都是直接编译成机器码，可以在机器上直接执行的。但是不同平台处理器有差异，导致用户可能需要为不同平台写多套程序。 java 就提出了 JVM，将代码一次编译成字节码，然后提供不同的 JVM，JVM 会将字节码解释执行为可运行的机器码。 但是解释执行是一行一行做的，就影响了执行效率。这也是为啥 c++ 等会诟病 java 很慢的原因。 (2) 为了提高解释执行的效率，使用了 JIT compiler 正如上文所说，因为解释执行慢，所以在程序运行起来后，同时会执行 JIT compiler，将字节码编译成可执行代码（相当于二次编译）。这就可以一定程度的加快解释执行的效率。而且 JIT compiler 因为可以获取运行时环境、参数等，所以可以做更多的优化 parallel 慎用？？？DZone: parallel 慎用 说因为 stream 公用线程池，一个 broken thread 会影响所有 healthy 线程的执行，所以要慎用。 简单看了一些，比如这个 stackoverflow，应该是说 stream 提供了方便的形式去写 function、可读性高、promote 大家写出 side-effects-free 的代码，但是 stream 本身还是有很多缺陷的。 公用线程池的测试代码如下： 123456789101112131415161718192021222324252627282930313233343536@Testpublic void should_be_influenced_by_long_tasks() throws InterruptedException &#123; /** Simulating multiple threads in the system * if one of them is executing a long-running task. * Some of the other threads/tasks are waiting */ int MAX = 12; ExecutorService es = Executors.newCachedThreadPool(); // 这个线程执行很慢，但是因为共享线程池，因此会影响其他线程的执行。极端情况，这里是一个 broken tread，其他 healthy thread 都会受影响 es.execute(() -&gt; countPrimes(MAX, 1000)); // 执行结果不确定，因为有上边的长线程。如果注释掉上边线程，下边这个可以很快执行 es.execute(() -&gt; countPrimes(MAX, 0)); es.execute(() -&gt; countPrimes(MAX, 0)); es.execute(() -&gt; countPrimes(MAX, 0)); es.execute(() -&gt; countPrimes(MAX, 0)); es.execute(() -&gt; countPrimes(MAX, 0)); es.shutdown(); es.awaitTermination(60, TimeUnit.SECONDS);&#125;private void countPrimes(int max, int delay) &#123; System.out.println(Thread.currentThread().getId() + ": " + range(1, max).parallel().filter(this::isPrime).peek(i -&gt; &#123; try &#123; sleep(delay); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).count());&#125;private boolean isPrime(long n) &#123; return n &gt; 1 &amp;&amp; rangeClosed(2, (long) sqrt(n)).noneMatch(divisor -&gt; n % divisor == 0);&#125; ForkJoinPool这个文章 介绍了 ForkJoinPool，说是 parallel stream 实现的主要原理和背后手段 stream 的并发执行现在基本上都是采用分治法，先拆分用多线程逐个处理，然后再合并结果。最后的合并操作必须在前边某几个线程执行完之后才做。 而普通的线程池 ThreadPoolExecutor 就是构建一个线程池，并发执行，但是它没办法决定线程执行的父子关系。 ForkJoinPool 就是为了解决上述问题而存在，它可以让子任务并发执行完成之后，才开始执行父任务。除此以外，和 ThreadPoolExecutor 一样，都是用一个无限队列来保存待执行的任务。 ForkJoinPool 采用了一个通用线程池，实现了 工作窃取。工作窃取指某个线程从其他队列里窃取任务来执行。ForkJoinPool 就可以？？？？？ 什么时候用 parallel目前来说，在 java 中： 如果是数据量很大的操作，可以考虑用 parallel 如果有性能问题，再考虑用 parallel 如果确实有多核，再考虑用 如果确实是无 side effect 的函数，才可以考虑用 如果已经有其他并行措施，可以不用 parallel 如果数据操作很慢，慎用（可能 block 其他 thread） 如果数据操作很快，也慎用（可能这个时候用并行的额外开销会超过它所能带来的优势） 这篇文章也对比了并行和串行 stream，然后画了个决策象限图，如下图所示： 跟上边类似，关注下边四个方面： number_of_elements * cost_per_element 比较大。这可以比较好的解决这种状况：每个元素运行很快时，如果数据量大就可以用；如果每个元素运行稍费时些，即使数据量不那么大，也 ok。但是应该要避免过于费时的那些场景，见上边的分析。 source collection 可以很高效的被拆分（这样才方便拆线程处理） 每个元素的函数执行是独立的（这才可以并行处理，即并行首先要求 side effect free） 多核]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fantastic tools]]></title>
    <url>%2F2022%2F10%2F22%2Ffantastic-tools%2F</url>
    <content type="text"><![CDATA[设计收集一些在设计时可能会用到的比较好的网站 icons from the fontawesome: 各种各样的图标，很漂亮 ant design：里边有 UI 设计价值观及图标资源等，还有前端组件库 前端组件库组件库大合集 组件库大合集2 mac强迫症的Mac设置指南 terminal10 Must know terminal commands and tips for productivity 介绍了一些，简单罗列下： iterm2：是一种 terminal，将其作为默认 terminal，可以方便的分屏等 oh my zsh：管理 zsh 配置的，使用 ~/.zshrc，利用 source ~/.zshrc 可以是配置立马生效 oh my zsh 插件 cat：不打开文件的情况下查看内容 imgcat：同上，不过查看的是图片 less [filename]：同 cat，不过如果长文件，可以用这个，仅显示一部分 pbcopy &lt; [filename]：复制文件内容到 clipboard touch：创建各种各样的文件，可以同时创建多个。eg. touch index.html readme.md index.php lsof -i :[port]：查看端口占用 &amp;&amp;：实现 command chaining，即同时写多个命令，依次执行。eg. npm i &amp;&amp; npm start open .：打开当前目录 iterm2 + oh my zsh + theme 配置 download iterm2，解压安装 设置 iterm2 为默认 terminal：iterm2 -&gt; make iterm2 default Term 安装 oh my zsh: sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 配置主题和颜色 主题，我选用默认的 robbyrussell。其他下载主题然后在 ~/.zshrc 中配置 ZSH_THEME 颜色，下载 iterm2 color schemes，然后在 iterm2 perferences -&gt; profiles -&gt; colors -&gt; color presets -&gt; import -&gt; 从前边下载的库中选择自己喜欢的 scheme 配置高亮 brew install zsh-syntax-highlighting 在 ~/.zshrc 中添加：source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh 刷新配置使其生效：source ~/.zshrc 显示快捷键配置 preferences -&gt; keys -&gt; hotKey -&gt; Show/Hide all… -&gt; 设置为 ⌘. 常用的配置： 配置新开重开使用之前的目录：preferences -&gt; profiles -&gt; general -&gt; working directory -&gt; reuse previous session’s directory 配置快速切换 iterm 的快捷键：preferences -&gt; keys -&gt; hot key -&gt; show/hide all windows with a system-wide hotkey 配置窗口透明度和默认启动大小：preferences -&gt; profiles -&gt; window 配置 command line move-by-word, delete-by-word: preferences -&gt; profiles -&gt; keys (stackoverflow) 向左移动 by word (⌥b)，向右移动 by word (⌥f)，删除右边 by word (⌥d) Under Profile Shortcut Keys, click the + sign. Type your key shortcut (option-b, option-f, option-d, option-left, etc.) For Action, choose Send Escape Sequence. Write b, d or f in the input field. 删除左边 by word (⌥⌫) preferences -&gt; profiles -&gt; keys -&gt; left option (⌥) key : Esc+ iterm2 常用快捷键 ⌘d：横分屏 ⌘⇧d：竖分屏 ⌘⌥ + direction：navigate between panes ⌘.：show/hide iterm2 ⌘⇧↩︎: 最大化当前 pane / 回复当前 pane Git git-fitler-repo 常用的 git config 配置 12345678910111213141516171819202122232425# 在 ~/.zshrc 中添加 alias$ vi ~/.zshrcalias g="/usr/bin/git"# 在 .gitconfig 中添加 alias 配置$ vi ~/.gitconfig[user] name = xxx email = xxx@some.com[alias] a = add aa = add . b = branch c = commit cm = commit -m ca = commit --amend d = diff l = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr)%Creset | %C(bold)%an' --abbrev-commit --date=relative o = checkout pl = pull pb = pull --rebase ps = push r = reset st = status reading 我的小书屋 readfree：可以直接推送到 kindle，很方便 不老歌：写日记 宝书网 music 超高无损音乐下载 json json formatter: json、xml 等都支持，可以格式化，有 json tree，统计了节点数 json editor online: 界面简单，有 json tree，统计了 tree 节点数 json viewer: 功能精简，格式化 json，没有 tree yaml to json: yaml 和 json 之间的转化 Editing typora: markdown tool clipy: free tool for pasting Others karabiner: 定义快捷键。可以将 caps 键重新映射，在自定义快捷键时避免冲突 shiftit: 窗口 size &amp; location 定义。用 brew 安装 brew cask install shiftit Eudic: 好用的词典，可以有一个简单悬浮窗口 Alfred: easy used spotlight substitute to search in Mac cheatsheet: 显示当前应用的快捷键。Just hold the ⌘-Key a bit longer to get a list of all active short cuts of the current application. 给图片加水印 美图秀秀：批量添加水印，编辑水印位置、大小、透明度 Photoshop 批量添加水印 免费软件： XnConvert：使用 XnConvert 批量添加水印 imagemagick: 基于命令行的图形处理库（现有的图像处理软件大多都用到此库） 6 个小工具，打造图片批处理工作流 ImageMagick123456789101112131415161718192021222324252627282930# 安装$ brew install imagemagick$ cd /images# 获取图片基本信息$ identify a.jpg# 转换图片格式$ magick a.jpg a.png# 批量为图片添加水印 1 #!/bin/bash 2 3 dir=$1 4 mark=$2 5 6 echo "the images dir to process: $dir" 7 echo "the mark location: $mark" 8 9 shopt -s nullglob # 如果不添加这个，当目录中没有 .png 类型的文件时，他会产生 "$dir"/*.png，那么后边就会报错 10 for each in "$dir"/&#123;*.jpg,*.jpeg,*.png&#125; 11 do 12 echo "each is: $each" 13 convert $each $mark -gravity southeast -geometry +5+20 -composite $each 14 convert $each $mark -gravity center -composite $each 15 convert $each $mark -gravity northwest -geometry +5+20 -composite $each 16 echo "$each: done" 17 done 18 shopt -u nullglob 19 exit 0 Windowsterminal打造 Windows 10 下最强终端方案：WSL + Terminus + Oh My Zsh + The Fuck 注意这里装的是 wsl1，可以改成 wsl2，参见install wsl copymac 下有免费的 clipy 来实现 copy history，windows 下有 copyq（也可以用于 mac） copyq 在它的窗口里可以配置各种命令的快捷键（全局或程序内），包括显示和隐藏 copyq 窗口的 searchmac 下有 alfred 来搜索文件，windows 有一些替代的： listary：有免费版 wox：开源]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clean mac other storage]]></title>
    <url>%2F2021%2F02%2F24%2Fclean-mac-other-storage%2F</url>
    <content type="text"><![CDATA[other storage 主要是存的系统的一些缓存、日志等数据。有时会占特别大空间，可以按下列步骤清理 1. 暂时关闭 SIP，以能查看和删除系统文件（解决 not permitted 问题） 以 recover mode 重启电脑：启动时，按 command + R 即可 选择 Utilities -&gt; Terminal 在 Terminal 中输入 csrutil disable 关闭 SIP 重启电脑 在完成 clean 后，应该重复 1、2，并在 terminal 中输入 csrutil enable 来启动 SIP 重启电脑后，可以通过 csrutil status 来查看 SIP 服务是否启动（清理完成后应该启动） 2. 按 size 查找大文件12$ cd /$ sudo du -sh -- *| sort -hr 3. 常见的大文件~/Libraray/Caches 和 /Library/Caches~/Libraray 和 /Library 下的 Caches 和 logs 等都是可以安全删除的。可以查看一下大小，把自己不用的 cache 删掉。 当然也可以查看 Library 下的所有大文件，确认是否可以删除 dockerDocker 的 images、volumes 等可能占很大空间，可以查看到 ~/Library/Containers/com.docker.docker 文件夹的大小 docker space for mac 12345678910111213141516171819202122$ cd ~/Library/Containers$ sudo du -sh -- *| sort -hr# 查看 docker 的系统占用，这是清理后了，占用很小$ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 1 1 100.8kB 0B (0%)Containers 1 0 0B 0BLocal Volumes 0 0 0B 0BBuild Cache 0 0 0B 0B# 清理磁盘，删除关闭的容器、无用的数据卷和网络，以及 dangling 镜像(即无 tag 的镜像)$ docker system prune# 清理得更加彻底，可以将没有容器使用 Docker 镜像都删掉$ docker system prune -a# 如果需要同时删除未被任何容器引用的数据卷需要显式的指定 --volumns 参数$ docker system prune --all --force --volumes# 删除所有 dangling 数据卷(即无用的 volume)$ docker volume rm $(docker volume ls -qf dangling=true)# 有时删完后，需要一段时间 reclaim space，可以使用以下命令，手动 trigger relamation$ docker run --privileged --pid=host docker/desktop-reclaim-space /Library/Updates这里可能会有很多文件，都可以删。这里正常应该在执行 App Store 里的 Update 时，才会有文件，但不知道为啥，即使 App Store 没有 Update 也会有。 如果这里有大文件，那么： 先尝试去执行 App Store 里的 Update 如果还有文件，可以删除文件夹里的所有文件（建议不要删那几个 .plist 文件），不要删除 /Library/Updates 文件夹本身，删除里边的内容 /private/var/tmp/WiFiDiagnostics*/private/var/tmp/ 里的文件删除的时候要小心些。 WiFiDiagnostics 是 wifi log，这些文件可以安全删除 但它可以用 command+control+option+shift+w 或 command+control+option+shift+&gt; 触发。如果你正好使用 karabiner 并且 remap 了 command+control+option+shift，在使用过程中可能正好和 +w 或 +&gt; 冲突了，那么每次都会启动 logging。所以需要修改配置。 karabiner -&gt; Misc -&gt; Open config folder open karabiner.json，在其中加入下面的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081"rules": [ &#123; "description": "Disabling command+control+option+shift+w. This triggers wifi logging.", "manipulators": [ &#123; "from": &#123; "key_code": "w", "modifiers": &#123; "mandatory": [ "command", "control", "option", "shift" ] &#125; &#125;, "to": [ &#123; "key_code": "escape" &#125; ], "type": "basic" &#125; ] &#125;, &#123; "description": "Disabling command+control+option+shift+&gt;. This triggers wifi logging also.", "manipulators": [ &#123; "from": &#123; "key_code": "&gt;", "modifiers": &#123; "mandatory": [ "command", "control", "option", "shift" ] &#125; &#125;, "to": [ &#123; "key_code": "escape" &#125; ], "type": "basic" &#125; ] &#125;, &#123; "description": "Change caps_lock key to command+control+option+shift. (Post escape key when pressed alone)", "manipulators": [ &#123; "from": &#123; "key_code": "caps_lock", "modifiers": &#123; "optional": [ "any" ] &#125; &#125;, "to": [ &#123; "key_code": "left_shift", "modifiers": [ "left_command", "left_control", "left_option" ] &#125; ], "to_if_alone": [ &#123; "key_code": "escape" &#125; ], "type": "basic" &#125; ] &#125;] /private/var/tmp 文件夹下可能还有很多 sysdiagnose 文件，这个应该是可以删，是系统诊断结果。但不太确定，我没删。 Sysdiagnose 可以通过 command+control+option+shift+. 来启动一次，所以也要注意]]></content>
  </entry>
  <entry>
    <title><![CDATA[多维数据模型]]></title>
    <url>%2F2020%2F12%2F21%2F%E5%A4%9A%E7%BB%B4%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[数据仓库建模 数据仓库的多维数据模型 数据仓库的多维数据模型 – 非常好的一系列文章 Kimball 维度建模维度建模就是时刻考虑如何能够提供简单性，以业务为驱动，以用户理解性和查询性能为目标 kimball维度建模详解 维度建模分为两种表：事实表和维度表 事实表：必然存在的一些数据，像采集的日志文件，订单表，都可以作为事实表 特征：是一堆主键的集合，每个主键对应维度表中的一条记录，客观存在的，根据主题确定出需要使用的数据 维度表：维度就是所分析的数据的一个量，维度表就是以合适的角度来创建的表，分析问题的一个角度：时间、地域、终端、用户等角度 多维数据模型的定义和作用 多维数据模型是为了满足用户从多角度多层次进行数据查询和分析的需要而建立起来的基于事实和维的数据库模型，其基本的应用是为了实现OLAP（Online Analytical Processing）。 当然，通过多维数据模型的数据展示、查询和获取就是其作用的展现，但其真的作用的实现在于，通过数据仓库可以根据不同的数据需求建立起各类多维模型，并组成数据集市开放给不同的用户群体使用，也就是根据需求定制的各类数据商品摆放在数据集市中供不同的数据消费者进行采购。 多维数据模型最大的优点就是其基于分析优化的数据组织和存储模式。 主题建模多维分析仓库构建-面向主题的建模 构成主题建模是对原始数据、原始业务理解的基础上，将数据归类为多个主题（e.g. 销量主题、维修订单主题、线索转化主题…）。 一般，一个主题就是由一张事实表、多张维表、以及结果聚合表所组成。 基于多维数据模型构建底层：事实表+维表 基于上述模型，聚合结果，生成聚合数据。 事实表要尽可能宽，尽可能容纳此主题下所有指标，如果有新指标需求，则动态添加指标。但是事实表太宽可能导致后续计算资源不足，如果需要拆分事实，拆分事实表的过程即拆分子主题。对事实表的拆分不明确，即主题不明确，会导致后续资源的浪费或者维护成本的提高。因为后续可能出现衍生指标需要两个主题出的情况，那么需要再新出一个综合主题。 优势主题建模是对数据的分类，这需要对领域或企业内数据特征有深刻理解。清晰的主题规划往往是数仓设计成败的关键。 与主题建模相对的，是按需输出数据，按照产品的需求出对应指标。 按需出指标，不必等待多维分析数仓建立完成即可开始，前期开发周期短。 主题建模是提前将维度和指标的全集定义好，聚合尽可能多的维度属性和指标的组合，与产品需求解耦，不会随产品需求增加而将数仓变得臃肿，可维护性好，长远来看性能上也更好。 扩展原子指标、衍生指标的增长：需要增加结果表的schema，所有数据库是兼容的，产生结果表的SQL修改其中读取事实表的查询，可以向后兼容 维度属性的增长：在维度表中增加具体的维度属性即可，不需要其他修改。 维度的增长：产生结果表的SQL增加新的维度表，结果表的schema也进行相应修改。 概念在看实例前，这里需要先了解两个概念：事实表和维表。事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；维表则是对事实表中事件的要素的描述信息。比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。这里不再展开了，解释概念真的很麻烦，而且基于我的理解的描述不一定所有人都能明白，还是直接上实例吧： 事实表里面主要包含两方面的信息：维和度量，维的具体描述信息记录在维表，事实表中的维属性只是一个关联到维表的键，并不记录具体信息；度量一般都会记录事件的相应数值，比如这里的产品的销售数量、销售额等。维表中的信息一般是可以分层的，比如时间维的年月日、地域维的省市县等，这类分层的信息就是为了满足事实表中的度量可以在不同的粒度上完成聚合，比如2010年商品的销售额，来自上海市的销售额等。 事实表事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；如系统的日志、销售记录、用户访问日志等信息，事实表的记录是动态的增长的，所以体积是大于维度表。即：用户关心的业务数据，如销售数量，库存数量，销售金额 维表维表则是对事实表中事件的要素的描述信息。比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。 维度表（Dimension Table）也称为查找表（Lookup Table）是与事实表相对应的表，这个表保存了维度的属性值，可以跟事实表做关联，相当于是将事实表中经常重复的数据抽取、规范出来用一张表管理，常见的有日期（日、周、月、季度等属性）、地区表等，所以维度表的变化通常不会太大。即：用来描述业务数据的数据，如日期、产品数据、地区、渠道 基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。 星型模型当所有维表都直接连接到“事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。数据有一定的冗余 雪花模型当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是：通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。 **]]></content>
      <tags>
        <tag>big data</tag>
        <tag>data modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow]]></title>
    <url>%2F2020%2F12%2F18%2Fairflow%2F</url>
    <content type="text"><![CDATA[installquickstart Airflow is published as apache-airflow package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open and applications usually pin them, but we should do neither and both at the same time. We decided to keep our dependencies as open as possible (in setup.cfg and setup.py) so users can install different version of libraries if needed. This means that from time to time plain pip install apache-airflow will not work or will produce unusable Airflow installation. In order to have repeatable installation, however, starting from Airflow 1.10.10 and updated in Airflow 1.10.13 we also keep a set of “known-to-be-working” constraint files in the constraints-master and constraints-1-10 orphan branches. Those “known-to-be-working” constraints are per major/minor python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow version and python versions in the URL. 123pip3 install --use-deprecated legacy-resolver "apache-airflow==1.10.14" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt" pip3 install "apache-airflow==1.10.14" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt" On November 2020, new version of PIP (20.3) has been released with a new, 2020 resolver. This resolver does not yet work with Apache Airflow and might leads to errors in installation - depends on your choice of extras. In order to install Airflow you need to either downgrade pip to version 20.2.4 pip upgrade --pip==20.2.4 or, in case you use Pip 20.3, you need to add option --use-deprecated legacy-resolver to your pip install command. 123456789101112131415161718192021222324252627# airflow needs a home, ~/airflow is the default,# but you can lay foundation somewhere else if you prefer# (optional)export AIRFLOW_HOME=~/airflow# install from pypi using pippip install apache-airflow# initialize the databaseairflow db initairflow users create \ --username admin \ --firstname petrina \ --lastname zheng \ --role Admin \ --email spiderman@superhero.org# start the web server, default port is 8080airflow webserver --port 8080# start the scheduler# open a new terminal or else run webserver with ``-D`` option to run it as a daemonairflow scheduler# visit localhost:8080 in the browser and use the admin account you just# created to login. Enable the example_bash_operator dag in the home page]]></content>
      <tags>
        <tag>schedule</tag>
        <tag>biodata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP install (offline using ambari)]]></title>
    <url>%2F2020%2F11%2F23%2Fambari-install-offline%2F</url>
    <content type="text"><![CDATA[reference 官方安装指导 Preparation除非说明，默认以下操作都是在所有节点上执行 修改 host1234567[root@master ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.105.137 master192.168.105.191 slave1192.168.105.13 slave2 修改 network config123456789101112[root@master ~]# vi /etc/sysconfig/network# Created by anacondaNETWORKING=yesHOSTNAME=master[root@master ~]# hostnamectl set-hostname master[root@master ~]# hostnamemaster# ping 各个节点，查看是否可连通[root@master ~]# ping slave1PING slave1 (192.168.105.191) 56(84) bytes of data. 同步时间 ntp关闭防火墙关闭 Selinux 和 THP修改文件打开最大限制SSH 无密码登录（主节点）Reboot1$ shutdown -r now 制作本地源（离线安装）文件目录访问（http 服务方式）制作本地源（主节点）安装本地源制作相关工具修改文件里面的源地址123456789101112131415161718192021222324252627282930[root@master ambari]# vi ambari/centos7/2.7.4.0-118/ambari.repo#VERSION_NUMBER=2.7.4.0-118[ambari-2.7.4.0]#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.jsonname=ambari Version - ambari-2.7.4.0baseurl=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118gpgcheck=1gpgkey=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[root@master ambari]# cp ambari/centos7/2.7.4.0-118/ambari.repo /etc/yum.repos.d/[root@master ambari]# vi HDP/centos7/3.1.4.0-315/hdp.repo#VERSION_NUMBER=3.1.4.0-315[HDP-3.1.4.0]name=HDP Version - HDP-3.1.4.0baseurl=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315gpgcheck=1gpgkey=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.22]name=HDP-UTILS Version - HDP-UTILS-1.1.0.22baseurl=http://192.168.105.137/ambari/HDP-UTILS/centos7/1.1.0.22gpgcheck=1gpgkey=http://192.168.105.137/ambari/HDP-UTILS/centowwws7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[root@master ambari]# cp HDP/centos7/3.1.4.0-315/hdp.repo /etc/yum.repos.d/ 将创建好的源文件（.repo）拷贝到子节点安装 ambari-server安装 ambari-server先安装，然后开始配置 1$ yum -y install ambari-server 设置并启动 ambari-server（主节点）how to find JAVA_HOME 1$ java -XshowSettings:properties -version 2&gt;&amp;1 &gt; /dev/null | grep 'java.home' 使用默认 postgresqlSetup server 时，有几个交互式配置： 是否自定义用户账户： 选 n。即默认设置了 Ambari GUI 的登录用户为 admin/admin。并且指定 Ambari Server 的运行用户为 root。 If you want to create a different user to run the Ambari Server, or to assign a previously created user, select y at the Customize user account for ambari-server daemon prompt, then provide a user name. JDK： 选 2. 因为默认会安装并使用 oracle jdk，但是（1）不能联网下载（2）好像 oracle jdk 不会下载依赖包。所以自己安装好，在这指定 path 即可 数据库： 按默认配置创建 postgres 数据库 12345678910111213141516171819202122232425262728293031323334353637383940[root@master yum.repos.d]# ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...SELinux status is 'disabled'Customize user account for ambari-server daemon [y/n] (n)? nAdjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Custom JDKEnter choice (1): 2WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /usr/java/jdk1.8.0_202Validating JDK on Ambari Server...done.Completing setup...Configuring database...Enter advanced database configuration [y/n] (n)? nConfiguring database...Default properties detected. Using built-in database.Configuring ambari database...Checking PostgreSQL...Running initdb: This may take up to a minute.Initializing database ... OKAbout to start PostgreSQLConfiguring local database...Configuring PostgreSQL...Restarting PostgreSQLCreating schema and user...done.Creating tables...done.Extracting system views...ambari-admin-2.7.4.0-118.jar...........Adjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully. 使用 mysql离线安装 mysql 离线安装1234567891011121314151617181920212223242526272829# 创建用户组$ groupadd hdp# 创建用户$ mkdir /home/mysql$ useradd -g hdp hive -d /home/mysql/hive$ passwd hiveyourPassword# 创建临时目录$ mkdir /home/mysql/hive/3306/data$ mkdir /home/mysql/hive/3306/log$ mkdir /home/mysql/hive/3306/tmp$ chown -R hive:hdp /home/mysql/hive# 解压$ mv mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz /usr/local$ cd /usr/local$ tar -xzvf mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz# Establish soft links for future upgrades$ ln -s mysql-5.7.27-linux-glibc2.12-x86_64 mysql# Modify users and user groups of all files under mysql folder$ chown -R mysql:mysql mysql/# 创建配置文件$ cd /etc$ vi my.cnf# 安装 mysql$ cd /usr/local/mysql/bin$ ./mysqld --initialize --user=hive 安装 driver, 并配置与 ambari-server 的 jdbc 连接mysql-connector-driver 下载 （选 RedHat 8 那个操作系统） 123456# 解压$ rpm -ivh mysql80-community-release-el7-3.noarch.rpm$ cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar# 配置 driver path$ vi /etc/ambari-server/conf/ambari.properties添加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar 配置 mysql12345678910111213141516171819202122232425262728293031# 设置开机启动，并启动 mysql$ cd /usr/local/mysql# Copy the startup script to the resource directory and modify mysql.server. It's better to modify mysqld as well. These two files are best synchronized.$ cp ./support-files/mysql.server /etc/rc.d/init.d/mysqld# Increase the execution privileges of mysqld service control scripts$ chmod +x /etc/rc.d/init.d/mysqld# Add mysqld service to system service$ chkconfig --add mysqld# Check whether the mysqld service is in effect$ chkconfig --list mysqld # mysql start$ service mysqld start# View mysql status$ service mysqld status# Check mysql related processes$ ps aux|grep mysql# 配置环境变量$ vi /etc/profileexport PATH=$PATH:/usr/local/mysql/bin$ source /etc/profile# 更新 root 密码$ mysql -uroot -p$ mysql&gt; set password for root@localhost=password("root");# 配置 remote access to the mysql$ mysql&gt; use mysql$ mysql&gt; update user set host='%' where user='root';$ mysql&gt; select host,user from user;$ mysql&gt; grant all privileges on *.* to 'root'@'%' identified by 'yourPassword';$ mysql&gt; flush privileges; 创建 database1234567891011121314151617181920212223242526272829CREATE DATABASE ambari; use ambari; CREATE USER &#39;ambari&#39;@&#39;%&#39; IDENTIFIED BY &#39;ambari&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;%&#39;; CREATE USER &#39;ambari&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ambar&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;localhost&#39;; CREATE USER &#39;ambari&#39;@&#39;master&#39; IDENTIFIED BY &#39;ambari&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;master&#39;; FLUSH PRIVILEGES; source &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;Ambari-DDL-MySQL-CREATE.sql CREATE DATABASE hive; use hive; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;%&#39;; CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;localhost&#39;; CREATE USER &#39;hive&#39;@&#39;master&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;master&#39;; FLUSH PRIVILEGES; CREATE DATABASE oozie; use oozie; CREATE USER &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;%&#39;; CREATE USER &#39;oozie&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;localhost&#39;; CREATE USER &#39;oozie&#39;@&#39;master&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;master&#39;; FLUSH PRIVILEGES; Mysql conf上边 cnf 的内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[client] # Client settings, the default connection parameters for the clientport = 3306 # Default connection portsocket = /home/mysql/hive/3306/tmp/mysql.sock # For socket sockets for local connections, the mysqld daemon generates this file[mysqld] # Server Basic Settings# Foundation setupuser = hivebind-address = 0.0.0.0 # Allow any ip host to access this databaseserver-id = 1 # The unique number of Mysql service Each MySQL service Id needs to be uniqueport = 3306 # MySQL listening portbasedir = /usr/local/mysql # MySQL installation root directorydatadir = /home/mysql/hive/3306/data # MySQL Data File Locationtmpdir = /home/mysql/hive/3306/tmp # Temporary directories, such as load data infile, will be usedsocket = /home/mysql/hive/3306/tmp/mysql.sock # Specify a socket file for local communication between MySQL client program and serverpid-file = /home/mysql/hive/3306/log/mysql.pid # The directory where the pid file is locatedskip_name_resolve = 1 # Only use IP address to check the client's login, not the host name.character-set-server = utf8mb4 # Database default character set, mainstream character set support some special emoticons (special emoticons occupy 4 bytes)transaction_isolation = READ-COMMITTED # Transaction isolation level, which is repeatable by default. MySQL is repeatable by default.collation-server = utf8mb4_general_ci # The character set of database corresponds to some sort rules, etc. Be careful to correspond to character-set-server.init_connect='SET NAMES utf8mb4' # Set up the character set when client connects mysql to prevent scramblinglower_case_table_names = 1 # Is it case sensitive to sql statements, 1 means insensitivemax_connections = 400 # maximum connectionmax_connect_errors = 1000 # Maximum number of false connectionsexplicit_defaults_for_timestamp = true # TIMESTAMP allows NULL values if no declaration NOT NULL is displayedmax_allowed_packet = 128M # The size of the SQL packet sent, if there is a BLOB object suggested to be modified to 1Ginteractive_timeout = 1800 # MySQL connection will be forcibly closed after it has been idle for more than a certain period of time (in seconds)wait_timeout = 1800 # The default value of MySQL wait_timeout is 8 hours. The interactive_timeout parameter needs to be configured concurrently to take effect.tmp_table_size = 16M # The maximum value of interior memory temporary table is set to 128M; for example, group by, order by with large amount of data may be used as temporary table; if this value is exceeded, it will be written to disk, and the IO pressure of the system will increase.max_heap_table_size = 128M # Defines the size of memory tables that users can createquery_cache_size = 0 # Disable mysql's cached query result set function; later test to determine whether to turn on or not based on business conditions; in most cases, close the following two itemsquery_cache_type = 0# Memory settings allocated by user processes, and each session will allocate memory size for parameter settingsread_buffer_size = 2M # MySQL read buffer size. Requests for sequential table scans allocate a read buffer for which MySQL allocates a memory buffer.read_rnd_buffer_size = 8M # Random Read Buffer Size of MySQLsort_buffer_size = 8M # Buffer size used for MySQL execution sortbinlog_cache_size = 1M # A transaction produces a log that is recorded in Cache when it is not committed, and persists the log to disk when it needs to be committed. Default binlog_cache_size 32Kback_log = 130 # How many requests can be stored on the stack in a short time before MySQL temporarily stops responding to new requests; the official recommendation is back_log = 50 + (max_connections/5), with a cap of 900# log settinglog_error = /home/mysql/hive/3306/log/error.log # Database Error Log Fileslow_query_log = 1 # Slow Query sql Log Settingslong_query_time = 1 # Slow query time; Slow query over 1 secondslow_query_log_file = /home/mysql/hive/3306/log/slow.log # Slow Query Log Fileslog_queries_not_using_indexes = 1 # Check sql that is not used in the indexlog_throttle_queries_not_using_indexes = 5 # Represents the number of SQL statements per minute that are allowed to be logged to a slow log and are not indexed. The default value is 0, indicating that there is no limit.min_examined_row_limit = 100 # The number of rows retrieved must reach this value before they can be recorded as slow queries. SQL that returns fewer than the rows specified by this parameter is not recorded in the slow query log.expire_logs_days = 5 # MySQL binlog log log file saved expiration time, automatically deleted after expiration# Master-slave replication settingslog-bin = mysql-bin # Open mysql binlog functionbinlog_format = ROW # The way a binlog records content, recording each row being manipulatedbinlog_row_image = minimal # For binlog_format = ROW mode, reduce the content of the log and record only the affected columns# Innodb settingsinnodb_open_files = 500 # Restrict the data of tables Innodb can open. If there are too many tables in the library, add this. This value defaults to 300innodb_buffer_pool_size = 64M # InnoDB uses a buffer pool to store indexes and raw data, usually 60% to 70% of physical storage; the larger the settings here, the less disk I/O you need to access the data in the table.innodb_log_buffer_size = 2M # This parameter determines the size of memory used to write log files in M. Buffers are larger to improve performance, but unexpected failures can result in data loss. MySQL developers recommend settings between 1 and 8Minnodb_flush_method = O_DIRECT # O_DIRECT reduces the conflict between the cache of the operating system level VFS and the buffer cache of Innodb itself.innodb_write_io_threads = 4 # CPU multi-core processing capability settings are adjusted according to read-write ratioinnodb_read_io_threads = 4innodb_lock_wait_timeout = 120 # InnoDB transactions can wait for a locked timeout second before being rolled back. InnoDB automatically detects transaction deadlocks and rolls back transactions in its own lock table. InnoDB notices the lock settings with the LOCK TABLES statement. The default value is 50 seconds.innodb_log_file_size = 32M # This parameter determines the size of the data log file. Larger settings can improve performance, but also increase the time required to recover the failed database. 停止 ambari-server12345[root@master ~] ambari-server stop #停止命令[root@master ~]# # ambari-server reset #重置命令[root@master ~]# # ambari-server setup #重新设置 [root@master ~]# # ambari-server start #启动命令 配置集群基本信息集群名字：ftms_hdp_qat 选择版本： hdp 3.1 redhat7 配置服务选择服务 选择 master/slave for 各服务 配置 hive/ozzie/ranger database使用 postgresql123# 先执行下边这句话，再继续配置$ ambari-server setup --jdbc-db=postgres --jdbc-driver=/usr/share/java/mysql-connector-java.jar 使用 mysql使用 mysql（生产环境推荐使用），且 mysql 在其他地方也在用，而 postgresql 和 mysql 的语法是有区别的。 12# 先执行下边这句话，再继续配置$ ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 配置 directories采用默认的 configurations采用默认的 异常调试查看 ambari 的配置1234567# 查看 ambari-server 的配置$ vi /etc/ambari-server/conf/ambari.properties# 查看 hdp 各服务的配置$ cd /usr/hdp/3.1.4.0-315/hbase/conf# 运行服务$ /usr/hdp/3.1.4.0-315/hbase/bin/hbase shell 查看错误日志12345678910# 查看 ambari-server 启动的错误日志$ tail /var/log/ambari-server/ambari-server.log -n 10 -f | less# 查看 hdp 各服务的日志$ cd /usr/hdp/3.1.4.0-315/hbase/logs/hbase/hbase-hbase-regionserver-slave2.log# 或者在 var 下看也一样，不知道是放了软 link 还是什么，日志好像是一样的$ cd /var/log/hbase/hbase-hbase-regionserver-slave2.log# 查看 ranger service 的配置$ /etc/ranger/ 重新安装123$ ambari-server stop$ ambari-server reset$ ambari-server setup 安装找不到包找不到 hdp.repo在实际安装时，ambari 会生成一个新的 ambari-hdp-1.repo，其中也定义了 hdp 的 baseurl 之类，这里对 hdp 定义的 name 可能是 [HDP-3.1-repo-1] , 而前边准备本地库时，定义的 hdp 的 name 是 [HDP-3.1.4.0]，这两个名字必须保持一致，否则 ambari 就找不到包（这是 ambari 的一个 bug）。 解决方案： 将 hdp.repo 中的 [HDP-3.1.4.0] 改为 [HDP-3.1-repo-1]，并重新 scp 到各个节点 postfix找不到libmysqlclient.so.18 还有一种简单的方法（没试过）： 1&gt;$ yum reinstall mysql-libs -y 123456789101112131415# 先删除现在安装的 postfix$ systemctl disable postfix$ systemctl stop postfix$ yum remove postfix# 给 libmysqlclient.so.18 加上软链$ find / -name '*libmysqlclient.so.18*'/usr/lib64/mysql/libmysqlclient.so.18$ ln -s /usr/lib64/mysql/libmysqlclient.so.18 /usr/lib/libmysqlclient.so.18# 重新安装 postfix 并启动$ yum install postfix$ systemctl enable postfix$ systemctl start postfix$ systemctl status postfix.service 找不到 libtirpc-devel如果出错，可能各个节点都需要做这件事 12345678# 不能联网# 先下载 https://centos.pkgs.org/7/centos-x86_64/libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm.html 包$ yum-config-manager --enable rhui-REGION-rhel-server-optional$ yum install libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm# 如果可以联网。这个可以从 CentOs-Base.repo 里下，但不能联网就没办法了$ cd /etc/yum.repos.d$ cp backup/CentOs-Base.repo . ranger-admin start failstart ranger-admin fail. error detail:This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you might want to use the less safe log_bin_trust_function_creators variable) Solution (stackflow): 12# Execute the following in the MySQL console:SET GLOBAL log_bin_trust_function_creators = 1; atlas server 启动失败Ranger authorization 失败 执行 cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n 命令时，失败报 404 Error detail: 1234567891011121314ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.coprocessor.CoprocessorException: HTTP 404 Error: HTTP 404 at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1261) at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1072) at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$AccessControlService$1.grant(AccessControlProtos.java:10023) at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$AccessControlService.callMethod(AccessControlProtos.java:10187) at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:8135) at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:2426) at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:2408) at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:42010) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:131) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304) 这个原因是 atlas 通过 ranger 访问 hbase 的时候没有权限。可能是由于安装先后顺序的原因，也有 atlas + ranger 本身需要手动配置的原因，导致 ranger 中没有配置 atlas 对 hbase、kafka 的访问权限。因此需要做做几件事： 在 ambari 的 ranger config 中，启动 hbase ranger plugin，并重启相关服务 在 ambari 的 ranger config 中，启动 kafka ranger plugin，并重启相关服务 ===&gt; 暂时没做 在 ranger 添加 hbase 的 service（参照 Configure a Resource-based Service: HBase) 这里 service 的名称必须和 /usr/hdp/3.1.4.0-315/ranger-hbase-plugin/install.properties 里配置 REPOSITORY_NAME 保持一致（但是似乎并不需要？） username，password 说是 The end system username that can be used for connection.（目前来看，我现在是随便写的 admin 的账号） Zookeeper 和 hbase.authentication 的配置是和 /usr/hdp/3.1.4.0-315/hbase/conf/hbase-site.xml 中的配置保持一致 添加 atlas 对 hbase tables、kafka topic 的访问 policies（可参看：tag-based security with atlas + ranger）===》 暂时没做 kafka 创建 hbase 的 policies 时，必须给 all - table, column-family, column 加上 hbase 这个 user，否则可能会遇到 403。这个原因是启动 metadata server 时，会执行 cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n 这么一条命令，执行时，会切换到 hbase 这个用户，如果这里不加权限，这条命令就会执行失败 ranger 的访问链接: http://ranger-server-host:6080/index.html (可以从 ambari ranger config 中看到) 生成 jar 包失败（报 no such file or directory) 在执行 source /usr/hdp/current/atlas-server/conf/atlas-env.sh ; /usr/hdp/current/atlas-server/bin/atlas_start.py 时报错， 123456789101112131415 File "/usr/hdp/current/atlas-server/bin/atlas_start.py", line 163, in returncode = main() File "/usr/hdp/current/atlas-server/bin/atlas_start.py", line 73, in main mc.expandWebApp(atlas_home) File "/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py", line 160, in expandWebApp jar(atlasWarPath) File "/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py", line 213, in jar process = runProcess(commandline) File "/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py", line 249, in runProcess p = subprocess.Popen(commandline, stdout=stdoutFile, stderr=stderrFile, shell=shell) File "/usr/lib64/python2.7/subprocess.py", line 711, in __init__ errread, errwrite) File "/usr/lib64/python2.7/subprocess.py", line 1327, in _execute_child raise child_exceptionOSError: [Errno 2] No such file or directory 通过在 atlas_config.py 中添加 log，发现最后是在执行 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/jar -xf /usr/hdp/3.1.4.0-315/atlas/server/webapp/atlas.war 时报错，找不到的是 jar 命令. 原因是 jar 是 jdk 中的命令，而使用的默认 openjdk 其实只安装了 jre. 12345678# 查看所有的 openjdk 列表$ yum list | grep jdk# 安装 jdk$ yum install java-1.8.0-openjdk.x86_64# copy jar 到指定目录$ cp /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/bin/jar /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/ Host Disk Usage alert安装过程下载了挺多乱七八糟的东西，导致硬盘报警了 12345678910111213141516171819# 查看目前各文件系统的硬盘使用情况，如果设置的 80% 报警，则只要有某个文件系统的使用哪个超了，就会报警$ df -h文件系统 容量 已用 可用 已用% 挂载点devtmpfs 32G 0 32G 0% /devtmpfs 32G 0 32G 0% /dev/shmtmpfs 32G 824M 31G 3% /runtmpfs 32G 0 32G 0% /sys/fs/cgroup/dev/mapper/centos-root 50G 31G 20G 61% //dev/sda1 1014M 154M 861M 16% /boot/dev/mapper/vg_data2-lv_data2 200G 55M 200G 1% /data02/dev/mapper/centos-home 47G 444M 47G 1% /home/dev/mapper/vg_data1-lv_data1 200G 4.9G 196G 3% /data01tmpfs 6.3G 0 6.3G 0% /run/user/0# 查看现在系统中的文件占用情况，找大文件去清$ du -hs /*$ du -hs /root/*$ du -hs /var/log/*$ du -h /var/* -d 1 | sort -n -r 验证各服务可用Hive创建文件夹（可选？？？）hive 验证可用 配置 ranger service &amp; policiesranger-hive-service service 命名 url: get from ambari-hive, or when you connect hive, it will show the whole connect string policy 中 all-database,table,column 加上 hive user 12# 重启 spark 服务Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient hive managed &amp; external tables acid table not supported now （acid new feature in hive，many others does not support it) spark-acids thoughts I figured it out. Just set: mapred.input.dir.recursive and hive.mapred.supports.subdirectories to true. (Hive-site.xml) /warehouse/tablespace/managed/hive/ftms_ods.db/test_user6/delta_0000001_0000001_0000 /warehouse/tablespace/managed/hive/ftms_ods.db/test_user7/part-00000-2a86feec-be9a-413d-a696-8ff115d14075-c000.snappy.orc 包冲突xbean-asm5-shaded-4.4.jar xmlbeans-3.1.0.jar xercesImpl-2.9.1.jar xerces2-xsd11-2.11.1.jar Xmlapis.jar Xml删除 /user/ftms/lib/xercesImpl-2.9.1.jar 和 /user/ftms/lib/xml-apis-1.3.04.jar 1234567891011121314151617181920212223242526272829303132333435363738ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: javax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:311) at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267) at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:120) at org.apache.hadoop.conf.Configuration.asXmlDocument(Configuration.java:3442) at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3417) at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3388) at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3384) at org.apache.hadoop.hive.conf.HiveConf.getConfVarInputStream(HiveConf.java:2410) at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:2703) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:2657) at com.ftms.datapipeline.common.HiveMetaStoreUtil$.hiveConf(HiveMetaStoreUtil.scala:18) at com.ftms.datapipeline.common.HiveMetaStoreUtil$.createHiveMetaStoreClient(HiveMetaStoreUtil.scala:23) at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveMetaStoreClient(HiveMetaStoreUtil.scala:34) at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionCols(HiveMetaStoreUtil.scala:78) at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionColNames(HiveMetaStoreUtil.scala:73) at com.ftms.datapipeline.common.HiveDataSource$.buildInsertSql(HiveDataSource.scala:7) at com.ftms.datapipeline.common.HiveDataSource$.save(HiveDataSource.scala:42) at com.ftms.datapipeline.tasks.dwd.DCompany$.main(DCompany.scala:197) at com.ftms.datapipeline.tasks.dwd.DCompany.main(DCompany.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308) ... 23 moreCaused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Provider org.apache.xerces.jaxp.DocumentBuilderFactoryImpl not found at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:294) at java.security.AccessController.doPrivileged(Native Method) at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289) ... 23 more Spark在 yarn-client 模式下运行 spark会出现 Library directory &#39;...\assembly\target\scala-2.11\jars&#39; does not exist; make sure Spark is built.，这个大致原因是 yarn-client 模式下 spark.yarn.jar和spark.yarn.archive的使用 Running Spark on YARN requires a binary distribution of Spark which is built with YARN support. Binary distributions can be downloaded from the downloads page of the project website. To build Spark yourself, refer to Building Spark. To make Spark runtime jars accessible from YARN side, you can specify spark.yarn.archive or spark.yarn.jars. For details please refer to Spark Properties. If neither spark.yarn.archive nor spark.yarn.jars is specified, Spark will create a zip file with all jars under $SPARK_HOME/jars and upload it to the distributed cache Install ClickHouse12345678910111213141516171819202122232425# 安装 clickhouse$ rpm -ivh clickhouse-server-common-19.4.3.11-1.el6.x86_64.rpm$ rpm -ivh clickhouse-common-static-19.4.3.11-1.el6.x86_64.rpm$ rpm -ivh clickhouse-server-19.4.3.11-1.el6.x86_64.rpm$ rpm -ivh clickhouse-client-19.4.3.11-1.el6.x86_64.rpm# 启动服务$ service clickhouse-server startStart clickhouse-server service: Path to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/# 通过 client 验证运行成功$ clickhouse-clientClickHouse client version 19.4.3.11.Connecting to localhost:9000 as user default.Connected to ClickHouse server version 19.4.3 revision 54416.master :) select 1SELECT 1┌─1─┐│ 1 │└───┘1 rows in set. Elapsed: 0.001 sec. client 启动失败 error detail: 123ClickHouse client version 20.8.3.18.Connecting to localhost:9000 as user default.Code: 102. DB::NetException: Unexpected packet from server localhost:9000 (expected Hello or Exception, got Unknown packet) 这个错表示，clickhouse-client 收到返回了，但是返回的结果是非预期错误。这个错一般是由于端口占用。可以通过 netstat -antp|grep LIST|grep 9000 查询。 Solution：123456# 更新 clickHouse 的端口为 9011$ vi /etc/clickhouse-server/config.xml:%s/9000/9011# 启动时带上端口号$ clickhouse-client --port 9011 安装 python3https://www.python.org/downloads/release/python-361/ reference doc 123456789101112131415# 解压并安装 python3$ tar -xf Python-3.?.?.tar.xz$ cd Python-3.?.?$ ./configure$ make altinstall$ python3.x -V# 创建软链$ which python3.6$ ln -s /usr/local/bin/python3.6.1 /usr/bin/python3$ python3# 安装 pip$ python3 -m ensurepip$ pip3 如果安装 pip 时报错 zlib not found： 12345678# 安装 zlib 相关依赖包$ yum -y install zlib*# 进入 python安装包,修改Module路径的setup文件，Modules/Setup.dist （或者 Modules/Setup） 文件$ vi Module/Setup#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz去掉注释 zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz 安装 airflow为了方便管理，可以安装个 mpack，然后就可以从 ambari 安装、管理、监控 airflow install airflow from ambari git 这个插件在安装/启动时，其实就是执行了 /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py ，脚本中提供了安装、启动、停止。安装时，本质是执行了以下内容： 1$ pip install apache-airflow[all]==1.9.0 apache-airflow[celery]==1.9.0 相关的安装、启动等脚本都在 /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts 目录下 但上述过程只能在有线环境执行，离线环境还是得自己下。 install airflow offlineairflow installation 官方 123456789101112131415# 先下载相关包$ mkdir airflow-install$ cd airflow-install$ pip download 'apache-airflow[all]==1.10.12' \--constraint 'https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.6.txt'# 接下来更新上述 `airflow_scheduler_control.py` 中的安装脚本，选择从本地文件安装即可$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_webserver_control.py# 下边这个命令是上述脚本的内容，这里只是介绍一下执行的命令$ pip install apache-airflow==1.10.12 --no-index -f ./# 过程中可能会有些包缺，例如 docutils、pytest-runner 等，从 https://pypi.org/ 下载相应的 .whl 文件，放到该目录下# 然后通过 --no-index -f ./ 或者 --no-index -f ./xxx.whl 来安装即可$ pip install pytest-runner --no-index -f ./ 实际操作时，下载完 airflow 后，就更新 airflow_scheduler_control.py 和 airflow_webserver_control.py （两个文件的 install method 是一模一样的，按同样的方式修改就行） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647################# 源文件 11 def install(self, env): 12 import params 13 env.set_params(params) 14 print('*' * 30) 15 print(env) 16 print('*' * 30) 17 self.install_packages(env) 18 Logger.info(format("Installing Airflow Service")) 19 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; pip")) 20 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; setuptools")) 21 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython==0.28")) 22 Execute(format("export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore- installed apache-airflow[all]==1.10.0")) 23 Execute(format("export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore- installed apache-airflow[celery]==1.10.0")) 24 Execute(format("chmod 755 /bin/airflow /usr/bin/airflow")) 25 Execute(format("useradd &#123;airflow_user&#125;"), ignore_failures=True) 26 Execute(format("mkdir -p &#123;airflow_home&#125;")) 27 airflow_make_startup_script(env) 28 Execute(format("chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;")) 29 Execute(format("export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb"), 30 user=params.airflow_user 31 )################### 修改后的 11 def install(self, env): 12 import params 13 env.set_params(params) # 这里是去安装 pip，已经装好了，就不装了 14 # self.install_packages(env) 15 Logger.info(format("Installing Airflow Service")) 16 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; pip")) 17 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; setuptools")) # 从指定目录找安装包 --no-index -f ./xxx 18 Execute(format("pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython --no-index -f /install/python_install/airflow-install/")) # 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12，且仅安装 minimal packages 19 Execute(format("export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow==1.10.12 --no-index -f /install/python_install/airflow-install/")) # 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12 20 Execute(format("export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow[celery]==1.10.12 --no-index -f /install/python_install/airflow-install/")) # 目前系统中默认是安装在 /usr/local/bin/airflow 21 Execute(format("chmod 755 /usr/local/bin/airflow")) 22 Execute(format("useradd &#123;airflow_user&#125;"), ignore_failures=True) 23 Execute(format("mkdir -p &#123;airflow_home&#125;")) 24 airflow_make_startup_script(env) 25 Execute(format("chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;")) 26 Execute(format("export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb"), 27 user=params.airflow_user 28 ) ReviewAdmin Name : admin Cluster Name : ftms_hdp_qat Total Hosts : 3 (3 new) Repositories: redhat7 (HDP-3.1): http://10.66.18.11/ambari/HDP/centos7/3.1.4.0-315/ redhat7 (HDP-3.1-GPL): http://10.66.18.11/ambari/HDP-GPL/centos7/3.1.4.0-315/ redhat7 (HDP-UTILS-1.1.0.22): http://10.66.18.11/ambari/HDP-UTILS/centos7/1.1.0.22/ Services: HDFS DataNode : 2 hosts NameNode : master NFSGateway : 0 host SNameNode : slave1 YARN + MapReduce2 Timeline Service V1.5 : slave1 NodeManager : 1 host ResourceManager : master Timeline Service V2.0 Reader : master Registry DNS : master Tez Clients : 1 host Hive Metastore : slave1 HiveServer2 : slave1 Database : Existing MySQL / MariaDB Database HBase Master : master RegionServer : 1 host Phoenix Query Server : 0 host Sqoop Clients : 1 host Oozie Server : master Database : Existing MySQL / MariaDB Database ZooKeeper Server : 3 hosts Infra Solr Infra Solr Instance : master Ambari Metrics Metrics Collector : slave2 Grafana : master Atlas Metadata Server : slave1 Kafka Broker : master Ranger Admin : slave1 Tagsync : 1 host Usersync : slave1 SmartSense Activity Analyzer : master Activity Explorer : master HST Server : master Spark2 Livy for Spark2 Server : 0 host History Server : master Thrift Server : 0 host]]></content>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[atlas]]></title>
    <url>%2F2020%2F11%2F13%2Fatlas%2F</url>
    <content type="text"><![CDATA[Architecture Installinstall steps Access Apache Atlas UI using a browser: http://localhost:21000 You can also access the rest api http://localhost:21000/api/atlas/v2 默认的用户名密码是 (admin, admin) Atlas Features定义元模型，规范元数据atlas 可以维护（增删改查） metadata types，支持 创建多种类型的 metadata types businessmetadatadef：业务元数据的元模型 classificationdef：标签数据的元模型 entitydef：一般元数据的元模型 enumdef relationshipdef：关系元数据的元模型 structdef 元模型支持定义属性约束、索引、唯一性等 按 id/typename/query 来检索 相关 API 定义 typedef request schema object 12# DELETE&#x2F;GET&#x2F;POST&#x2F;PUT&#x2F;v2&#x2F;types&#x2F;typedef 约束 typename 全局唯一 可以维护元数据import metadataatlas 提供以下途径将元数据引入系统： REST API：atlas 提供 api 可以 bulk saveOrUpdate 某个 type 的元数据 文件：atlas 可以上传文件，并 saveOrUpdate 文件中所定义的元模型、元数据等 atlas hook：atlas 通过监听 kafka topic ATLAS_HOOK ，来实时引入数据源中的元数据。目前已提供 Apache Hive/Apache HBase/Apache Storm/Apache Sqoop 的 hook hive hook 可以 import hive databases &amp; tables 元数据 可以监听以下类型的 hive 操作，capture 其中的元数据： create database create table/view, create table as select load, import, export DMLs (insert) alter database alter table (skewed table information, stored as, protection is not supported) alter view sqoop 目前仅支持监听 sqoop 的 hiveimport operation 完成后，capture 其中的元数据。 业务元数据atlas 可以: 定义业务元数据元模型规范业务元数据 在技术元数据上，添加业务元数据，来实现关联 支持按业务元数据检索 血缘atlas 可以查询元数据血缘关系。应该是基于关系图实现。 目前 hive 表可以支撑到 column 级别的血缘分析。 可以维护标签atlas 中的 classification 即标签，可以打在元数据、术语等地方。 可以基于 classification 检索元数据。 可以做 classification 的传播： 基于继承关系链的传播 基于血缘关系链的传播 Classification vs labelAtlas 中的 classification 和 label 都是标签的概念，label 是轻量级、谁都可以加的简单标签，classification 则有更多的支持。 atlas classification vs label 可以维护企业术语表atlas 可以维护企业的术语，并将术语与元数据关联，支持按术语检索元数据，通过以下三个概念来实现细粒度的术语管理： glossary：术语表，最高级别，所有的术语都必须属于某个术语表 category：术语分类，必须挂在某个 glossary。可以拥有 childCategories term：术语。必须挂在某个 glossary。可以挂在某个 category Notificationsatlas 通过 kafka 实现 hook，引入元数据；也通过 kafka 广播元数据修改，供消费者使用。 notifications to atlas：ATLAS_HOOK. 目前已提供 Apache Hive/Apache HBase/Apache Storm/Apache Sqoop 的 hook，来监听这些数据源的元数据 notifications from atlas: ATLAS_ENTITIES 123456789101112# 监听并发布以下事件的通知ENTITY_CREATE: sent when an entity instance is createdENTITY_UPDATE: sent when an entity instance is updatedENTITY_DELETE: sent when an entity instance is deletedCLASSIFICATION_ADD: sent when classifications are added to an entity instanceCLASSIFICATION_UPDATE: sent when classifications of an entity instance are updatedCLASSIFICATION_DELETE: sent when classifications are removed from an entity instance# notification dataAtlasEntity entity;OperationType operationType;List&lt;AtlasClassification&gt; classifications 检索atlas 所有的元数据存储在图数据库 JanusGraph，而索引数据则存储在 index store（solr / elasticsearch）来做全文检索 atlas 支持以下检索方式： 唯一定位元数据：通过 id basic 检索：基于 type、attributes、classifciation、terms 等 query parameter 做全文检索 advance 检索：可以使用 dsl 语言做全文检索 高可用 基础设施高可用。 atlas 使用 JanusGraph 存储元数据，并将 HBase（默认，可采用其他数据库）作为 backing store。HBase 本身的高可用特性支撑了 metadata store 的高可用 atlas 使用 solr / elasticsearch 存储元数据索引。这些组件同样支持高可用 Web Service 高可用。 目前 atlas 的 web service 同一时间只能有一个 active instance 响应，以实现元数据维护、缓存等的一致性问题。高可用模式即有多个备用（passive）instances，当 active instance down 后，可以自动切换某个 passive instance，作为新的 active instance。 访问控制atlas 支持非常细粒度的访问控制： 元模型：基于某个元模型或某类元模型的访问控制。典型 example： Admin users can create/update/delete types of all categories Data stewards can create/update/delete classification types Healthcare data stewards can create/update/delete types having names start with “hc” 元数据：基于元模型、标签、元数据 id 的元数据访问控制。典型 example： Admin users can perform all entity operations on entities of all types Data stewards can perform all entity operations, except delete, on entities of all types Data quality admins can add/update/remove DATA_QUALITY classification Users in specific groups can read/update entities with PII classification or its sub-classification Finance users can read/update entities whose ID start with ‘finance’ admin 操作：可以控制 user/group 来 import/export entities UI有个 ui 可以管理元数据、标签、术语 限制 web service 只有一个 active instance Typename 全局唯一 ui 挺慢的 基于 atlas 我们可以做什么 数据集发现 数据字典浏览和检索 数据集导入和导出 导出数据集，供系统之间交互使用 导入数据集（数据标准、指标口径等） 标签管理： 维护用户画像标签 (增删改查) 基于标签检索数据集 数据标准维护和浏览：（可以通过术语做？） 维护数据标准（增删改查） 可以为数据标准加标签 指标和口径维护和浏览 维护指标口径（增删改查） 可以为指标口径加标签 数据集 staticstics 数据接入和使用情况统计 辅助数据分析师生成分析： 通过关联技术元数据和业务元数据，当数据分析师按业务语言定义分析后，可以快速查找相关的表、字段等]]></content>
      <tags>
        <tag>big data</tag>
        <tag>metadata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据导入 hive]]></title>
    <url>%2F2020%2F11%2F11%2Fimport-data-to-hive%2F</url>
    <content type="text"><![CDATA[ftp .csv 文件导入可以先将文件弄到 HDFS，然后创建/更新 hive 表来关联到 HDFS 文件。 将文件弄到 HDFS有以下一些方法： ftp -&gt; local -&gt; hdfs: 将文件先下载到本地，再通过 hdfs 命令拷贝到 hdfs 中 ftp -&gt; hdfs: 直接连接 FTP，将文件拷到 hdfs 中，省却本地拷贝 已有的数据采集工具：使用实时数据流处理系统，来实现不同系统之间的流通 一、ftp -&gt; local -&gt;hdfs几种方案： hadoop fs -get ftp://uid:password@server_url/file_path temp_file | hadoop fs -moveFromLocal tmp_file hadoop_path/dest_file 参照这个实现用 python 包从 ftp 中读，然后用 hdfs 命令写到 hdfs 1234567891011121314151617from urllib.request import urlopenfrom hdfs import InsecureClient# You can also use KerberosClient or custom clientnamenode_address = 'your namenode address'webhdfs_port = 'your webhdfs port' # default for Hadoop 2: 50070, Hadoop 3: 9870user = 'your user name'client = InsecureClient('http://' + namenode_address + ':' + webhdfs_port, user=user)ftp_address = 'your ftp address'hdfs_path = 'where you want to write'with urlopen(ftp_address) as response: content = response.read() # You can also use append=True # Further reference: https://hdfscli.readthedocs.io/en/latest/api.html#hdfs.client.Client.write with client.write(hdfs_path) as writer: writer.write(content 参考 ftp 提取文件到 hdfs 二、ftp -&gt; hdfs几种方案：(参考 ftp 提取文件到 hdfs) 用 FTP To HDFS 连接 ftp，把文件直接放到 hdfs HDFS dfs -cp: 简单快速，但不显示进度，适用于小文件 1$ hdfs dfs –cp [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path] Hadoop distcp: 分布式提取，快，能显示拷贝进度，不支持流式写入（即拷贝的文件不能有其他程序在写入），适合大量文件或大文件的拷贝 1$ hadoop distcp [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path] 三、已有的数据采集工具文件导入 apache NiFi 来实现不同系统之间的流通，似乎拷贝完，会直接删除 ftp 上的文件 Apache Flume是一个分布式、可靠、高可用的日志收集系统，支持各种各样的数据来源。基于流式数据，适用于日志和事件类型的数据收集，重构后的Flume-NG版本中一个agent（数据传输流程）中的source（源）和sink（目标）之间通过channel进行链接，同一个源可以配置多个channel。多个agent还可以进行链接组合共同完成数据收集任务，使用起来非常灵活。 flume 采集 ftp 文件 上传到 hadoop 使用 spooldir source（不确定是不是能用）, 也可以使用第三方 source 组件 flume-ftp-source Flume 也支持 sql source 的流式导入（使用 flume-ng-sql-source 插件），并提供对数据进行简单处理，并写到各数据接收方的能力。因此它的实时性更好。 DataX：阿里的开源框架，本身社区不太活跃，但有很多 fork 再改的，似乎架构不错 Gobllin: Gobblin是用来整合各种数据源的通用型ETL框架，Gobblin的接口封装和概念抽象做的很好，作为一个ETL框架使用者，我们只需要实现我们自己的Source，Extractor，Conventer类，再加上一些数据源和目的地址之类的配置文件提交给Gobblin就行了。Gobblin相对于其他解决方案具有普遍性、高度可扩展性、可操作性。 kettle：一款开源的ETL工具 其他数据源（非 FTP 文件） Apache Sqoop：RDBMS HDFS Aegisthus：针对 Cassandra 数据源 mongo-hadoop：针对 mongodb 数据源 数据导入需要关注的问题 数据源都有哪些？ 结构化（sql）、半结构化（json, xml…)、非结构化（video、image、file…) 日志数据（csv)、业务数据 是否可以直接连接数据库？ 针对关系型数据，如果可连接数据库，可以通过 sqoop 导入数据到 hive 增量式导入？？ 针对关系型数据，如果不能连接数据库： 是否可以默认周期性导出符合特定标准的 .csv 文件？ 如果数据库导出 dump 文件，再将 dump 文件导入到 hadoop，则比较麻烦，以 oracle 为例，可能需要使用 COPYToBDA 来创建 hive table Query Oracle Dump File on BDA Using Copy2BDA ，或者将 dump 文件先导入到一个 temp oracle 数据库中，再用 sqoop 导入到 hive 如果数据库周期性导出 .csv 文件，将这些 .csv 文件使用上述工具（flume 等）导入到 hive，需要关注增量式导出和导入 增量式导出：文件的组织结构、命名规范 ，.csv 内 record 要求包含 modified date, delete date（在增量式导入时，需要基于这些时间来合并表） 增量式导入：将新增的 .csv 文件作为 hive external table，然后通过中间 view 来合并基表和incremental 表，并更新基表、清空 incremental 表。Incrementally Updating a Table 导入周期和实时性需求 哪些需要每天批量导入、哪些需要流式实时导入 哪些需要全量导入、哪些需要增量式导入？ 如何实现增量式导入？删除的数据是否有删除标识（软删除）？ 如果用 sqoop，参考 sqoop 增量导入，不支持对删除数据的处理 如果用 flume 如果是 sql source，使用 flume-ng-sql-source, 对于 mysql 可以通过 query agent.sources.sqlSource.custom.query 来获取增量 source 如果是文件导入，则需要通过 Incrementally Updating a Table 来合并表 Spark SQL]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop]]></title>
    <url>%2F2020%2F11%2F10%2Fsqoop%2F</url>
    <content type="text"><![CDATA[ConceptSqoop: sq are the first two of “sql”, oop are the last three of “hadoop”. It transfers bulk data between hdfs and relational database servers. It supports: Full Load Incremental Load Parallel Import/Export (throught mapper jobs) Compression Kerberos Security Integration Data loading directly to HIVE Sqoop cannot import .csv files into hdfs/hive. It only support databases / mainframe datasets import. ArchitectureSqoop provides CLI, thus you can use a simple command to conduct import/export. The import/export are executes in fact through map tasks. When Import from DB: it split to some map tasks. And each map task will connect to DB, and fetch some rows/tables, and write it to a file into HDFS When export to DB: it also split to some map tasks. And each map task will fetch a HDFS file, and write each record in the file as a row by specified delimiter to some table. Sqoop v.s. Spark jdbcsqoop uses mapreduce technique, while spark is a revolutionary engine to replace mapreduce technique with its in-memory execution and DAG smartness. Thus Spark jdbc is way faster than sqoop. You can combine all the read, transform and write operations into one script/program instead of reading it separately through SQOOP in one script and then doing transformation and write in another. You can define a new split column on the fly (using functions like ORA_HASH) if you want the data to be partitioned in a proper way. You can control the number of connection to the database. Increasing the number of connection will surely speed up your data import. Common Commands1234567891011$ sqoop import \ --connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt; --table &lt;my_table name&gt; --username &lt;username_for_my_sql&gt; --password &lt;password&gt; --target-dir &lt;target dir in HDFS where data needs to be imported&gt; $ sqoop export \ --connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt; --table &lt;my_table name&gt; --username &lt;username_for_my_sql&gt; --password &lt;password&gt; --export-dir &lt;target dir in HDFS where data needs to be exported&gt; Incremental Import增量导入时，sqoop 需要识别到增量数据，有三种方法： 根据自增字段识别新数据（append 模式）：可以直接识别新数据并导入到 hive 中 根据修改时间识别新数据（lastmodified 模式）：要求这个字段会随数据的改变而改变，但是似乎只能导入到 hdfs 中，不能直接导入到 hive 中。导入时，可以通过制定--merge-key id 来按 id 字段进行合并，或者之后通过 sqoop merge 功能单独运行 根据 where 或 query 识别新数据：可能之后只能通过 sqoop merge 来 merge 数据 目前 sqoop 导入时不能识别删除数据，都需要通过其他方式来解决（对比数据，或者数据上有 delete 标识符时，通过 Incrementally Updating a Table 来实现） append 模式12345678910111213141516171819sqoop import \--connect jdbc:mysql://192.168.33.2:3306/doit_mall \--username root \--password root \--table oms_order \--target-dir /tmp/query \--hive-import \--hive-table doit12.oms_order \--as-textfile \--fields-terminated-by ',' \--compress \--compression-codec gzip \--split-by id \--null-string '\\N' \--null-non-string '\\N' \--incremental append \ # append 模式--check-column id \ # 自增字段--last-value 22 \ # 自增字段的 last value-m 2 lastmodified 模式1234567891011121314151617sqoop import \--connect jdbc:mysql://192.168.33.2:3306/dicts \--username root \--password root \--table doit_stu \--target-dir /doit_stu/2020-02-09 \--as-textfile \--fields-terminated-by ',' \--split-by id \--null-string '\\N' \--null-non-string '\\N' \--incremental lastmodified \ # lastmodified 模式--check-column update_time \ # 时间字段--last-value '2020-02-09 23:59:59' \ # 上一次获取的数据时间--fields-terminated-by ',' \--merge-key id \ #按照id字段进行合并-m 1 条件查询这里写的都是全量导入 hive。如果要增量，只能先导入到 hdfs，然后再做 merge –where123456789101112131415161718sqoop import \--connect jdbc:mysql://192.168.33.2:3306/doit_mall \--username root \--password root \--table oms_order \--hive-import \--hive-table doit12.oms_order \--delete-target-dir \--as-textfile \--fields-terminated-by ',' \--compress \--compression-codec gzip \--split-by id \-m 2 \--null-string '\\N' \--null-non-string '\\N' \--where "delivery_company is null" \ # filter condition--hive-overwrite –query123456789101112131415161718sqoop import \--connect jdbc:mysql://192.168.33.2:3306/doit_mall \--username root \--password root \--target-dir /tmp/query \ # sqoop 导入数据到 hive，本质就是先将数据导入到 hdfs，然后再去 hive 数据库创建元数据。这里需要手动指定中间临时目标目录（不太清楚为啥）--hive-import \--hive-table doit12.oms_order \--delete-target-dir \--as-textfile \--fields-terminated-by ',' \--compress \--compression-codec gzip \--split-by id \--null-string '\\N' \--null-non-string '\\N' \--hive-overwrite \--query "select id,member_id,order_sn,total_amount,pay_amount,status from oms_order where status=4 and \$CONDITIONS" \ # 查询语句，也支持复杂查询-m 2 运行 sqoop action在数据接入时，特别是连接数据库增量导入数据时，这种周期性任务的执行，有很多种方式： 写一个 long running 脚本，不断执行增量 import 采用 Oozie 等调度工具来运行]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPP (Massively Parallel Processing)]]></title>
    <url>%2F2020%2F11%2F10%2FMPP%2F</url>
    <content type="text"><![CDATA[Concept5分钟了解MPP数据库 MPP (Massively Parallel Processing)，即大规模并行处理。简单来说，MPP是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果(与Hadoop相似，但主要针对大规模关系型数据的分析计算)。 MPP架构特征 任务并行执行; 数据分布式存储(本地化); 分布式计算; 私有资源; 横向扩展; Shared Nothing架构。 MPPDB v.s. Hadoop知乎-为什么说HADOOP扩展性优于MPP架构的关系型数据库？ hadoop 和 MPPDB 最大的区别在于：对数据管理理念的不同。 HDFS/Hadoop 对于数据管理是粗放型管理，以一个文件系统的模式，让用户根据文件夹层级，把文件直接塞到池子里。处理也以批处理为主，就是拼命 scan。如果想在一大堆数据里找符合条件的数据，hadoop 就是粗暴的把所有文件从头到尾 scan 一遍，因为对于这些文件他没有索引、分类等，他管的少，知道的也少，用的时候每次就要全 scan。 数据库的本质在于数据管理，对外提供在线访问、增删改查等一系列操作。数据库的内存管理比较精细，有一套很完善的数据管理和分布体系。如果想在一大堆数据里找符合条件的数据，他可以根据分区信息先落到某个机器，再根据多维分区落到某个文件，再在文件里通过索引数据页的树形结构查询，可以直接定位到记录。 因为这样的基本理念不同，使得 hadoop 的扩展只需要简单的增加机器，内部平衡和迁移 data block；而数据库的扩充则涉及到数据拓扑结构的变更、或者不同机器间数据的迁移，当变化迁移的时候，依然需要维护分区、索引等，这种复杂度就比粗放的 HDFS 要高很多了。目前两者的存储模型不同。hadoop 用的是 HDFS，而 MPP 需要自己切分扩展。HDFS 扩展是通过元数据做的，name node 存元数据，增加一个节点，修改元数据就行，所以 HDFS 的扩展能力受到管理元数据的机器（name node） 的性能限制，一般来说可以到 10k 的规模。但 MPP 采用没有中心节点的存储模型，比如 hash，每次增加节点，都需要 rehash，规模增加到几百台的时候，扩展能力就有下降下来了。 通常来讲，MPP数据库有对SQL的完整兼容和一些事务的处理能力，Hadoop在处理非结构化和半结构化数据上具备优势，所以MPP适合多维度数据自助分析、数据集市等；Hadoop适合海量数据存储查询、批量数据ETL、非结构化数据分析(日志分析、文本分析)等海量数据批处理应用要求。但通过 sql 还是 map-reduce 来查，其实只是一种查询形式。目前也有很多 sql on hadoop 的方案，例如 impala （sql on hadoop，其实是一个 MPP engine，所以它的查询性能会更好，提供更低的延迟和更少的处理时间）、spark Sql 等。现在Spark的重点都在Spark SQL，因为它已经不仅仅是SQL了，而是新的 “spark core”。（详见最后链接中Reynold Xin对此的解释） Spark SQL is not just about SQL. It turns out the primitives required for general data processing (eg ETL) are not that different from the relational operators, and that is what Spark SQL is. Spark SQL is the new Spark core with the Catalyst optimizer and the Tungsten execution engine, which powers the DataFrame, Dataset, and last but not least SQL. 常用的MPP数据库有哪些 自我管理的数据仓库 HPE vertica MemSql Teradata 按需 MPP 数据库 aws redshift azure sql 数据仓库 google bigQuery GreenPlum Sybase IQ TD Aster Data Share Nothing数据库架构设计的三种模式：share nothing , share everythong , share disk 数据库构架设计中主要有Shared Everthting、Shared Nothing、和Shared Disk： Shared Everthting:一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer Shared Disk：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。 Shared Nothing：各个处理单元都有自己私有的CPU/内存/硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表DB2 DPF和Hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。 GreenPlumgreenplum]]></content>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data lake]]></title>
    <url>%2F2020%2F11%2F09%2Fdata-lake%2F</url>
    <content type="text"><![CDATA[Concept数据湖 数据湖是： 装有一些便于提取、分析、搜索、挖掘的设备（本身不具备分析能力，是其他分析工具可以方便的在湖上运行，而不需要把湖的数据挪出去再分析） 存放各种数据（格式不统一，原始数据）：结构、半结构、非结构化 来源各种各样，能很方便的导入到数据湖 数据湖就是原始数据保存区. 虽然这个概念国内谈的少，但绝大部分互联网公司都已经有了。国内一般把整个HDFS叫做数据仓库（广义），即存放所有数据的地方，而国外一般叫数据湖（data lake）。把需要的数据导入到数据湖，如果你想结合来自数据湖的信息和客户关系管理系统（CRM）里面的信息，我们就进行连接，只有需要时才执行这番数据结合。 数据湖是多结构数据的系统或存储库，它们以原始格式和模式存储，通常作为对象“blob”或文件存储。数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。数据湖中的数据包括结构化数据（关系数据库数据），半结构化数据（CSV、XML、JSON等），非结构化数据（电子邮件，文档，PDF）和二进制数据（图像、音频、视频），从而形成一个容纳所有形式数据的集中式数据存储。 数据湖从本质上来讲，是一种企业数据架构方法，物理实现上则是一个数据存储平台，用来集中化存储企业内海量的、多来源，多种类的数据，并支持对数据进行快速加工和分析 (支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统）。从实现方式来看，目前Hadoop是最常用的部署数据湖的技术，但并不意味着数据湖就是指Hadoop集群。为了应对不同业务需求的特点，MPP数据库+Hadoop集群+传统数据仓库这种“混搭”架构的数据湖也越来越多出现在企业信息化建设规划中。 Data Lake： 数据湖Data Swamp： 数据沼泽Data Mart： 数据集市Data Warehouse： 数据仓库Data Cube：数据立方体Data Stream：数据流Data Virtualization：数据虚拟化 错误认知 错误认知1： 数据湖仅用于“存储”数据 支持对数据进行快速加工和分析。支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统 错误认知2：数据湖仅存储“原始”数据 需要有定义的机制来编目和保护数据。这些元素并非原始数据，而是对数据湖的管理数据。 数据和和分析解决方案的基本要素组织构建数据湖和分析平台时，他们需要考虑许多关键功能，包括： 数据移动（支持大规模的数据以原始形式导入）数据湖允许您导入任何数量的实时获得的数据。您可以从多个来源收集数据，并以其原始形式将其移入到数据湖中。此过程允许您扩展到任何规模的数据，同时节省定义数据结构、Schema 和转换的时间。 安全地存储和编目数据（编目使得数据是被监督的，可用的）数据湖允许您存储关系数据（例如，来自业务线应用程序的运营数据库和数据）和非关系数据（例如，来自移动应用程序、IoT 设备和社交媒体的运营数据库和数据）。它们还使您能够通过对数据进行爬网、编目和建立索引来了解湖中的数据。最后，必须保护数据以确保您的数据资产受到保护。 是数据湖里的数据本身有索引吗？还是基于数据湖做catalog、元数据管理等？catalog 即是对数据湖数据的索引？？？ 分析（可以直接在数据湖上，运行快速加工和分析）数据湖允许组织中的各种角色（如数据科学家、数据开发人员和业务分析师）通过各自选择的分析工具和框架来访问数据。这包括 Apache Hadoop、Presto 和 Apache Spark 等开源框架，以及数据仓库和商业智能供应商提供的商业产品。数据湖允许您运行分析，而无需将数据移至单独的分析系统。 机器学习（在数据湖上进行机器学习）数据湖将允许组织生成不同类型的见解，包括报告历史数据以及进行机器学习（构建模型以预测可能的结果），并建议一系列规定的行动以实现最佳结果。 Data swamp搭建数据湖容易，但是让数据湖发挥价值是很难。如果只是一直往里面灌数据，而应用场景极少，没有输出或者极少输出，形成单向湖。 企业的业务是实时在变化的，这代表着沉积在数据湖中的数据定义、数据格式实时都在发生的转变，企业的大型数据湖对企业数据治理（Data Governance）提升了更高的要求。大部分使用数据湖的企业在数据真的需要使用的时候，往往因为数据湖中的数据质量太差而无法最终使用。数据湖，被企业当成一个大数据的垃圾桶，最终数据湖成为臭气熏天，存储在Hadoop当中的数据成为无人可以清理的数据沼泽. 数据湖架构的主要挑战是存储原始数据而不监督内容。对于使数据可用的数据湖，它需要有定义的机制来编目和保护数据。没有这些元素，就无法找到或信任数据，从而导致出现“数据沼泽”。 满足更广泛受众的需求需要数据湖具有管理、语义一致性和访问控制。 Data Lake v.s. Data Warehouseaws 什么是数据湖 数据仓库、数据湖 -&gt; 数据中台 数据仓库里的数据都满足特定的 schema，而数据湖则没有。仓库里的数据是简单整理过的，湖里的则是原始的（但不全是原始的）。仓库里的来源也都是规范的关系数据，而湖里则什么都有。 数据仓库是一个优化的数据库，用于分析来自事务系统和业务线应用程序的关系数据。事先定义数据结构和 Schema 以优化快速 SQL 查询，其中结果通常用于操作报告和分析。数据经过了清理、丰富和转换，因此可以充当用户可信任的“单一信息源”。 数据湖有所不同，因为它存储来自业务线应用程序的关系数据，以及来自移动应用程序、IoT 设备和社交媒体的非关系数据。捕获数据时，未定义数据结构或 Schema。这意味着您可以存储所有数据，而不需要精心设计也无需知道将来您可能需要哪些问题的答案。您可以对数据使用不同类型的分析（如 SQL 查询、大数据分析、全文搜索、实时分析和机器学习）来获得见解。 随着使用数据仓库的组织看到数据湖的优势，他们正在改进其仓库以包括数据湖，并启用各种查询功能、数据科学使用案例和用于发现新信息模型的高级功能。Gartner 将此演变称为“分析型数据管理解决方案”或“DMSA”。 特性 数据仓库 数据湖 数据 来自事务系统、运营数据库和业务线应用程序的关系数据 来自 IoT 设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据 Schema 设计在数据仓库实施之前（写入型 Schema） 写入在分析时（读取型 Schema） 性价比 更快查询结果会带来较高存储成本 更快查询结果只需较低存储成本 数据质量 可作为重要事实依据的高度监管数据 任何可以或无法进行监管的数据（例如原始数据） 用户 业务分析师 数据科学家、数据开发人员和业务分析师（使用监管数据） 分析 批处理报告、BI 和可视化 机器学习、预测分析、数据发现和分析]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[az data engineer certificate]]></title>
    <url>%2F2020%2F10%2F10%2Faz-data-engineer-certificate%2F</url>
    <content type="text"><![CDATA[learning paths On-premises Env vs Cloudlink The term total cost of ownership (TCO) describes the final cost of owning a given technology. In on-premises systems, TCO includes the following costs: Hardware Software licensing Labor (installation, upgrades, maintenance) Datacenter overhead (power, telecommunications, building, heating and cooling) Cloud systems like Azure track costs by subscriptions. A subscription can be based on usage that’s measured in compute units, hours, or transactions. The cost includes hardware, software, disk storage, and labor. Because of economies of scale, an on-premises system can rarely compete with the cloud in terms of the measurement of the service usage. The cost of operating an on-premises server system rarely aligns with the actual usage of the system. In cloud systems, the cost usually aligns more closely with the actual usage. Comment However, many companies use the cloud still in a wasting way. The cost in cloud systems in fact rarely aligns with the actual usage, too. The main advantage of Cloud is that it can be charged on usage. Thus, this advantage only works when using the cloud by need. Otherwise, the cloud advantage evaporates, especially from the aspect of cost. The advantages of cloud: charge on usage enjoy the high quality and compresensive services of big company. Data typeslink For nonstructured Data, the data structure is defined only when the data is read. The difference in the definition point gives you flexibility to use the same source data for different outputs. JSON is in fact semistructured data. Examples of nonstructured data include binary, audio, and image files. NoSQL is in fact semistructured data. The open-source world offers four types of NoSQL databases: Key-value store: Stores key-value pairs of data in a table structure. Document database: Stores documents that are tagged with metadata to aid document searches. Graph database: Finds relationships between data points by using a structure that’s composed of vertices and edges. Column database: Stores data based on columns rather than rows. Columns can be defined at the query’s runtime, allowing flexibility in the data that’s returned performantly. Azure StorageAzure Storage account is the base storage type. It’s mainly used to store data but with poor or no query ability. Azure Storage offers four configuration options: Azure Blob: A scalable object store for text and binary data. The cheapst choice to store bot not query data. Azure Files: Managed file shares for cloud or on-premises deployments Azure Queue: A messaging store for reliable messaging between application components Azure Table: A NoSQL store for no-schema storage of structured data Tasks of an Azure data engineer: link example Data Engineer vs Data Scientist vs AI engineer: Data engineer decide how to organized the data and pre-process the data. Data scientist use the result of Data engineer to create analysis model, and extract value. AI engineer use the existed model &amp; tools to process the data. AI engineer may need the help of Data engineer to store the result, and the help of Data analysis to generate new model. Here are some of the tasks of an Azure data engineer: Design and develop data storage and data processing solutions for the enterprise. Set up and deploy cloud-based data services such as blob services, databases, and analytics. Secure the platform and the stored data. Make sure only the necessary users can access the data. Ensure business continuity in uncommon conditions by using techniques for high availability and disaster recovery. Monitor to ensure that the systems run properly and are cost-effective. Plan the data storage solutionDetermine operational needs What are the main operations you’ll be completing on each data type, and what are the performance requirements? Ask yourself these questions: Will you be doing simple lookups using an ID? Do you need to query the database for one or more fields? How many create, update, and delete operations do you expect? Do you need to run complex analytical queries? How quickly do these operations need to complete? a storage solution example on the e-commerce system: Product catalog data: cosmosDB Data classification: Semi-structured because of the need to extend or modify the schema for new products Operations: Customers require a high number of read operations, with the ability to query on many fields within the database. The business requires a high number of write operations to track the constantly changing inventory. Latency &amp; throughput: High throughput and low latency Transactional support: Required Photos &amp; videos: azure blob (with azure CDN) Data classification: Unstructured Operations: Only need to be retrieved by ID. Customers require a high number of read operations with low latency. Creates and updates will be somewhat infrequent and can have higher latency than read operations. Latency &amp; throughput: Retrievals by ID need to support low latency and high throughput. Creates and updates can have higher latency than read operations. Transactional support: Not required Buisiness Data: azure sql (with azure analysis services) Data classification: Structured Operations: Read-only, complex analytical queries across multiple databases Latency &amp; throughput: Some latency in the results is expected based on the complex nature of the queries. Transactional support: Required QuestionsPrivate cloud vs Public Cloud vs Specific Cloud [x] Are there still private cloud and public cloud?? ===&gt; yes [ ] What’s the difference? The private cloud will have higher quality cloud? The so-called SLA of public cloud is in fact not assured?? [x] Maybe there’re also specific cloud, like financial cloud (maybe more secure and fast) ===&gt; yes Points: There is specific cloud. Mayi has financial cloud. There are both public cloud &amp; private cloud, but they’re closely related. That is, they use the same cloud technology (images), but with different clusters &amp; differnt tariff. Azure storage account how to configure to use different type? Why they’re all in storage account instead of as independant service?? azure blob vs azure file storage Column database How the data is organized in the hardware? How should we save the data into column database? Wide Column Stores BigTable Cosmosdb(Cassendra) vs HBase: infrastructure, load-balance??? Cassendra: Wide Column Stores ELT vs ELTL In fact, often ELTL??? How is the data stored in T(transform)??? Data Types what’s markup language??? why yaml is not markup language but json is??? Azure SQL how azure sql support queries across multiple databases?? Why does azure sql data warehouse not support???]]></content>
      <tags>
        <tag>big data</tag>
        <tag>certificate</tag>
        <tag>cloud computing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache Memory]]></title>
    <url>%2F2020%2F06%2F28%2FCache-Memory%2F</url>
    <content type="text"><![CDATA[General Concept CPU Core Caching Cache Lines Cache MemoryAssociative Memory Direct-Mapped Memory Set Associative Memory Cache Read/Write Policies cache coherency MESI protocol: (Modified, Exclusive, Shared, Invalid) Invalid lines are cache lines that are either not present in the cache, or whose contents are known to be stale. For the purposes of caching, these are ignored. Once a cache line is invalidated, it’s as if it wasn’t in the cache in the first place. Shared lines are clean copies of the contents of main memory. Cache lines in the shared state can be used to serve reads but they can’t be written to. Multiple caches are allowed to have a copy of the same memory location in “shared” state at the same time, hence the name. Exclusive lines are also clean copies of the contents of main memory, just like the S state. The difference is that when one core holds a line in E state, no other core may hold it at the same time, hence “exclusive”. That is, the same line must be in the I state in the caches of all other cores. Modified lines are dirty; they have been locally modified. If a line is in the M state, it must be in the I state for all other cores, same as E. In addition, modified cache lines need to be written back to memory when they get evicted or invalidated – same as the regular dirty state in a write-back cache. Principle Of Locality Summary]]></content>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache - MicroService]]></title>
    <url>%2F2020%2F06%2F03%2FCache-MicroService%2F</url>
    <content type="text"><![CDATA[Where is my cache for a serviceArchitectural Patterns for Caching Microservices Patterns: embedded: save cache in the service client-server: a completely separate cache server reverse-proxy: put the cache in front of each service Sidecar: put the cache as a sidecar container that belongs to the service How does cache work? The application receives the request and checks if the same request was already executed (and stored in the cache) EmbeddedEmbedded Distributed Cache Why distributed? Same requests happen on different instances, and we want to use cache on the second same request no matter whether they reached the same instance. An update request reached one instance, and we want all the cache of this resource on all instances should be updated. Client-Serverpros: sperarate server, so you can use any programming language you want separate management. you can scale up/down, do backup, design security separatly on need. cons: a new deployment &amp; related ops work (cloud can make this simple) SidecarA mixture of Embedded &amp; client-server. (sidecar container pattern) Take k8s as example (now most of the sidecar pattern is implemented on k8s, becaused it supports this pattern inborn), we put the cache server with the service as separate containers on the same pod. The request reach the service container first, and then the service can access the cache server by localhost . The sidecar pattern is a technique of attaching an additional container to the main parent container so that both would share the same lifecycle and the same resources. You may think of it as a perfect tool for decomposing your application into reusable modules, in which each part is written in a different technology or programming language. Reverse proxy Everytime you try to access the servcie, you go to the reverse proxy (e.g. nginx). And the proxy will first check the cache for the request. If cache exists, return immediately, else, forward to the service and write the cache. pros: In this way, the service has no idea about the cache, so nothing will change on the service when cache introduced. cons: cannot use any application-based code to invalidate the cache. Also, you can put the reverse proxy into the side-car, that is, reverse-proxy-sidecar pattern. Now, there is no mature HTTP Reverse Proxy Cache Sidecar solution at all. Nginx can do it, but it’s not a good choice since it’s not mature. Caching Practices Always use caching in one place for a service. Mutliple caches will make the cache invalication and error-prone difficult.]]></content>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSSQL: multiple cascade paths]]></title>
    <url>%2F2020%2F05%2F27%2FMSSQL-multiple-cascade-paths%2F</url>
    <content type="text"><![CDATA[SymptomsYou may receive the following error message when you create a FOREIGN KEY constraint: (microsoft report) 1Server: Msg 1785, Level 16, State 1, Line 1 Introducing FOREIGN KEY constraint &#39;fk_two&#39; on table &#39;table2&#39; may cause cycles or multiple cascade paths. Specify ON DELETE NO ACTION or ON UPDATE NO ACTION, or modify other FOREIGN KEY constraints. Server: Msg 1750, Level 16, State 1, Line 1 Could not create constraint. See previous errors. For example, the table definition is like this: 12345678Table t1: Id: primaryKeyTable t2: Id: primaryKey parent: ForeignKey(t1, Id) on cascade delete child: ForeignKey(t1, Id) on cascade delete# this would raise the above exception CauseBasically, you can’t create multiple cascade paths to same table with cascade delete/update. Since you may define t2.parent with cascade delete, t2.child with cascade update(e.g. update as null), this will make sql server ambiguous when t1 is deleted. MSSQL doesn’t detect whether there’re actual circle or not, it just forbids the operation to make the design simple. A table cannot appear more than one time in a list of all the cascading referential actions that are started by either a DELETE or an UPDATE statement. For example, the tree of cascading referential actions must only have one path to a particular table on the cascading referential actions tree. stackoverflow SQL Server does simple counting of cascade paths and, rather than trying to work out whether any cycles actually exist, it assumes the worst and refuses to create the referential actions (CASCADE): you can and should still create the constraints without the referential actions. If you can’t alter your design (or doing so would compromise things) then you should consider using triggers as a last resort. FWIW resolving cascade paths is a complex problem. Other SQL products will simply ignore the problem and allow you to create cycles, in which case it will be a race to see which will overwrite the value last, probably to the ignorance of the designer (e.g. ACE/Jet does this). I understand some SQL products will attempt to resolve simple cases. Fact remains, SQL Server doesn’t even try, plays it ultra safe by disallowing more than one path and at least it tells you so.Microsoft themselves advises the use of triggers instead of FK constraints. WorkaroundUse trigger instead of ForeignKey to keep the integrity and avoid the exceptions. Set cascade delete on t2.child Use instead of trigger to cascade delete on t2.parent (you can also use trigger for all fields instead of foreign key constraints) 123456789CREATE TRIGGER [DELETE_t2] ON dbo.[t1] INSTEAD OF DELETEAS BEGIN SET NOCOUNT ON; DELETE FROM [t2] WHERE parent IN (SELECT Id FROM DELETED) DELETE FROM [t1] WHERE Id IN (SELECT Id FROM DELETED)END]]></content>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NHibernate: inverse, cascade]]></title>
    <url>%2F2020%2F05%2F25%2FNHibernate-inverse-cascade%2F</url>
    <content type="text"><![CDATA[playing-nhibernate-inverse-and-cascade, nhibernate-inverse bidirectional associations In database, there may be biodirectional relationships, e.g. Parent has multiple child, and Child has a parent.1234567891011121314#### class definitionclass Parent: - String id - IList&lt;Child&gt; childs class Child: - String id - Parent parent #### db definitiontable Parent: - idtable Child: - id - parentId InverseInverse focus on the association. It defines which side is responsible of the association maintenance (create, update, delete), that is, the assignment of parentId column. It doesn’t care about the maintenance of associated objects which is what cascade cares). By default, invert=false, then the assignment of parentId is maintened when parent is created/updated/deleted. If we set parent.child.inverse=true and the child.parent is not-null, then the assignment of parentId is maintened when child is created/updated/deleted. many-to-one is always inverse=&quot;false&quot; (the attribute does not exist), that is, it means nothing to set child.parent.inverse=true Cascadecascade instead will focus on the associated objects. It defines if the current object is responsible of the maintenance of associated objects. By default, cascade=None, that is, when saving parent, the childs on parent won’t be saved cascadelly. See cascade stackoverflow none - do not do any cascades, let users handle them by themselves. save-update - when the object is saved/updated, check the associations and save/update any object that requires it (including save/update the associations in many-to-many scenario). delete - when the object is deleted, delete all the objects in the association. delete-orphan - when the object is deleted, delete all the objects in the association. In addition, when an object is removed from the association and not associated with another object (orphaned), also delete it. all - when an object is save/update/delete, check the associations and save/update/delete all the objects found. all-delete-orphan - when an object is save/update/delete, check the associations and save/update/delete all the objects found. In additional to that, when an object is removed from the association and not associated with another object (orphaned), also delete it.]]></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 502 vs 504]]></title>
    <url>%2F2020%2F04%2F27%2F502-vs-504%2F</url>
    <content type="text"><![CDATA[nginx 502 和 504 超时演示 502 Bad Gateway: The server was acting as a gateway) or proxy and received an invalid response from the upstream server. 504: he server was acting as a gateway or proxy and did not receive a timely response from the upstream server. Conclusion504 是 nginx 没有及时从上游服务获取响应，超时了： 上游服务响应慢，读取 response / 发送 request 超时（upstream timed out (110: Operation timed out) **while** reading response header from upstream） 某些请求处理就是慢。此时就应该调大 proxy_read_timeout (默认 60s) 上游服务压力太大，响应变慢。此时可以增加上游服务的响应能力，也可以适当提升 proxy_send_timeout, proxy_read_timeout 连接上游服务超时。可能是上游服务已经断了，但由于 keepalive，nginx 依然保有 tcp 连接，但实际操作时，却连不上，就超时了。 502 是一般是上游服务器故障导致的。比如停机，进程被杀死，上游服务 reset 了连接，进程僵死等各种原因。在 nginx 的日志中我们能够发现 502 错误的具体原因，分别为：104: Connection reset by peer，113: Host is unreachable，111: Connection refused]]></content>
  </entry>
  <entry>
    <title><![CDATA[c# contextual keywords: yield]]></title>
    <url>%2F2020%2F03%2F10%2Fyield%2F</url>
    <content type="text"><![CDATA[yield is a contextual keywords. When it shows in a statement, it means the method or get accessor in which it appears is an iterator. Thus it provides a simple way to define an iterator, rather than a class that implements IEnumerable or IEnumerator. When you use the yield contextual keyword in a statement, you indicate that the method, operator, or get accessor in which it appears is an iterator. Using yield to define an iterator removes the need for an explicit extra class (the class that holds the state for an enumeration, see IEnumerator for an example) when you implement the IEnumerable and IEnumerator pattern for a custom collection type. Grammar12yield return expression; &#x2F;&#x2F; return an element in the iteratoryield break; &#x2F;&#x2F; end the iterator Q &amp; AWhat’s an iterator?An iterator means that it can be looped in foreach or LINQ query. In this case, it means the method or get accessor containing yield can be consumed by foreach or LINQ query. what does yield means in this iterator?The yield return will return an element in the iterator. During the loop, the iterator use MoveNext to get i (take power method as an example), and the MoveNext stop at the next yield return expression, and the Current property of the iterator is updated as this value, too. when does the iterator stopped? when there’s yield break when the method body is end what’s the requirements to define such iterator? The return type must be IEnumerable, IEnumerable, IEnumerator, or IEnumerator. The declaration can’t have any in ref or out parameters. Don’t use yield in Lambda expressions and anonymous methods. Don’t use yield in methods that contain unsafe blocks. For more information, see unsafe. Don’t use yield return in a try-catch block. A yield return statement can be located in the try block of a try-finally statement. yield break can be located in a try block or a catch block but not a finally block examplesyield in method12345678910111213141516171819static void Main(string[] args)&#123; var powers &#x3D; Power(2, 10); &#x2F;&#x2F; won&#39;t execute the body of Power foreach (var i in powers) &#123; Console.WriteLine($&quot;&#123;i&#125; &quot;); &#125;&#125;&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; yield in method &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;public static IEnumerable&lt;int&gt; Power(int number, int exponent)&#123; int result &#x3D; 1; for (int i &#x3D; 0; i &lt; exponent; i++) &#123; result &#x3D; result * number; yield return result; &#125;&#125; yield in get accessor123456789101112131415161718192021222324252627282930313233static void Main(string[] args)&#123; foreach (var i in new Galaxies().AllGalaxies) &#123; Console.WriteLine($&quot;&#123;i.Name&#125;, &#123;i.Age&#125;&quot;); &#125;&#125;class Galaxies&#123; &#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; yield in get accessor &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; public IEnumerable&lt;Galaxy&gt; AllGalaxies &#123; get &#123; yield return new Galaxy(&quot;The milky way&quot;, 1000); yield return new Galaxy(&quot;Tadpole&quot;, 1000); yield return new Galaxy(&quot;Andromeda&quot;, 1000); &#125; &#125;&#125;class Galaxy&#123; public string Name &#123; get; &#125; public int Age &#123; get; &#125; public Galaxy(string name, int age) &#123; Name &#x3D; name; Age &#x3D; age; &#125;&#125;]]></content>
      <tags>
        <tag>c#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FluentValidator]]></title>
    <url>%2F2020%2F01%2F17%2FFluentValidator%2F</url>
    <content type="text"><![CDATA[FluentValidation Knowledge The RuleFor method create a validation rule. To specify a validation rule for a particular property, call the RuleFor method, passing a lambda expression that indicates the property that you wish to validate. Rules are run syncronously By default, all rules in FluentValidation are separate and cannot influence one another. This is intentional and necessary for asynchronous validation to work. Must, NotNull…. are built-in validators. WithMessage is a method on a validator. When defines condition for validator(s). Append multiple validators on a same property are called chaining validators. Chainning Validators are executed by sequence. CascadeMode can define how these chaining validators are exectued. Continue is default which means even a validator fail, the latter validators will still be invoked. StopOnFirstFailure will stop at the first failure of these chaining validators. When defines condition for validator(s)/rules. ApplyConditionTo.AllValidators is the default setting. ApplyConditionTo.CurrentValidator will make the condition only work to the preceding validator. By default FluentValidation will apply the condition to all preceding validators in the same call to RuleFor]]></content>
      <tags>
        <tag>DTO validator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c# basic]]></title>
    <url>%2F2019%2F06%2F05%2Fc-sharp-basic%2F</url>
    <content type="text"><![CDATA[.net, asp.net, cc# is like java language specification; .net is like jdk/javase/javaee asp.net: is like springboot default, as, is sln: solution ——&gt; csproj: c sharp project ——&gt; files .sln vs .csproj Key conceptsref: c# concepts solution: a complete application, similar to maven project. It contains several c# project like frontend, backend, library to compose a complete application. project: similar to maven module. It can be a web project, a library, a windows program, etc. assembly: similar to maven jar. A c# project is corresponding to an assembly. An assembly can be a dll, exe, etc. namespace: similar to java package. It’s a logical concept to avoid naming conflicts while assembly is a physical concept. A namespace can be in different assemblies and an assembly can contains multiple namespaces. Accessibility levelsref: accessibility levels public: access is not restricted. (all) private: limited to containing type (only self) protected: limited to the containing class and types derived from the containing class. (sub-classes) internal: limited to the current assembly (only the same assembly) protected internal: limited to the current assembly or types derived from the containing class (same assembly &amp; sub-classes) protected private: limited to the containing class and types derived from the containing class in the current assembly (sub-classes in same assembly) data typesNullableNullable (T?): similar to Optional in java, but the T can only be value type, which are simple types, enum types, struct types, and nullable types. Because value type has no null value (not alike reference type — — object), and there’re situations when their values are undefined, nullable is born. 1int? x &#x3D; null &#x2F;&#x2F; int? is the shorthand for Nullable&lt;int&gt; Delegatedelegate is like @FunctionalInterface in java. A delegate type represents references to methods with a particular parameter list and return type. Delegates make it possible to treat methods as entities that can be assigned to variables and passed as parameters. 12345delegate double Function(double x); &#x2F;&#x2F; functional interface definitionstatic double[] Apply(double[] a, Function f) &#123;&#125; &#x2F;&#x2F; use delegate as a method paramApply(&#123;0.0, 0.5, 1.0&#125;, (double x) -&gt; x*x); &#x2F;&#x2F; define an anoymous function MethodMethod contains parameters &amp; return type &amp; body &amp; modifier &amp; type parameters. parametersArgument is where the initial value is from, and parameter is used to pass value/references to methods. Parameter has modifier (e.g. out, final, this, params) and type Argument are passed as parameter in 4 ways: value parameter:parameter change won’t affect the argument only for input parameter passing optional by specify default value reference parameterparameter change will affect the argument for input/output parameter passing 1public void swap(ref int x, ref int y) output parametersimilar to reference parameter except for that the initial value is unimportant. 123public void divide(int x, int y, out int res, out int remainder)&#123;&#125;divide(1,2,out var res, out var remainder); parameter arrayssimilar to java … extension methodsextension method is a mechanism that you can “add method” to a class without extending from it. This method: must be static works in scope when you explictly import the namespace into you source code with a using directive. must be the first param of method. when used, it’s same as the normal instance method. 1234567891011namespace ExtensionMethods&#123; public static class MyExtensions &#123; public static int WordCount(this String str) &#123; return str.Split(new char[] &#123; &#39; &#39;, &#39;.&#39;, &#39;?&#39; &#125;, StringSplitOptions.RemoveEmptyEntries).Length; &#125; &#125; &#125;]]></content>
      <tags>
        <tag>c#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python basic]]></title>
    <url>%2F2019%2F03%2F26%2Fpython-basic%2F</url>
    <content type="text"><![CDATA[great free learning website with quiz great online editor &amp; debugger Python学习资料/文章/指南整理 Zen of PythonBy typing import this, you can see the zen of python. Some need more explanation: Explicit is better than implicit: It is best to spell out exactly what your code is doing. This is why adding a numeric string to an integer requires explicit conversion, rather than having it happen behind the scenes, as it does in other languages.Flat is better than nested: Heavily nested structures (lists of lists, of lists, and on and on…) should be avoided.Errors should never pass silently: In general, when an error occurs, you should output some sort of error message, rather than ignoring it. PEPPython Enhancement Proposals (PEP) are suggestions for improvements to the language, made by experienced Python developers. PEP 8 is a style guide on the subject of writing readable codePEP 20: The Zen of PythonPEP 257: Style Conventions for Docstrings Naming conventionsPEP 8 is a style guide on the subject of writing readable code. It contains a number of guidelines in reference to variable names, which are summarized here:- modules should have short, all-lowercase names;- class names should be in the CapWords style;- most variables and function names should be lowercase_with_underscores;- constants (variables that never change value) should be CAPS_WITH_UNDERSCORES;- names that would clash with Python keywords (such as ‘class’ or ‘if’) should have a trailing underscore. installBrew install1$ brew install python install multiple version123456789101112131415161718192021222324252627282930313233343536# use pyenv to install multiple version$ brew update$ brew install pyenv# Clone the repository to to get the latest version of pyenv$ git clone https://github.com/pyenv/pyenv.git ~/.pyenv# define envs$ echo 'export PYENV_ROOT="$HOME/.pyenv"' &gt;&gt; ~/.zshrc$ echo 'export PATH="$PYENV_ROOT/bin:$PATH"' &gt;&gt; ~/.zshrc$ source ~/.zshrc# get all versions that can be installed$ pyenv install --list$ pyenv install 3.7# all current installed$ pyenv versions# current active$ pyenv version# set as gloabl version$ pyenv global 3.7# set a local version # This command creates a .python-version file in your current directory. If you have pyenv active in your environment, this file will automatically activate this version for you.$ pyenv local 3.7# set as shell version# This command activates the version specified by setting the PYENV_VERSION environment variable. This command overwrites any applications or global settings you may have. If you want to deactivate the version, you can use the --unset flag.$ pyenv shell 3.7# check the version$ python3 --version# add this to ~/.zshrc if the global command not work, to active the pyenv shell features$ eval "$(pyenv init -)" Other pythons installed: /usr/local/bin /usr/local/Cellar pippip is the package installer for python. 1$ pip install package-name On windows, to use pip after python installation, you need to config both the python &amp; pip path. 1$ export PATH=$PATH:c/users/xiaoming/AppData/Programs/Python/Python37:c/users/xiaoming/AppData/Programs/Python/Python37/Scripts common used commands1234567# list all installed packages$ pip freeze# downgrade pip$ python -m install pip=20.2.4# upgrade pip$ python -m pip install --upgrade pip PackagingIn Python, the term packaging refers to putting modules you have written in a standard format, so that other programmers can install and use them with ease.This involves use of the modules setuptools and distutils. organize existing files correctly. Place all of the files you want to put in a library in the same parent directory. This directory should also contain a file called __init__.py, which can be blank but must be present in the directory. __init__.py turns a directory to a module.This directory goes into another directory containing the readme and license, as well as an important file called setup.py. 12345678SoloLearn&#x2F; LICENSE.txt README.txt setup.py sololearn&#x2F; __init__.py sololearn.py sololearn2.py Write the setup.py file. This contains information necessary to assemble the package so it can be uploaded to PyPI and installed with pip (name, version, etc.). 1234567 setup( name='SoloLearn', version='0.1dev', packages=['sololearn',], license='MIT', long_description=open('README.txt').read(),) Write Other Files Build a source distribution, use the command line to navigate to the directory containing setup.py, and run the command python setup.py sdist. Run python setup.py bdist or, for Windows, python setup.py bdist_wininst to build a binary distribution. Upload the package to PyPI. Use python setup.py register, followed by python setup.py sdist upload to upload a package. install a package with python setup.py install. __init__.pyPython init.py 作用详解 __init__.py 文件的作用是将文件夹变为一个Python模块,Python 中的每个模块的包中，都有__init__.py 文件。 通常__init__.py 文件为空，但是我们还可以为它增加其他的功能。我们在导入一个包时，实际上是导入了它的__init__.py文件。这样我们可以在__init__.py文件中批量导入我们所需要的模块，而不再需要一个一个的导入。 12345678910# package# __init__.pyimport reimport urllibimport sysimport os# a.pyimport package print(package.re, package.urllib, package.sys, package.os) 注意这里访问__init__.py文件中的引用文件，需要加上包名。 __init__.py中还有一个重要的变量，__all__, 它用来将模块全部导入。 12345# __init__.py__all__ &#x3D; [&#39;os&#39;, &#39;sys&#39;, &#39;re&#39;, &#39;urllib&#39;]# a.pyfrom package import * 这时就会把注册在__init__.py文件中__all__列表中的模块和包导入到当前文件中来。 可以了解到，__init__.py主要控制包的导入行为。 setup.pysetuptools is the packaging tool for python (like maven/gradle for java?) When coding in local, you can use pip install to install dependency. However, when deploy a python project, you need to package all dependency, modules into one project. You need setup.py. When organize python in modules, you need to know and import the function in other modules. In compilation stage, you need to feel the other modules&amp;packages. setup.py does the magic. (This is my guess) Packaging for usersFor normal users who don’t have python on computer, we need to convert scripts to executables for them. For Windows, py2exe or PyInstaller or cx_Freeze can be used to package a Python script, along with the libraries it requires, into a single executable.For Macs, use py2app, PyInstaller or cx_Freeze. Some special grammarsif _name__ == ‘\_main__’如何简单地理解Python中的if _name__ == ‘\_main__’ If you are test.py, then other module knows you as __name__==&#39;test&#39;, and you know yourself as __name==&#39;__main__&#39;. The code block in if __name__ == &#39;__main__&#39; will only be executed when the file is executed directly. If other module import test.py as module, the code block won’t be executed. It’s kind of like main function in java/c, but with big difference. Python is a script language — interpretive execution, which can be run without any so-called main entry. You don’t need to make test.py runnable by providing main, but you can provide if __name__ == &#39;__main__&#39; if you want this block only executed when called directly rather than as module. elseThe else statement can be used in: the if statement a for or while loop, the code within it is called if the loop finishes normally / completely (when a break statement does not cause an exit from the loop). the try/except statements, the code within it is only executed if no error occurs in the try statement. 1234567891011121314151617181920212223242526272829303132333435363738394041424344def if_else(a): if a == 'if': print('this is in if') else: print('this is not if!')def for_else(): for i in range(10): if i &gt; 2: # print('Will get here. The else won\'t be executed') break else: print('first for finish completely') for i in range(10): if i &gt; 10: # print('Never get here. The else will be executed') break else: print('second for finish completely')def try_else(): try: print(5/0) except: print('there is exception, the first else won\'t be executed') else: print('first try finished successfully') try: print(5/1) except: print('there is no exception, the else will be executed') else: print('second try finished successfully')if_else('not_if') # this is not if!for_else() # second for finish completelytry_else()'''there is exception, the first else won't be executed5.0second try finished successfully''' classClasses are created using the keyword class and an indented block, which contains class methods (which are functions). All methods must have self as their first parameter, you do not need to include it when you call the methods. Within a method definition, self refers to the instance calling the method. To inherit a class from another class, put the superclass name in parentheses after the class name. Classes can also have class attributes, created by assigning variables within the body of the class. These can be accessed either from instances of the class, or the class itself. 1234567891011121314151617181920class Animal: def __init__(self, name, color): self.name = name self.color = color def shout(self): print('ha')class Dog(Animal): legs = 4 # the class attribute def shout(self): print('wong') super().shout() # call super methodDog.legs # 4d = Dog('heidou', 'black')d.legs # 4d.name # 'heidou'd.shout() # wong# ha __new__ vs __init____new__ in python _new__ is a static method which creates an instance. It allocates memory for an object. \_init__ initialize the value of the object (instance). __new__ is rarely overriden. When you are instantiate an instance by calling the class, the __new__ gets called first to create the object in memory, and then __init__ is called to initialize it. __new__ must return the created object. Only when __new__ returns the created instance then __init__ gets called. If __new__ does not return an instance then __init__ would not be called. The __new__ definition: 1__new__(cls, *args, **kwargs) An example: 123456789101112131415161718192021222324import datetime as dtclass A: def __new__(cls, *args, **kwargs): print(cls) print(args) print(kwargs) obj = object.__new__(cls) setattr(obj, 'created_at', dt.datetime.now()) return obj # if no return here, __init__ won't be called later def __init__(self, a, named): print('in init') self.a = a self.b = named def __repr__(self): return 'a=&#123;0&#125;, b=&#123;1&#125;, created_at: &#123;2&#125;'.format(self.a, self.b, self.created_at)a = A(1, named=2)'''&lt;class '__main__.A'&gt;(1,)&#123;'named': 2&#125;in init'''print(a) # a=1, b=2, created_at: 2020-10-19 15:22:19.558501 __del__Destruction of an object occurs when its reference count reaches zero. The del statement reduces the reference count of an object by one, and this often leads to its deletion.The magic method for the del statement is __del__. 12a = 5del a privateThe Python philosophy is often stated as “we are all consenting adults here”, meaning that you shouldn’t put arbitrary restrictions on accessing parts of a class. Hence there are no ways of enforcing a method or attribute be strictly private. However, there are ways to discourage people from accessing parts of a class, such as by denoting that it is an implementation detail, and should be used at their own risk. Weakly private methods and attributes have a single underscore at the beginning. This signals that they are private, and shouldn’t be used by external code. Its only actual effect is that from module_name import * won’t import variables that start with a single underscore. Strongly private methods and attributes have a double underscore at the beginning of their names. This causes their names to be mangled, which means that they can’t be accessed from outside the class by the name. The purpose of this isn’t to ensure that they are kept private, but to avoid bugs if there are subclasses that have methods or attributes with the same names. Basically, Python protects those members by internally changing the name to include the class name. 12345678class A: __egg = 7 def __init__(self, a): self.__a = aA._A__egg # 7a = A('a')a._A__a # 'a' class methods vs instance methods vs static methodsInstance methods are called by a instance, which is passed to the self parameter of the method. Class methods are called by a class, which is passed to the cls parameter of the method.A common use of these are factory methods, which instantiate an instance of a class, using different parameters than those usually passed to the class constructor.Class methods are marked with a classmethod decorator. Static methods are similar to class methods, except they don’t receive any additional arguments; they are identical to normal functions that belong to a class.They are marked with the staticmethod decorator. 123456789101112131415161718class Rectangle: def __init__(self, a, b): self.a = a self.b = b def area(self): print(self.a * self.b) @classmethod def new_square(cls, a): return cls(a, a) @staticmethod def validate_int(a): assert isinstance(a, int), 'must be int'r = Rectangle(4, 5)r.area() # 20s = Rectangle.new_square(4)s.area() # 16Rectangle.validate_int('fd') PropertiesProperties are created by putting the property decorator above a method, which means when the instance attribute with the same name as the method is accessed, the method will be called instead.One common use of a property is to make an attribute read-only. Properties can also be set by defining setter/getter functions.The setter function sets the corresponding property’s value. To define a setter, you need to use a decorator of the same name as the property, followed by a dot and the setter keyword. ​`pythonclass Pizza: def init(self, a): self._a = a @property def a_list(self): return [self._a] * 10 @a_list.setter def a_list(self, value): self._a = value[0] p = Pizza(7)p.a_list # [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]p.a_list = [1]p.a_list # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172## Magic Methods**Magic methods** are special methods which have **double underscores** at the beginning and end of their names. They are used to create functionality that can&#39;t be represented as a normal method.&#96;__init__&#96; is the constructor.Magic methods for common operators (operator overload):&#96;__add__&#96; for -&#96;__sub__&#96; for -&#96;__mul__&#96; for *&#96;__truediv__&#96; for &#x2F;&#96;__floordiv__&#96; for &#x2F;&#x2F;&#96;__mod__&#96; for %&#96;__pow__&#96; for **&#96;__and__&#96; for &amp;&#96;__xor__&#96; for ^&#96;__or__&#96; for |Magic methods for comparisons (If &#96;__ne__&#96; is not implemented, it returns the opposite of &#96;__eq__&#96;):&#96;__lt__&#96; for &lt;&#96;__le__&#96; for &lt;&#x3D;&#96;__eq__&#96; for &#x3D;&#x3D;&#96;__ne__&#96; for !&#x3D;&#96;__gt__&#96; for &gt;&#96;__ge__&#96; for &gt;&#x3D;Magic methods for making classes act like containers:&#96;__len__&#96; for len()&#96;__getitem__&#96; for indexing&#96;__setitem__&#96; for assigning to indexed values&#96;__delitem__&#96; for deleting indexed values&#96;__iter__&#96; for iteration over objects (e.g., in for loops)&#96;__contains__&#96; for inMagic methods for converting objects to built-in types:&#96;__int__&#96; for int()&#96;__str__&#96; for str()### &#96;r&#96; methods&lt;a name &#x3D; &quot;r-methods&quot; &#x2F;&gt;There are equivalent **r** methods for all magic methods that overloads operators. For example, the expression **x + y** is translated into **x.__add__(y)**. However, if x hasn&#39;t implemented __add__, and x and y are of different types, then **y.__radd__(x)** is called.&quot;r&quot; stands for &quot;right&quot;, meaning that the operator passed is on the right. If the conversion of the 1st type to the 2nd isn&#39;t supported, it simply tries calling the inverse one (2nd conversion to the 1st) for the other type.**r** methods can be used when you want to overload operator between a thirt lib type and your type. Since you may not modify the third library, you can add the reverse **r** method in your type.&#96;&#96;&#96;pythonclass PairNumber:def __init__(self, n1, n2): self.n1 &#x3D; n1 self.n2 &#x3D; n2def __floordiv__(self, other): return PairNumber(self.n1&#x2F;&#x2F;other.n1, self.n2 &#x2F;&#x2F; other.n2)def __rtruediv__(self, other): return PairNumber(other&#x2F;self.n1, other&#x2F;self.n2)def __repr__(self): return &#39;PairNumber&#123;0&#125;&#39;.format((self.n1, self.n2))def __str__(self): return str((self.n1, self.n2))class IntPairNumber(PairNumber):def __int__(self): return self.n1 + self.n2p1 &#x3D; IntPairNumber(4, 10)p2 &#x3D; IntPairNumber(3, 19)p1 &#x2F;&#x2F; p2 # PairNumber(1, 0)print(p1 &#x2F;&#x2F; p2) # (1, 0)str(2 &#x2F; p1) # PairNumber(0.5, 0.2)int(p1) # 14 __call__ method__call__ method can call an object as a function. Thus you can transfer the object as a func parameter in a function. 12345678910111213class PairNumber: def __init__(self, n1, n2): self.n1 = n1 self.n2 = n2 def __call__(self, other): return self.n1 + self.n2 + otherdef addTwice(func, other): return func(other) + otherp1 = PairNumber(1,2)p1(4) # 7addTwice(p1, 4) # 11 __str__ vs __repr__In short, the goal of __repr__ is to be unambiguous and __str__ is to be readable. If __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__. The commandline will show the content in __repr__ when you simply type the object. And when print(some_obj), it will show the content in __str__ of that object. So many would say: __repr__ is for developers, __str__ is for customers. e.g. obj = uuid.uuid1(), obj._str\_() is “2d7fc7f0-7706-11e9-94ae-0242ac110002” and obj.__repr__() is “UUID(‘2d7fc7f0-7706-11e9-94ae-0242ac110002’)”. To sum up: implement __repr__ for any class you implement. This should be second nature. Implement __str__ if you think it would be useful to have a string version which errs on the side of readability. See example in r method Opening FilesYou can specify the mode used to open a file by applying a second argument to the open function.Sending “r” means open in read mode, which is the default.Sending “w” means write mode, for rewriting the contents of a file.Sending “a” means append mode, for adding new content to the end of the file. Adding “b” to a mode opens it in binary mode, which is used for non-text files (such as image and sound files).For example: 123456789# write modeopen("filename.txt", "w")# read modeopen("filename.txt", "r")open("filename.txt")# binary write modeopen("filename.txt", "wb") You can use the + sign with each of the modes above to give them extra access to files. For example, r+ opens the file for both reading and writing. Use help(open) to see the complete file modes supported. “r”Read from file - YESWrite to file - NOCreate file if not exists - NOTruncate file to zero length - NOCursor position - BEGINNING “r+” ====&gt; Opens a file for both reading and writing (write from the current cursor, that is, if you already call file.read(), then it means cursor is already at the end, then the writing is appending. However, if you start with file.write(), the cursor is at the beginning, thus it will rewrite from the beginning)Read from file - YESWrite to file - YESCreate file if not exists - NOTruncate file to zero length - NOCursor position - BEGINNING “w”Read from file - NOWrite to file - YESCreate file if not exists - YESTruncate file to zero length - YESCursor position - BEGINNING “w+” ===&gt; Opens a file for both writing and readingRead from file - YESWrite to file - YESCreate file if not exists - YESTruncate file to zero length - YESCursor position - BEGINNING “a”Read from file - NOWrite to file - YESCreate file if not exists - YESTruncate file to zero length - NOCursor position - END “a+” ===&gt; Opens a file for both appending and readingRead from file - YESWrite to file - YESCreate file if not exists - YESTruncate file to zero length - NOCursor position - END Iterable containersList/string/tuple slicingslice can have 3 parameters: Start index (included, count from end of the list if negative, default as 0) End index (excluded, count from end of the list if negative, default as end of the list) Step 12345678910111213141516171819str = '0123456'tp = tuple(str) # ('0', '1', '2', '3', '4', '5', '6')lst = list(str) # ['0', '1', '2', '3', '4', '5', '6']# get first 2str[:2]# get last 2str[5:]str[-2:]# get odd number (the third number is the step)str[1::2]# reversestr[::-1]# get the 3 number from 1(included) and reverse itstr[3:0:-1] List Comprehensions123456cubes = [i**3 for i in range(5)] # [0, 1, 8, 27, 64]# with if statementevenSquares = [i**2 for i in range(5) if i ** 2 % 2 == 0] # [0, 4, 16]evenSquares = [i**2 for i in range(0,5,2)] # [0, 4, 16]evens = [i for i in range(5) if i%2 == 0] # [0, 2, 4] all &amp; anyuse all() / any() to check a list of bool. 123456nums = [1,2,3,4,5]if all([i &gt; 5 for i in nums]): print("All larger than 5")if any([i % 2 == 0 for i in nums]): print("At least one is even") Tuple1234567891011# define a tuplet1 = 1,2,3t2 = (1,2,3)t1 == t2 # True# unpack a tuplea,b,*c,d = range(10)a # 0b # 1c # [2,3,4,5,6,7,8]d # 9 GeneratorGenerators are a type of iterable, like lists or tuples. Unlike lists, they don’t allow indexing with arbitrary indices, but they can still be iterated through with for loops.They can be created using functions and the yield statement. Using generators results in improved performance, which is the result of the lazy (on demand) generation of values, which translates to lower memory usage. Furthermore, we do not need to wait until all the elements have been generated before we start to use them. 12345678910def spell(): word = '' for c in 'spam': word += c yield wordfor w in spell(): print(w)print(list(spell())) # to normal list ['s', 'sp', 'spa', 'spam'] SetSets can be combined using mathematical operations (list, tuple, dict do not support these operators)The union operator | combines two sets to form a new one containing items in either.The intersection operator &amp; gets items only in both.The difference operator - gets items in the first set but not in the second.The symmetric difference operator ^ gets items in either set, but not both. 12345678s1 = &#123;1,2,3,4,5&#125;s2 = &#123;3,4,5,6,7&#125;print(s1 | s2) # union: &#123;1,2,3,4,5,6,7&#125;print(s1 &amp; s2) # intersection: &#123;3,4,5&#125;print(s1 - s2) # in s1 but not s2: &#123;1,2&#125;print(s2 - s1) # in s2 but not s1: &#123;6,7&#125;print(s1 ^ s2) # not in s1 or not in s2: &#123;1,2,6,7&#125; String format12345####### hello world hello'&#123;0&#125; &#123;1&#125; &#123;0&#125;'.format('hello', 'world')'&#123;x&#125; &#123;y&#125; &#123;x&#125;'.format(x='hello', y='world') # use namer'fdjks/fjdkls' # r means raw, so you don't need to escape any character FunctionsFunction ArgumentsUsing *args as a function parameter enables you to pass an arbitrary number of arguments to that function. The arguments are then accessible as the tuple args in the body of the function. \kwargs (standing for keyword arguments) allows you to handle named arguments that you have not defined in advance. The keyword arguments return a dictionary in which the keys are the argument names, and the values are the argument values. 123456789101112131415161718192021def some_func(a, some_default='this is a default value', *args, **kwargs): print(a) print(some_default) print(args) print(kwargs)some_func('this is a')'''this is athis is a default value()&#123;&#125;'''some_func('this is a', 'value rather than default', 'first arg', 'second arg', first_kwarg='1', second_kwarg='2')'''this is avalue rather than default('first arg', 'second arg')&#123;'first_kwarg': '1', 'second_kwarg': '2'&#125;''' LambdaA lambda defines an anoymous function. It consists of the lambda keyword followed by a list of arguments, a colon, and the expression to evaluate and return. 12345678## use named functiondef cube(x): return x ** 3 print(cube(3)) # 27## use lambdacube = lambda x: x ** 3print(cube(3)) # 27 Common used functors:map, filter module itertools is a standard library that contains several functions that are useful in FP. One type of function it produces is infinite iterators.The function count counts up infinitely from a value.The function cycle infinitely iterates through an iterable (for instance a list or string).The function repeat repeats an object, either infinitely or a specific number of times.takewhile - takes items from an iterable while a predicate function remains true;chain - combines several iterables into one long one;accumulate - returns a running total of values in an iterable.product - get possible combinations of some iterablespermutation - get permutation of an iterable 12345678from itertools import accumulate, takewhile, product, permutationsnums = list(accumulate(range(5))) # [0,1,3,6,10]list(takewhile(lambda x: x &lt;6, nums)) # [0,1,3]letters = ('a', 'b')list(product(letters, range(2))) # [(a,0), (a,1), (b,0), (b,1)] The result is always iterable of tuplelist(permutations(letters)) # [('a', 'b'), ('b', 'a')] DecoratorDecorators provide a way to modify functions using other functions. Python provides support to wrap a function in a decorator by pre-pending the function definition with a decorator name and the @ symbol. You can use a decorator if you want to modify more than one function in the same way. It’s the common template of multiple functions. 12345678910111213141516171819202122232425262728293031from functools import wraps## define two decoratorsdef input_print_deca(func): ## this annotation make the callback return the original func's name instead of the wrap function's name. @wraps def wrap(): x = int(input('x: ')) y = int(input('y: ')) res = func(x, y) print('the res: &#123;0&#125;'.format(res)) return wrap## use a decorator, and replace the origin method with the decorated onedef add(x,y): return x+yadd = input_print_deca(add) # if the add method is a function from third library, you may want to add function to it, this way will help. However, you don't need to replace the original method, and you can just assign it to a new one.## use @ grammar to quick add common code to a function. This has the same effect as the above equation.@input_print_decadef diff(x, y): return x - y@input_print_decadef divide(x, y): assert y != 0, 'cannot divide zero' return x / y## call the decorated functions, notice now there's no inputadd()diff()divide() A single function can have multiple decorators. They are used from top to down. Everytime the original func is called, the current decorator is used. 1234567891011121314151617181920212223242526272829303132333435363738## define anther two decoratorsdef success_deca(func): def wrap(x,y): print('in success deca') res = func(x,y) print('yay success!' + '*' * 10) return res return wrapdef success_deca2(func): def wrap(x,y): print('in success deca2') res = func(x,y) print('yay success 2' + '-' * 10) return res return wrap## use all these decorators@input_print_deca@success_deca@success_deca@success_deca2def diff(x,y): return x - y## call the decorated funcdiff()"""x: 5 # the call to diff() trigger the input_print_deca decoratory: 6in success deca # in input_print_deca, the call to func trigger the first success_deca decoratorin success deca # in success_deca, the call to func trigger the second success_deca decoratorin success deca2 # in the second success_deca, the call to func trigger the success_deca2 decoratoryay success 2---------- # the success_deca2 first returnyay success!********** # the second success_deca returnyay success!********** # the first success_deca returnthe res: -1 # the input_print_deca return""" Generatorsee Iterable Containers / Generator above. async/awaitsome links: Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - youtube GitHub - dabeaz/curio: Good Curio! a library to separate the asynchronus world and synchronus world by the author of the above video. Python Asyncio与多线程/多进程那些事 Python Async/Await入门指南 从0到1，Python网络编程的入门之路 从0到1，Python异步编程的演进之路 async create a coroutine which should be excuted when called. Use await in an async function to hung up the coroutine itself until the awaited coroutine finished. await can only be used in async functions. The object after await must be an Awaitable (as long as you implement __await__(), it’s an Awaitable. Coroutine extends from Awaitable) async can be used anywhere except for: lambda list comprehension (available since python 3.6) default methods (e.g. __init__()), but we can use metaclass to make __init__ awaitable. eventloopThe essence of aysncio is an eventloop. All awaitables are added to the eventloop and wait for executing sequentially. If an event is interrupted by IO or sth, another event will be executed. For every thread, it has its own eventloop. Events in different eventloops cannot communicate. So basically, events in asyncio are executed sequentially, whereas their requests to IO may be paralled. And the await keyword means to emit the Awaitable event at once and the program will wait until it finishes. Thus if we want to emit two or more Awaitable events at almost the same time, we use asyncio.create_task(awaitable). (function) create_task: (coro: Generator[Any, None, _T@create_task] | Coroutine[Any, Any, _T@create_task], *, name: str | None = …) -&gt; Task[_T@create_task] Schedule the execution of a coroutine object in a spawn task. Return a Task object. When using create_task, the Awaitable is told to be emited as soon as possible. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import asynciofrom datetime import timedeltafrom time import time, time_nsdef ns_to_time(nanoseconds): return timedelta(microseconds=(nanoseconds/1000))def now(): return ns_to_time(time_ns())first_times = []second_times = []async def print_first(n, if_print = True): print(f'&#123;now()&#125;: first print start.') for i in range(n): anow = time_ns() first_times.append(anow) if if_print: print(f'&#123;ns_to_time(anow)&#125;: first-&#123;i&#125;') await asyncio.sleep(0.1) print(f'&#123;now()&#125;: first print end.')async def print_second(n, if_print = True): print(f'&#123;now()&#125;: second print start.') for i in range(n): anow = time_ns() second_times.append(anow) if if_print: print(f'&#123;ns_to_time(anow)&#125;: second-&#123;i&#125;') await asyncio.sleep(0.1) print(f'&#123;now()&#125;: second print end.')async def main(): # task1 &amp; task2 are emitted immediately task1 = asyncio.create_task(print_first(5)) task2 = asyncio.create_task(print_second(5)) # wait for the two tasks to finish, or the program will end while the tasks are still running. await task1 await task2 # no intersection and the print result shows that all the events are executed sequentially print(first_times) print(second_times) print(f'the intersected times: &#123;set(first_times) &amp; set(second_times)&#125;')asyncio.run(main()) async &amp; decorator12345678910111213141516171819202122232425262728293031323334# 创建一个 anotation, 可以根据 function 是否在 async 环境运行def from_coro(n): return bool(sys._getframe(n).f_code.co_flags &amp; 0x80)def run(coro): try: coro.send(None) except StopIteration as e: return e.valuefrom functools import wrapsdef awaitable(syncfunc): def decorator(asyncfunc): @wraps(asyncfunc) def wrapper(*args, **kwargs): if from_coro(2): print('from coro...') return asyncfunc(*args, **kwargs) else: print('not from coro...') return syncfunc(*args, **kwargs) return wrapper return decoratordef spam(): print('the blue one')@awaitable(spam)async def spam(): print('the red one')spam()async def main(): await spam() Awaitable &amp; Coroutine classThe object after await must be an Awaitable (as long as you implement __await__(), it’s an Awaitable. Coroutine extends from Awaitable. 12345678910111213141516171819202122232425262728293031323334# The abstract `Awaitable` and `Coroutine` class:class Awaitable(metaclass=ABCMeta): __slots__ = () @abstractmethod def __await__(self): yield @classmethod def __subclasshook__(cls, C): if cls is Awaitable: return _check_methods(C, "__await__") return NotImplementedclass Coroutine(Awaitable): __slots__ = () @abstractmethod def send(self, value): ... @abstractmethod def throw(self, typ, val=None, tb=None): ... def close(self): ... @classmethod def __subclasshook__(cls, C): if cls is Coroutine: return _check_methods(C, '__await__', 'send', 'throw', 'close') return NotImplemented Some convinent functions help: eg. help(Exception) or help(e), to get the methods &amp; data in a class/object dir : eg. dir(Exception) or dir(e), is a simple version of help, shows all members as a string inspect.getmro(type(e)): get the class hierachy of a type Major 3rd-Party LibrariesDjango: The most frequently used web framework written in Python, Django powers websites that include Instagram and Disqus. It has many useful features, and whatever features it lacks are covered by extension packages.CherryPy and Flask are also popular web frameworks.BeautifulSoup is very useful when scraping data from websites, and leads to better results than building your own scraper with regular expressions. A number of third-party modules are available that make it much easier to carry out scientific and mathematical computing with Python.The module matplotlib allows you to create graphs based on data in Python.The module NumPy allows for the use of multidimensional arrays that are much faster than the native Python solution of nested lists. It also contains functions to perform mathematical operations such as matrix transformations on the arrays.The library SciPy contains numerous extensions to the functionality of NumPy. Python can also be used for game development.Usually, it is used as a scripting language for games written in other languages, but it can be used to make games by itself.For 3D games, the library Panda3D can be used. For 2D games, you can use pygame. # IssuesMake class method return self typehttps://stackoverflow.com/questions/33533148/how-do-i-type-hint-a-method-with-the-type-of-the-enclosing-class If you are using Python 3.10 or later, it just works. As of today (2019) in 3.7+ you must turn this feature on using a future statement (from __future__ import annotations) - for Python 3.6 or below use a string. Python 3.7+: from __future__ import annotations12345from __future__ import annotationsclass Position: def __add__(self, other: Position) -&gt; Position: ... Python &lt;3.7: use a string1234class Position: ... def __add__(self, other: 'Position') -&gt; 'Position': ... Circular importcircular import modify the position of import to fix the issues]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s general]]></title>
    <url>%2F2019%2F03%2F01%2Fk8s%2F</url>
    <content type="text"><![CDATA[k8s is a platform to manage containerized workloads and services. Conceptskubernetes Objects Kubernetes abstract a desired state of cluster as objects. an object configuration includes: spec: describe the desired state apiVersion: the api version of kubernetes metadata: the name &amp; namespace spec: the desired state definition. status: describe the actual state of the object cluster state (understanding kubernetes objects): what containerized applications are running and where they’re running how many resources (disk, network, etc.) are attached to the the container the policies around how the application behaves, such as restart policies, fault-tolerance, etc. WorkloadsObjects that set deployment rules of pods. All controllers are workloads. PODpod is a minimize runnable object in k8s object model. A Pod encapsulates an application container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. It represents a single instances of application. There’re two common use cases: Pods that run a single container, which is the most case. So often pod is a synomynous with container. Pods that run multiple containers that need to work together. All the containers in a pod share the storage &amp; network, which means they use the same ip and same storage volume. Pod lifecycleA pod doesn’t self-heal &amp; self-scale. It’s just a running instance. It stopped &amp; deleted when the process is finished, or the node is failed, or the resource is exhaused… Controllers (eg. deployment, statefulSet…) instead can create and manage multiple pods. Pod restarting is different from container restarting. Pod provides the env for containers: os, storage, network… ControllersControllers (eg. deployment, statefulSet…) instead can create and manage multiple pods. ServiceService is an abstraction which defines a logical set of pods, and a policy by which to access the pods. It enables the decoupling of access &amp; the real pods, which means you can access by service name/ip rather than the pod ip, while pod ip is not stable. Q: How does k8s get all pod endpoints by service? A: By LabelSelector. You can define labels for each pod. And we define LabelSelector in service, so that k8s can search pod node by label first, and then using the targetPort to locate the pod on node. kubernetes control plane the control plane manage the cluster state to match the desired state of objects the control plane consists of a collection of processes for the above intention. kubernetes master: is responsible for the maintaining the desired state. “master” in fact refers to three processes: kube-apiserver, kube-controller-manager and kube-scheduler. these three processes are typically run on single node in the cluster. This node is called “master”, too. The master can also be replicated for avaibility and redundancy. kubernetes nodes: the nodes to run applications. there’re two process in each node: kubelet: communicate with the master kube-proxy: a network proxy. kubernetes apiThe api communicates with master to operate on kubernetes objects. kubectl is a cli to implement the above intention. It in fact calls the api internally. All kinds of sdk (java, python…) encapsulate the api, too. There are two kinds of api groups: core groups: the original k8s api other groups: other api to extend the core group, like you may want to abstract more objects? Access services on clusterThere are several levels. ip: pod ip: cluster ip: the virtual ip for a service node ip In-pod accessEach pod has a unique IP. And all containers in a pod share the ip, which means that they can access each other by localhost In-cluster accessBy default, a service can be accessed by other pods in the same cluster, through: cluster-ip:port cluster ip is the virtual ip of a service port is is the node port of the service. Every node on k8s has a kube-proxy. It installs iptable rules which keep a simple record of (servicename:clusterip:serviceport). In some node: call cluster-ip:port————— In a pod the kube-proxy search the iptable, and get some info.———— pod =&gt; kube-proxy of node where the pod resides. the kube-proxy using the info to ask master for the real endpoints.—— kube-proxy of node =&gt; master kube-api the kube-proxy chooses a endpoint by SessionAffinity defined in service (round-robin by default) —— in kube-proxy the kube-proxy redirect request to pod-ip:podPort —— kube-proxy =&gt; pod endpoint kube-proxy: enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding servicename.namespacenameWhen accessing by service name, there’re two ways: environement viaribleswhen create a service, k8s will create some env for each serivce, e.g. {SVCNAME_CAPTIPAL}_SERVICE_HOST. So the kube-proxy in fact gets the service cluster ip from these envs first. When a pod calls a service, the service must be created before the pod so that the envs are created. 12345678# the cluster ipREDIS_MASTER_SERVICE_HOST=10.0.0.11REDIS_MASTER_SERVICE_PORT=6379REDIS_MASTER_PORT=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP_PROTO=tcpREDIS_MASTER_PORT_6379_TCP_PORT=6379REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 DNSIt’s an add-on k8s object, which can be chosed to add to the cluster. It’s an in-cluster dns, which serves DNS records for Kubernetes services. It watches the k8s api for new services and create DNS SVC records for each. Containers started by Kubernetes automatically include this DNS server in their DNS searches. between-cluster access NodePort: expose nodeIP LoadBalancer: expose loadbalancer ip Ingress: expose loadbalancer &amp; path]]></content>
      <tags>
        <tag>container</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[obs]]></title>
    <url>%2F2019%2F03%2F01%2Fobs%2F</url>
    <content type="text"><![CDATA[Huawei Obs is an object storage service on cloud. ConceptsObject The real complete file or byte stream to save object name is the unique id in a bucket it’s used as part of url path. The naming restrictions are fit to url path naming restrictions. Access(based on version in fact) Object ACL: general control to object: read object, read/write object ACL, only users in the same account Object policy fine-grained control to object: fine-grained actions(put,delete…) on object, all users multi-versions an object can has multiple versions, each of which has an unique id. Whether there’s multi-version, it’s a policy set on a bucket. directory: directory is just a view. Essentially, it’s an empty object end with “/“. all objects in a bucket are on the same level. There’s no multi-level directory in fact. to create the directory view, you need to create an object with name ending with “/“ explicility, eg. “sub1/sub2/ . It will create a two-level dir in console. There’s no need to create “sub1/“ first then “sub1/sub2”. object actions: For writing, there’s only write/restricted-append/delete, no put basically-write-once-read-many upload modes: stream file multi-part (support breakpoint resume) append Bucket The place to save objects bucket name is the unique id for one account(a tenant). it’s used as part of domain name on url. The naming restrictions are fit to domain naming restrictions. Access Bucket ACL: general control to bucket and all objects in bucket: read/put buckets, read/write bucket ACL, only users in the same account Bucket policy fine-grained control to specific objects in bucket: fine-grained actions on bucket or specific objects in bucket, all users storage type standard: quick access &amp; high throughput. It’s used for high access requests and not so big files. warm: low access. cold: very very low access regionThe region of nodes where the storage really happens. signatureThe signature to identify a user when accessing buckets/objects. ak(access key): represent a user. one user can have multi aks. It’s kind of like an user role sk(secret key): one-to-one corresponding with ak. The secret key used for RSA authentication &amp; authorization.]]></content>
      <tags>
        <tag>cloud</tag>
        <tag>object storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline process: beam]]></title>
    <url>%2F2019%2F01%2F30%2Fpipeline-process-beam%2F</url>
    <content type="text"><![CDATA[What’s beambeam is a open-source, unified model for defining both batched &amp; streaming data-parallel processing pipelines. open-source (apache v2 license) to define data-parallel processing pipelines an unified model to define pipelines. The real processing is run by the underlying runner (eg. spark, apache apex, etc.). all available runners can process both batched (bounded datasets) &amp; streaming (unbounded datasets) datasets Use itSee the wordcount examples, wordcount src Now we define a simple pipeline and run it. Transform, Count are all built-in atom operations to define the pipeline scripts. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package org.apache.beam.examples;import java.util.Arrays;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.Filter;import org.apache.beam.sdk.transforms.FlatMapElements;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.KV;import org.apache.beam.sdk.values.TypeDescriptors;public class MinimalWordCount &#123; public static void main(String[] args) &#123; // Create a PipelineOptions object. This object lets us set various execution // options for our pipeline, such as the runner you wish to use. PipelineOptions options = PipelineOptionsFactory.create(); // Create the Pipeline object with the options we defined above Pipeline p = Pipeline.create(options); // Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/*")) // Concept #2: Apply a FlatMapElements transform the PCollection of text lines. .apply( FlatMapElements.into(TypeDescriptors.strings()) .via((String word) -&gt; Arrays.asList(word.split("[^\\p&#123;L&#125;]+")))) .apply(Filter.by((String word) -&gt; !word.isEmpty())) // Concept #3: Apply the Count transform to our PCollection of individual words. .apply(Count.perElement()) .apply( MapElements.into(TypeDescriptors.strings()) .via( (KV&lt;String, Long&gt; wordCount) -&gt; wordCount.getKey() + ": " + wordCount.getValue())) // Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline. .apply(TextIO.write().to("wordcounts")); p.run().waitUntilFinish(); &#125;&#125; Some conceptionsI/O (data source/target)Beam can process both batched (bounded datasets) &amp; streaming (unbounded datasets) datasets. built-in io transforms Take reading as example, you specify the file location (the location must be accessable for the runner), and then the reader pull from datasource. You may also define the trigger to collect input window. When trigger is satisfied, window elements are emitted. For unbounded datasets, they are split into windows. And each window is again a bounded datasets. In each window, there’re some elements. You can define how the elements are grouped as a window and when to emit the window elements for processing. window concept RunnerBeam is an unified model. It abstracts the conception to define and run a pipeline. The real execution is conducted by the underlying runners. all available runners For unbounded datasets, the underlying runner must support stream processing.]]></content>
      <tags>
        <tag>bigdata</tag>
        <tag>distributed processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux commands]]></title>
    <url>%2F2019%2F01%2F23%2Flinux-command%2F</url>
    <content type="text"><![CDATA[chmod, chownunderstanding linux file permissions File permissions are defined by permission group and permission type permission group owner(u) group(g) all other users(a) permission type read (r - 4) write(w - 2) execute(x - 1) permission presentationThe permission in the command line is displayed as _rwxrwxrwx 1 owner:group the first character (underscore _ here) is the special permission flag that can vary. the following three groups of rwx represent permission of owner, group and all other users respectively. If the owner and all users has no read permission, it is __wxrwx_wx follwing that grouping since the integer displays the number of hardlinks to the file the last piece is the owner and group assignment. special permission flagThe special permission flag can be: _: no special permissions d: directory l: the file or dir is a symbolic link s: This indicated the setuid/setgid permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a s in the read portion of the owner or group permissions. t: This indicates the sticky bit permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a t in the executable portion of the all users permissions permission modification1234567891011# grant read and write permissions to the user and group$ chmod ug+rw file1# remove read and write permissions to the user and group$ chmod ug-rw file1# set permission using binary references (owner: rwx = 4+2+1, group: rx = 4+1, all users: rx = 4+1)$ chmod 755 file1# change the file permission recursively in the file/dir instead of just the files themselves$ chmod -R 755 dir1 change owner:group assignments12# change the owner of file1 to user1 and group to family$ chown user1:family file1 find12# find all xml files from the current dir$ find ./* -name '*.xml' To find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories: 1$ find /directory_path -mtime -1 -ls Should be to your liking The - before 1 is important - it means anything changed one day or less ago. A + before 1 would instead mean anything changed at least one day ago, while having nothing before the 1 would have meant it was changed exacted one day ago, no more, no less. Another, more humane way: 1find /&lt;directory&gt; -newermt "-24 hours" -ls or: 1find /&lt;directory&gt; -newermt "1 day ago" -ls or: 1find /&lt;directory&gt; -newermt "yesterday" -ls ls12# list file with creation date and sort by it$ ls -lct du1$ du -sh -- * | sort -hr List users1$ cat /etc/passwd | cut -d: -f1 pbcopy12# copy file content to clipboard$ pbcopy &lt; test.txt dstat1$ dstat -t -a --tcp --output network.log]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lombok]]></title>
    <url>%2F2019%2F01%2F17%2Flombok%2F</url>
    <content type="text"><![CDATA[lombok is a library to help your write java cleaner and more efficiently. It’s plugged into the editor and build tool, which works at compile time. Essentially, it modifies the byte-codes by operating AST (abstract semantic tree) at compile time, which is allowed by javac. This is, in fact, a way to modify java grammar. UsageTo use it, install lombok plugin in intellij add package dependency in project (to use its annotations) 123456&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; See all the annotations. Give some example here: @Data@Data bundles the features of @ToString, @EqualsAndHashCode, @Getter / @Setter and @RequiredArgsConstructor together. With lombok: 123456789101112131415161718import lombok.AccessLevel;import lombok.Setter;import lombok.Data;import lombok.ToString;@Data public class DataExample &#123; private final String name; @Setter(AccessLevel.PACKAGE) private int age; private double score; private String[] tags; @ToString(includeFieldNames=true) @Data(staticConstructor="of") public static class Exercise&lt;T&gt; &#123; private final String name; private final T value; &#125;&#125; Vanila java: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import java.util.Arrays;public class DataExample &#123; private final String name; private int age; private double score; private String[] tags; public DataExample(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125; void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return this.age; &#125; public void setScore(double score) &#123; this.score = score; &#125; public double getScore() &#123; return this.score; &#125; public String[] getTags() &#123; return this.tags; &#125; public void setTags(String[] tags) &#123; this.tags = tags; &#125; @Override public String toString() &#123; return "DataExample(" + this.getName() + ", " + this.getAge() + ", " + this.getScore() + ", " + Arrays.deepToString(this.getTags()) + ")"; &#125; protected boolean canEqual(Object other) &#123; return other instanceof DataExample; &#125; @Override public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof DataExample)) return false; DataExample other = (DataExample) o; if (!other.canEqual((Object)this)) return false; if (this.getName() == null ? other.getName() != null : !this.getName().equals(other.getName())) return false; if (this.getAge() != other.getAge()) return false; if (Double.compare(this.getScore(), other.getScore()) != 0) return false; if (!Arrays.deepEquals(this.getTags(), other.getTags())) return false; return true; &#125; @Override public int hashCode() &#123; final int PRIME = 59; int result = 1; final long temp1 = Double.doubleToLongBits(this.getScore()); result = (result*PRIME) + (this.getName() == null ? 43 : this.getName().hashCode()); result = (result*PRIME) + this.getAge(); result = (result*PRIME) + (int)(temp1 ^ (temp1 &gt;&gt;&gt; 32)); result = (result*PRIME) + Arrays.deepHashCode(this.getTags()); return result; &#125; public static class Exercise&lt;T&gt; &#123; private final String name; private final T value; private Exercise(String name, T value) &#123; this.name = name; this.value = value; &#125; public static &lt;T&gt; Exercise&lt;T&gt; of(String name, T value) &#123; return new Exercise&lt;T&gt;(name, value); &#125; public String getName() &#123; return this.name; &#125; public T getValue() &#123; return this.value; &#125; @Override public String toString() &#123; return "Exercise(name=" + this.getName() + ", value=" + this.getValue() + ")"; &#125; protected boolean canEqual(Object other) &#123; return other instanceof Exercise; &#125; @Override public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof Exercise)) return false; Exercise&lt;?&gt; other = (Exercise&lt;?&gt;) o; if (!other.canEqual((Object)this)) return false; if (this.getName() == null ? other.getValue() != null : !this.getName().equals(other.getName())) return false; if (this.getValue() == null ? other.getValue() != null : !this.getValue().equals(other.getValue())) return false; return true; &#125; @Override public int hashCode() &#123; final int PRIME = 59; int result = 1; result = (result*PRIME) + (this.getName() == null ? 43 : this.getName().hashCode()); result = (result*PRIME) + (this.getValue() == null ? 43 : this.getValue().hashCode()); return result; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[automatic drive]]></title>
    <url>%2F2019%2F01%2F15%2Fautomatic-drive%2F</url>
    <content type="text"><![CDATA[reference: coco: one format for data labelling]]></content>
      <tags>
        <tag>automatic drive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark]]></title>
    <url>%2F2019%2F01%2F10%2Fspark%2F</url>
    <content type="text"><![CDATA[Conceptspark is a fast and general-purpose cluster computing system like Hadoop Map-reduce. It runs on the clusters. Spark EcosystemThe components of Apache Spark Ecosystem spark core: cluster computing system. Provide API to write computing functions. Spark SQL. SQL for data processing, like hive? MLlib for machine learning. GraphX for graph processing Spark Streaming. Spark CoreSpark Core is the fundamental unit of the whole Spark project. Its key features are: It is in charge of essential I/O functionalities. Provide API to defines and manipulate the RDDs. Significant in programming and observing the role of the Spark cluster. Task dispatching, scheduling Fault recovery. It overcomes the snag of MapReduce by using in-memory computation. Spark makes use of Special data structure known as RDD (Resilient Distributed Dataset). Spark Core is distributed execution engine with all the functionality attached on its top. For example, MLlib, SparkSQL, GraphX, Spark Streaming. Thus, allows diverse workload on single platform. All the basic functionality of Apache Spark Like in-memory computation, fault tolerance, memory management, monitoring, task scheduling is provided by Spark Core.Apart from this Spark also provides the basic connectivity with the data sources. For example, HBase, Amazon S3, HDFS etc. Action, Job, Stage, TaskActions are RDD’s operation. reduce, collect, takeSample, take, first, saveAsTextfile, saveAsSequenceFile, countByKey, foreach are common actions in Apache spark. In a Spark application, when you invoke an action on RDD, a job is created. Jobs are the main function that has to be done and is submitted to Spark. The jobs are divided into stages depending on how they can be separately carried out (mainly on shuffle boundaries). Then, these stages are divided into tasks. Tasks are the smallest unit of work that has to be done the executor. When you call collect() on an RDD or Dataset, the whole data is sent to the Driver. This is why you should be careful when calling collect(). An example: What is Spark Job ? let’s say you need to do the following: Load a file with people names and addresses into RDD1 Load a file with people names and phones into RDD2 Join RDD1 and RDD2 by name, to get RDD3 Map on RDD3 to get a nice HTML presentation card for each person as RDD4 Save RDD4 to file. Map RDD1 to extract zipcodes from the addresses to get RDD5 Aggregate on RDD5 to get a count of how many people live on each zipcode as RDD6 Collect RDD6 and prints these stats to the stdout. So, The driver program\ is this entire piece of code, running all 8 steps. Producing the entire HTML card set on step 5 is a job\ (clear because we are using the save action, not a transformation). Same with the collect on step 8 Other steps will be organized into stages\, with each job being the result of a sequence of stages. For simple things a job can have a single stage, but the need to repartition data (for instance, the join on step 3) or anything that breaks the locality of the data usually causes more stages to appear. You can think of stages as computations that produce intermediate results, which can in fact be persisted. For instance, we can persist RDD1 since we’ll be using it more than once, avoiding recomputation. All 3 above basically talk about how the logic of a given algorithm will be broken. In contrast, a task\ is a particular piece of data that will go through a given stage, on a given executor. RDDRDD 数据模型 属性名 成员类型 属性含义 dependencies 变量 生成该RDD所依赖的父RDD compute 方法 生成该RDD的计算接口 partitions 变量 该RDD的所有数据分片实体 partitioner 方法 划分数据分片的规则 preferredLocations 变量 数据分片的物理位置偏好 SparkContextSparkContext is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. If you want to create SparkContext, first SparkConf should be made. The SparkConf has a configuration parameter that our Spark driver application will pass to SparkContext. Some of these parameter defines properties of Spark driver application. While some are used by Spark to allocate resources on the cluster, like the number, memory size, and cores used by executor running on the worker nodes.In short, it guides how to access the Spark cluster. After the creation of a SparkContext object, we can invoke functions such as textFile, sequenceFile, parallelize etc.Once the SparkContext is created, it can be used to create RDDs, broadcast variable, and accumulator, ingress Spark service and run jobs. All these things can be carried out until SparkContext is stopped. 12345678910111213141516171819202122232425262728293031from __future__ import print_functionimport sysfrom operator import addfrom pyspark.sql import SparkSessionif __name__ == "__main__": if len(sys.argv) != 2: print("Usage: wordcount &lt;file&gt;", file=sys.stderr) sys.exit(-1) # the builder here defines the sparkConf, and then create a sparkSession with an underlying SparkContext `spark.sparkContext` spark = SparkSession\ .builder\ .appName("PythonWordCount")\ .getOrCreate() # here by `spark.read.text('some.txt')`, we use SparkContext create an DataFrame lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]) counts = lines.flatMap(lambda x: x.split(' ')) \ .map(lambda x: (x, 1)) \ .reduceByKey(add) # this is the spark action `collect` output = counts.collect() for (word, count) in output: print("%s: %i" % (word, count)) # this in fact stop the sparkContext spark.stop() How does it runSpark core contains the main api, driver engine, scheduler… to support the cluster computing. The real computing is completed on the cluster. Spark can connect to many cluster managers(spark’s own standalone cluster manager, mesos, yarn) to complete the jobs. Typically, the process is like this: The user submits a spark application using the spark-submit command. Spark-submit launches the driver program on the same node in (client mode) or on the cluster (cluster mode) and invokes the main method specified by the user. The driver program contacts the cluster manager to ask for resources to launch executor JVMs based on the configuration parameters supplied. The cluster manager launches executor JVMs on worker nodes. The driver process scans through the user application. Based on the RDD actions and transformations in the program, Spark creates an operator graph. When an action (such as collect) is called, the graph is submitted to a DAG scheduler. The DAG scheduler divides the operator graph into stages. A stage comprises tasks based on partitions of the input data. The driver sends work to executors in the form of tasks. The executors process the task and the result sends back to the driver through the cluster manager. APIYou can write spark function (eg. map function, reduce funciton) using Java/scala/python/R API. See api docs. Installation On YarnSee run spark on Yarn, Install, Configure, and Run Spark on Top of a Hadoop YARN Cluster downloads page download the spark tar -xvf spark-xxx.tgz configuration config in /conf/spark-env.sh 1234# config this to specify the installed HADOOP pathexport SPARK_DIST_CLASSPATH=$HADOOP_HOME/binexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop config in /conf/spark-default.conf. (all configuration properties) 12345# config the spark masterspark.master yarnspark.driver.memory 512mspark.yarn.am.memory 512mspark.executor.memory 512m history server configWhen the spark job is running, you can access the job log by localhost:4040. When the job is finished, by default, the log is not persisted which means you can’t access it. To access the logs later, need to config the following: (see spark Monitoring and Instrumentation and using history server to replace the spark web ui) 123456789101112131415# in /conf/spark-default.conf# config history serverspark.ui.filters org.apache.spark.deploy.yarn.YarnProxyRedirectFilter# tell spark use history server url as the trackint urlspark.yarn.historyServer.allowTracking true# enable log persistencespark.eventLog.enabled true# log write dir. Here use the hdfs dir and you must create the dir in hdfs firstspark.eventLog.dir hdfs://localhost:9000/spark-logs# log read dir. Sometimes logs are transfered.spark.history.fs.logDirectory hdfs://localhost:9000/spark-logs example execution Start histroy server: sbin/start-history-server.sh execute spark job: 12345678$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode client \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ examples/jars/spark-examples*.jar \ 10 Then you can: Check the job/application info in yarn: http://localhost:8088/cluster/apps Check the job/application using Spark history server: http://localhost:18080/ Glossaryglossary NoteThe following table summarizes terms you’ll see used to refer to cluster concepts: Term Meaning Application User program built on Spark. Consists of a driver program and executors on the cluster. Application jar A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime. Driver program The process running the main() function of the application and creating the SparkContext Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Worker node Any node that can run application code in the cluster Executor A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. Task A unit of work that will be sent to one executor Job A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. Deploy modeyarn-client vs yarn-clusteryarn-client vs yarn-cluster 深度剖析 stackoverflow spark-shell vs spark-submitSpark shell is only intended to be use for testing and perhaps development of small applications - is only an interactive shell and should not be use to run production spark applications. For production application deployment you should use spark-submit. The last one will also allow you to run applications in yarn-cluster mode Spark DataFrameauto increment idtwo ways for auto increment id Row_number123456789/** * 设置窗口函数的分区以及排序，因为是全局排序而不是分组排序，所有分区依据为空 * 排序规则没有特殊要求也可以随意填写 */val spec = Window.partitionBy().orderBy($"lon")val df1 = dataframe.withColumn("id", row_number().over(spec))df1.show() rdd.zipWithIndex123456789101112// 在原Schema信息的基础上添加一列 “id”信息 val schema: StructType = dataframe.schema.add(StructField("id", LongType)) // DataFrame转RDD 然后调用 zipWithIndex val dfRDD: RDD[(Row, Long)] = dataframe.rdd.zipWithIndex() val rowRDD: RDD[Row] = dfRDD.map(tp =&gt; Row.merge(tp._1, Row(tp._2))) // 将添加了索引的RDD 转化为DataFrame val df2 = spark.createDataFrame(rowRDD, schema) df2.show() Add constant columnadd constant column 12345678import org.apache.spark.sql.functions.typedLitdf.withColumn("some_array", typedLit(Seq(1, 2, 3)))df.withColumn("some_struct", typedLit(("foo", 1, 0.3)))df.withColumn("some_map", typedLit(Map("key1" -&gt; 1, "key2" -&gt; 2)))from pyspark.sql.functions import litdf.withColumn('new_column', lit(10)) select latest recordstackoverflow Hive Hintshive hints Optimizationdeep dive - spark optimization performance tuning Get Baseline 利用 spark-ui 观察任务运行情况（long stages，spill，laggard tasks, etc.） 利用 yarn 等观察资源利用情况（CPU 利用率 etc.） Memory spillSpark 运行时会分配一定的 memory（可以指定资源需求)， 分 storage 和 working memory。 storage memory 是 persist 会用的 memory。当调用 persist（或 cache，一种使用 StorageLevel.MemoryAndDisk 的 persist）时，如果指定的 storage_level 有 memory，那么就会将数据存到 memory。 working memory 是 spark 运算所需要的 memory，这个大小是动态变化的。当 storage memory 占用过多内存时，working memory 就不够了。然后就会有 spill，就会慢。 memory spill 表示 working memory 不够，spark 开始使用 disk。而 disk 的 I/O 效率是极低的。所以一旦出现 spill，性能就会大大降低。 working memory 不够有很多原因： Memory 资源申请的太少了，就是不够 ====》 增加 spark.executor.memory 数据在 memory/disk 的存储一般是 serialized，以节省空间。但数据 load 到 working memory 时，一般都是 deserialized 的，处理更快，但是更占空间。 资源可以了，partition 太少，每个 partition 处理的数据太多，所以 spill 了 ====》 增加 shuffle partition 有不均衡出现，导致某些 task 处理的数据尤其多 ====》see balance 有太多 persist，持久化了太多东西，占用过多的 storage memory ====》see persistence 指定资源需求Spark-submit 运行时，可通过指定以下参数来定义运行所需的资源： --conf spark.num.executors=xx (或 --num-executors xx)：指定运行时需要几个 executor（也可以通过 dynamic allocation 来根据运算动态分配 executors） --conf spark.executor.memory=xxG（或 --executor-memory xxG）：指定每个 executor 所需要的内存 --conf spark.executor.cores=xx（或 --executor-cores xx）：指定每个 executor 所需要的 cores --conf spark.driver.memory=xxG（或 --driver-memory xxG）：指定每个 driver 所需要的内存。当执行 df.collect()时，会将数据 collect 到 driver，此时就需要 driver 有很多的 memory --conf spark.driver.cores=xx（或 --driver-cores xx）：指定每个 driver 所需要的 cores Some issues–executor-cores settinng not working需要配置 yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator，因为默认的使用的是 DefaultResourceCalculator，它只看 memory(–executor-memory)，DominantResourceCalculator 则同时考虑 cpu 和 memory see stackoverflow –spark.dynamicAllocation.maxExecutors not working这个需要和其他配置配合使用 spark.dynamicAllocation.enabled = trueThis requires spark.shuffle.service.enabled or spark.dynamicAllocation.shuffleTracking.enabled to be set. The following configurations are also relevant: spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorAllocationRatio 如果还不工作，可能要按 [spark dynamic allocation not working](https://community.cloudera.com/t5/Support-Questions/Spark-dynamic-allocation-dont-work/td-p/140227 设置各 nodemanager 并重启 See spark dynamic allocation Partitions接下来从输入、运行、输出三个阶段的 partition 优化来看 一般 1 partition -&gt; 1 task，分多少个 partition，就拆多少个 task 来运行。 Avoid the spills Maximize parallelism utilize all cores provision only the cores you need 输入（input）12spark.default.parallelism (don&#39;t use)spark.sql.files.maxPartitionBytes (mutable，控制每个 partition 读的文件大小) Shuffle12spark.sql.shuffle.partitions（控制使用多少个 partition 来 shuffle）spark.default.parallelism（控制 rdd 的 partition 数目？？？？？？） 如果配置了 spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#39;true&#39;) 或 spark.sql.adaptive.coalescePartitions.enabled ，它会动态控制 parition count （参见 coalescing post shuffle partitions），根据 shuffle 数据大小来动态设置 partition 数目。但是这个设置可能不合理，因为 shuffle 过程中，最终操作的数据可能远大于 shuffle read 的大小，这个过程中存在 deserialize 等。如果配置了动态控制，依然出现了 shuffle spill，那么可以先关掉这个配置，手动控制 shuffle partitions 大小。 输出（output）1234coalescerepartitionrepartition(range) &#x3D;&#x3D;&#x3D;&gt; range partitioner???df.localCheckPoint().repartition().... &#x3D;&#x3D;&gt; how to use tis BalanceWhen some partitions are significantly larger than most, there is skew. Balance 体现在很多方面：网络、GC、数据，当然最常见的问题是数据的不均匀。 通过查看 spark ui 可以看到不均匀的任务（这个时候需要停掉重跑）： 查看 staggling tasks 查看 stage 执行进度：stage 里剩余几个 task 执行特别慢，这个时候各个 task 处理的数据肯定存在不均匀，导致那几个 task 处理的尤其慢 查看 stage 执行 metric：大部分时候没有 spill，但是 max 的时候有 spill；或者大部分的时候 read size 和 max read size 有很大差别 查看 stage 里各个节点的 GC time，GC time 分布不均匀，也是有问题的（什么问题？？） Persistence当 execution plan 中，有些 superset 被多个 subset 所使用，superset 计算复杂、耗时久，这个时候就可以选择将 superset persist，从而避免重复运算。 spark core 中有几个概念，其中只有 action 会触发一次 dag 的运行。同一段代码，可能会生成不同的 dag，每次都需要执行。所以如果被多次使用的 superset，最好将它 cache，避免后续的重复运算。 persist/cache 要慎用，因为： 占资源。当 persist 消耗了太多的 storage memory 时，就会出现 memory spill 也有时间损耗（serialize, deserialize, I/O)。persist 一般都以 serialized 的形式存储，节省空间，而 load 到 working memory 时，又需要 deserialiize In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, DISK_ONLY_2, and DISK_ONLY_3.* cacheCache 是选择 default 的 persist。persist 可以选择不同的 persistence storage level With cache(), you use only the default storage level : MEMORY_ONLY for RDD MEMORY_AND_DISK for Dataset With persist(), you can specify which storage level you want for both RDD and Dataset. From the official docs: You can mark an RDD to be persisted using the persist() or cache() methods on it. each persisted RDD can be stored using a different storage level The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). Use persist() if you want to assign a storage level other than : MEMORY_ONLY to the RDD or MEMORY_AND_DISK for Dataset 1234spark.catalog.cacheTable("tableName")spark.catalog.uncacheTable("tableName")dataFrame.cache() broadcast joinspark 执行 map-join 优化 spark broadcast join 几种方式： 1. spark 自动识别小表 broadcastspark.sql.statistics.fallBackToHdfs=True, 这样它会直接分析文件的大小，而不是 metastore 数据 2. 使用 hinthive hints 1select /*+ BROADCAST (b) */ * from a where id not in (select id from b) 3. 使用 dataframe api12from pyspark.sql.functions import broadcastbroadcast(spark.table("b")).join(spark.table("a"), "id").show() cache vs broadcastcache vs broadcast RDDs are divided into partitions. These partitions themselves act as an immutable subset of the entire RDD. When Spark executes each stage of the graph, each partition gets sent to a worker which operates on the subset of the data. In turn, each worker can cache the data if the RDD needs to be re-iterated. Broadcast variables are used to send some immutable state once to each worker. You use them when you want a local copy of a variable. These two operations are quite different from each other, and each one represents a solution to a different problem. 小文件问题spark-sql 优化小文件过多 On Spark, Hive, and Small Files: An In-Depth Look at Spark Partitioning Strategies 为什么会有小文件？当 spark 要 write 到 hive 表时，这实际也是一个 shuffle stage，就会分很多个 sPartition (spark partition)。每个 sPartition 在处理时，都会生成一个文件（如果是动态分区，则更严重，因为每个 sPartition 的数据分布式均匀的，每个 sPartition 可能包含很多个 hive paritition key，spark 每遇到一个 partition key 就生成一个文件），那么 sPartition 数目越多（动态分区的情况下，会更不可控），文件数就会越多。 简单来说，就是 spark 的一个 stage 分成了很多个 task（shuffle partitions 控制这个数量），即 sPartition，每个 sPartition 可能对应多个 hPartitiion（hive partition）key，多个 sPartition 也对应一个 hPartition key。而每个 sPartition 里对应的每个 hPartition key，都会生成一个文件。 那么，如果一个 sPartition 和 hPartition 只是一个 多（可控数目，对应最后每个 hPartitiion 的文件数）对一 的情况，那么文件数就是可控的。 使用 hive 时，不会有小文件问题。hive 里只需要设置下边的这些参数，就 In pure Hive pipelines, there are configurations provided to automatically collect results into reasonably sized files, nearly transparently from the perspective of the developer, such as hive.merge.smallfiles.avgsize, or hive.merge.size.per.task. 解决方案 coalesce repartition Distribute by adaptive execution Adaptive Execution 让 Spark SQL 更高效更智能 1234# 启用 Adaptive Execution ，从而启用自动设置 Shuffle Reducer 特性spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#39;true&#39;)# 设置每个 Reducer 读取的目标数据量，单位为字节。默认64M，一般改成集群块大小spark.conf.set(&quot;spark.sql.adaptive.shuffle.targetPostShuffleInputSize&quot;, &#39;134217728&#39;) python udf vs scala udfpython udf vs scala udf IssuesNull-aware predicate sub-queries cannot be used in nested conditionsnot in 不能和 or 之类的 condition 一块用。现在好像还没有修复，参见：SPARK-25154]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed computing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn]]></title>
    <url>%2F2019%2F01%2F09%2Fyarn%2F</url>
    <content type="text"><![CDATA[yarn architectureYarn is used to manage/allocate cluster resource &amp; schedule/moniter jobs. These parts – resource manager – are split up from hadoop framework. Yarn has two main components: Schedular: manage resources (cpu, memory, network, disk, etc.) and allocate it the applications. node manager will tell Schedular the node resource info (node status) application master will ask Schedular for resources. When partitioning resources among various queues, applications, Schedular supports pluggable policies. For example: CapacityScheduler allocate resources by tenant request. It’s used especially for multi-tenant scenario, designed to allow sharing a large cluster while giving each organization capacity guarantees. Each client/tenant can request any resources that are not used by others. And there’s strict ACLs to ensure the security of resources between tenants. The primary abstraction is queue. Different tenant use different queue to utilize the resources. And hierachical queues are provided to support data separation in one tenant. FairScheduler assigning resources to appliations such that all apps get, on average, an equal shares of resources over time. It’s mainly designed to share cluster between a number of users. It lets short apps are completed in a reasonable time while not starving long-lived apps. (resources might free up when new apps are submitted). ApplicationManager: accept job-submisons, negotiate to exeuct application masters, and moniter reboot app master when failure. AppMaster are the one who apply to Schedular for resources boot up job execution moniter the job execution status tell app manager if the job fails or succeeds. Other components: ReservationSystem: reserve some resources to ensure the predictable execution of important jobs. YARN Federation: join clusters to scale and allow multiple independent clusters. an exampleTake hive as example: load data – non-distributed-computing jobs user uses hive command to load data into hive table. (eg. LOAD DATA LOCAL INPATH &#39;/path/to/datafile/&#39; OVERWRITE INTO TABLE table_name;) hive calls hdfs to write data. node inform schedular the new node status. query data – distributed computing jobs user uses hive command to query data (eg. select count(*) from xxx) hive submits a map-reduce job to appliction manager application manager applies to Schedular (??? not sure) for a container to execute application master and boots it. application master applies to Schedular for resoures to excute map-reduce job and boots the job. the map-reduce job get input data from hdfs, and write output data into hdfs the map-reduce job informs the application master the status of the job. application manager will restart application master on failure (application failure/hardware failure). (when application failed, the job informs the app master, and app manager knows it, and then reboot it) JobHistoryServerOn YARN all applications page, here’s a link to job history. However, you must config to make it take effect. Follow the instructions config of johhistory in hadoop. Also, see Hadoop Cluster Setup to get info about starting log and jobhistory server. See History Server Rest API, JobHistoryServer javadoc Notes: The host of mapreduce.jobhistory.webapp.address and mapreduce.jobhistory.address may need to be set as the real ip (get from ipconfig getifaddr en0) or some other host (eg. cncherish.local) instead of localhost. When start history server, you can see the start host in the log. It may look like this: 1234STARTUP_MSG: Starting JobHistoryServerSTARTUP_MSG: host &#x3D; CNcherish.local&#x2F;192.168.xx.xxxSTARTUP_MSG: args &#x3D; []STARTUP_MSG: version &#x3D; 3.1.1 This might be because the JobHistoryServer told yarn web proxy that its host is ‘cncherish.local/192.168.xx.xxx’ (mapping host ‘cncherish.local’ to the real ip ‘192.168.xx.xxx’), while yarn knows that history host for map-reduce job is ‘localhost’ from mapred-site.xml — the config for map-reduced jobs. The incompatible info reduce the jobhistory link is unreachable. 1.add the following properties into the mapred-site.xml (config the map-reduce framework) 123456789101112131415161718192021222324252627282930&lt;!-- config to persist the jobhistory logs in hdfs --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.cleaner.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;&lt;/description&gt;&lt;/property&gt;&lt;!-- 设置jobhistoryserver 没有配置的话 history入口不可用 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;192.168.x.xxx:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置web端口 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;192.168.x.xxx:19888&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置正在运行中的日志在hdfs上的存放路径 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;/history/done_intermediate&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置运行过的日志存放在hdfs上的存放路径 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;/history/done&lt;/value&gt;&lt;/property&gt; 2.add the following properties into the yarn-site.xml (config the yarn — resource manager) 1234&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 3.start the historyserver 1234567# The following command will run server as a background daemon$ mapred --daemon start historyserver# The following command will run server on the current terminal.# In this way, you can know how the server is started, stopped and what it does.# Also you can know the real server host from the log, which should be aligned by the mapred-site.xml$ mapred historyserver rest api yarn rest api: throught postman http://localhost:8088/ws/v1/cluster/apps (get all the apps) history rest api: http://cnpzzheng.local:19888/ws/v1/history (get server info) use 19888 instead of 10020 CheatSheetlink 123456789101112131415# 查看正在运行的程序资源使用情况$ yarn top$ yarn node -all -list# 查看指定queue使用情况$ yarn queue -status root.users.xxx$ yarn application -movetoqueue application_1528080031923_0067 -queue root.users.xxx$ yarn application -list -appStates [ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING,FINISHED,FAILED,KILLED]$ yarn application -list -appTypes [SUBMITTED, ACCEPTED, RUNNING]$ yarn applicationattempt -list application_1528080031923_0064$ yarn application -kill application_1528080031923_0067$ yarn logs -applicationId application_1528080031923_0064]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed computing</tag>
        <tag>resource manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop]]></title>
    <url>%2F2019%2F01%2F07%2Fhadoop%2F</url>
    <content type="text"><![CDATA[Hadoop is a framework of distributed storage &amp; computing. distributed storage: hadoop use HDFS to save large amount of data in cluster. distributed computing: hadoop use map-reduce framework to conduct fast data analysis (query &amp; writing) over data in HDFS. resource manager &amp; job schedular: hadoop use yarn to manage/allocate cluster resources (memory, cpu, etc.) and to schedule and moniter job executing. Architecturecluster architecture request processing Fault ToleranceUse rack aware so that your replicas will be saved into different racks, which can solve the rack failure issue. Each data node will send heartbeat and block report to the namenode. Thus when data node fails, the name node knows it and will re-replicated to 3. High AvailabilityHigh Availability: Percentage of Uptime of the system. Fault Tolerance, on the other hand, mainly focus on the data loss / system un-recovered damage tolerance. For example, a name-node failure can be processed by reboot from the aspect of fault-tolerance, while there must be a quick working solution from the aspect of high availability. Name Node FailureFor a name node failure, we want to switch to a standby name node with all the informations quickly. How? A name node saves the file namespaces in memory, besides, it also saved editlog for each change into the disk. A name node failure will lose the in-memory fsImage, but we can reproduce the fsImage from the editlogs A common solution is to use QJM to save the editlogs. And the standby name node will read from the editlogs to rebuild the fsImage. Besides, there’s two failover controllers on each name node and a zookeeper. ZooKeeper keeps a lock, and both name nodes are requesting the lock. When the active name node fails, it lost the lock, and the standby nn will acquire the lock. Name Node RebootWhat if you just want to reboot the name node, and since the fsImage is in memory, it will be gone at once and it takes a long time to rebuild from the editlogs? The main issue here is that the fsImage is in memory. Thus to reboot quickly, we need to save the fsImage into disk. The secondary name node is for this. It periodically merge the old fsImage with the editlogs, and replace the old fsImage in the disk, and then truncate the logs. Secondary Name Node is not necessary. If needed, you can build it on the standby nn. install hadoop on macsee default ports used by hadoop services 3.1.0 when config password-free login by ssh, it may only work when generate key into id_rsa/id_dsa. The other user defind key file name won’t work. access hdfs hdfs default ports are changed. see hdfs issue, or check the dfs.namenode.http-address property in hdfs-default.xml for the newest setting. Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 – &gt;9864 When running the example, it seems that jar can only search files. Thus you need to ensure there’s no sub-dirs in search dir. access yarn Access resource manager through localhost:8088. Or check the property yarn.resourcemanager.webapp.address in yarn-default.xml for the newest configuration start hadoop locallymap reduce is a framework to write applications which process vast amounts of data in-parallel on large clusters. A map-reduce job usually splits the input data into independent chunks, and map in a parallel manner. Then the frameworks sorts the output and then reduce to the integrate output. 1234567891011# initialize the namenode$ hdfs namenode -format# start namenode and datanode daemon (access namenode at localhost:9870)$ start-dfs.sh# start ResourceManager &amp; NodeManager daemon (access yarn at localhost:8088)$ start-yarn.sh# stop namenode and datanode daemon$ stop-dfs.sh# stop ResourceManager &amp; NodeManager daemon$ stop-yarn.sh Note: The hdfs namenode -format command must be executed everytime you restarted your computer. And it’s initialized again. Need to figure out other ways to avoid this. There are other commands used to start these daemon: 123456# deprecated to use the above$ start-all.sh# used on specific node (eg. when a new node is added into the cluster, execute on that node)$ hadoop-daemon.sh start datanode/namenode$ yarn-daemon.sh start resourcemanager]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed storage</tag>
        <tag>distributed computing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs]]></title>
    <url>%2F2019%2F01%2F07%2Fhdfs%2F</url>
    <content type="text"><![CDATA[hdfs architectureHDFS 集群以 master-slave 模型运行。其中有两种节点： namenode: master node. know where the files are to find in hdfs datanode: slave node: have the data of the files namenode参见 namenode and datanode Namenode 管理着文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块 (block) 所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。 每个 file 有多个 block 构成，这些 block 分散的存储在各个 datanode 上（并且根据 replication factor，有冗余副本），而 namenode 知道如何一个 file 有哪些 block (file 的元数据信息)，根据 datanode 发送给它的 block 列表，namenode 就可以构建每个文件中各个 block 的位置信息。即根据文件元数据 + datanode block 列表，可以重建文件 block 位置信息，因此不需要持久化。 容错机制由于 datanode 只是分布式地存储 block，不知道这些 block 是怎么组织成文件，以及文件是怎么组织成文件树的。因此 namenode 一旦当掉，整个文件系统就挂了（没办法写和查文件）。因此 namenode 的容错机制很重要。常见的方式： 同步备份。即 namenode 中需要持久化存储的镜像文件和log，同步地持久化存储到其他文件系统中。 secondary namenode (异步)。secondary namenode 一般定期地去同步本地 namenode 的镜像和 log。但除此之外，secondary namenode 还有其他用途，比如合并镜像和log（避免文件过大），这个合并过程很占用 cpu 和内存，所以正好在 secondary namenode 上做。合并完后，在 secondary namenode 上也保存一份。不过这种备份恢复会丢掉一部分数据。 datanodedatanode 根据客户端或者 namenode 调度存储/检索数据，并定期向 namenode 发送它们所存储的 block 列表。 commandshdfs namenode -formatstackoverflow Remove all metadata in namenode. Initialize the namenode. However, the data in datanode is not removed. hdfs dfs -mkdir xxxCreate a directory. To see the data location, see local storage hdfs dfs -put source destcopy content in source to dest hdfs dfs -gethdfs dfs -du -h -vIt displays sizes of files and directories contained in the given directory or the length of a file in case it’s just a file. The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files. Without the -s option, the calculation is done by going 1-level deep from the given path. The -h option will format file sizes in a human-readable fashion (e.g 64.0m instead of 67108864) The -v option will display the names of columns as a header line. The -x option will exclude snapshots from the result calculation. Without the -x option (default), the result is always calculated from all INodes, including all snapshots under the given path. hadoop fs -count -h /dir/*显示文件夹下的所有文件数、大小 web uihdfs default ports are changed. see here Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 –&gt; 9864 local storageFrom localhost:9870, you can get the namenode information. To see the data you created locally: Login localhost:9870, get the ‘configuration‘ from the ‘utilities‘ Find dfs.datanode.data.dir to get the data location Issue: Permission denied: user=dr.whoWhen ‘browse the file system‘ from ‘utilities‘, there are some dirs (e.g. /tmp) you have no permission to access. It may show: 1Permission denied: user&#x3D;dr.who, access&#x3D;READ_EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:cherish:supergroup:drwx------ The ‘dr.who‘ is just a configured static user in core-default.xml: 1hadoop.http.staticuser.user=dr.who And there is permission check because it’s set to check by default in hdfs-default.xml: 1dfs.permissions.enabled=true There are three ways to solve it: solutionsdisable the permission check This is not recommended in the prod mode. Add the following property in hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; change the staticuserAdd the following property in core-site.xml 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;cherish&lt;/value&gt;&lt;/property&gt; modify the file permission1$ hdfs dfs -chmod -R 755 /tmp Replica]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>distributed storage</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive introduction]]></title>
    <url>%2F2019%2F01%2F03%2Fhive-introduction%2F</url>
    <content type="text"><![CDATA[apache hive 是一个 data warehouse 应用。支持分布式存储的大数据读、写和管理，并且支持使用标准的 SQL 语法查询。Hive is not a database. This is to make use of SQL capabilities by defining a metadata to the files in HDFS. Long story short, it brings the possibility to query the hdfs file. hive 并没有固定的数据存储方式。自带的是 csv（comma-separated value）和 tsv (tab-separated values) connectors，也可以使用 connector for other formats。 database v.s. warehouse参见 the difference between database and data warehouse database：存储具体的业务数据，完善支持 concurrent transaction 操作（CRUD）。 database contains highly detailed data as well as a detailed relational views. Tables are normalized to achieve efficient storage, concurrent transaction processing, as well as return quick query results. 主要用于 OLTP (online trancaction processing)。 use a normalized structure. 即通常会组织成 table、row、column，冗余信息很少（比如三张表 product、color、product-color），所以节省空间。在查询时就需要通过复杂的 join 来实现，所以分析性的查询会比较耗时 no historical data. 主要处理 transaction 数据，只保存现在的数据，进行的查询和分析也是基于现有数据。即它的分析是 static one-time reports optimization 主要是优化写速度、读速度。复杂分析因为涉及很多 join，其性能提升也是一个主要的问题。 经常需要满足关系型数据库的 ACID 原则（atomicity, consistency, isolation, and durability）。所以它需要支持并发操作下的数据完整性。对 concurrent transaction 的支持要求比较高。 data warehouse将企业中的各种数据收集起来，重新组织，对这些数据做高效 分析 A data warehouse is a system that pulls together data from many different sources within an organization for reporting and analysis. The reports created from complex queries within a data warehouse are used to make business decisions. The primary focus of a data warehouse is to provide a correlation between data from existing systems, i.e., product inventory stored in one system, purchase orders for a specific customer, stored in another system. Data warehouses are used for online analytical processing (OLAP), which uses complex queries to analyze rather than process transactions. 主要用于 OLAP (online analysis processing). 它收集企业内各个数据源的数据，建立数据关联，对这些数据做复杂的查询分析，以辅佐业务决策。 use a denormalized structure. 它收集多个相关数据源的数据，将这些 table denormailize、transform，获得 summarized data、multidimentional views，并基于这些数据实现快速分析和查询。它不在乎冗余，相反，很多时候正是通过冗余重新组织数据，使得查询更方便。 store historical data. data warehouse 主要是用于分析的，所以通常会存储历史数据，以实现对历史数据和现有数据的对比分析。 optimization 主要是查询响应速度。它对大数据做分析，响应速度是主要的衡量标准。 一般不支持高并发操作。支持一定并发，但支持程度远不如 database installationSee hadoop: setting up a single-node cluster, GettingStarted Hive relies on hadoop. And we need a db (eg. mysql) to store hive metadata. So the prerequisites are: hadoop installed mysql installed: to store hive metadata java installed: ?? ssh installed and sshd running: when running hadoop scripts and managing remote hadoop daemons, it use ssh to authenticate. install hadoop on macinstall hive on mac After init mysql, you may find that you can’t connect mysql using ‘-uhive -pxxx’. Then try to grant privileges to &#39;hive&#39;@&#39;%&#39; instead of &#39;hive&#39;@&#39;localhost&#39;. Use wildcard % to match all hosts. After installation, can try the simple example to see how to conduct analysis on hive. set envTo use hadoop and hive conveniently, set the bin in Path. Just add the follow config into ~/.zshrc, and then source it source ~/.zshrc. 1234export HADOOP_HOME=/usr/local/Cellar/hadoop/3.1.1/libexecexport HIVE_HOME=/usr/local/Cellar/hive/3.1.1/libexecexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport PATH=$PATH:$HIVE_HOME/bin running using beelinebeeline is a new hive client to replace the deprecated HiveCli. With beeline, you can execute write, load, query, etc. on hive. To connect simply, type the following: 12$ hiveserver2$ beeline -u jdbc:hive2:&#x2F;&#x2F; To create, alter database/table/column/etc. on hive, see Hive Data Definition Language. To get the query commands, see LanguageManual Select To load data from file, insert, delete, merge, update data, see DML (data manipulation language) Other non-sql commands to use in HiveQL or beeline, see LanguageManual Commands. The Hive Resources related commands are non-sql commands. Configure Hivehow to configure hive properties To show hive config in hive cli: (show conf) 12# to show current: `set confName`0: jdbc:hive2://slave1:2181,slave2:2181,maste&gt; set hive.fetch.task.conversion; There are two hive-site.xml files. See two hive-site.xml config files on HDP /etc/hive/conf/hive-site.xml is the config for Hive service itself and is managed via Ambari through the Hive service config page. /usr/hdp/current/spark-client/conf/hive-site.xml actually points to /etc/spark/conf/hive-site.xml . This is the minimal hive config that Spark needs to access Hive. This is managed via Ambari through the Spark service config page. Ambari correctly configures this hive site for Kerberos. Depending upon your version of HDP you may not have the correct support in Ambari for configuring Livy. The hive-site.xml in Spark doesn’t have the same template as Hive’s. Ambari will notice the hive-site.xml and overwrite it in the Spark directory whenever Spark is restarted. analysis on hiveWhen you start a sql function (eg. select count(*) from xxx), it in fact starts an map-reduce job based on hadoop to search among all datanodes. Such functions are simple analysis implemented by hive. Hive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable: 1mapred.job.tracker For complex analysis, you may need to write custom mappers (map data) &amp; reducers (collect data) scripts. UseTRANSFORM keyword in hive to achieve this. For example, the weekday_mapper.py to convert unixtime to weekday: 12345678import sysimport datetimefor line in sys.stdin: line = line.strip() userid, movieid, rating, unixtime = line.split('\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\t'.join([userid, movieid, rating, str(weekday)]) And then use the script: 1234567891011121314151617181920CREATE TABLE u_data_new ( userid INT, movieid INT, rating INT, weekday INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t';add FILE weekday_mapper.py;INSERT OVERWRITE TABLE u_data_newSELECT TRANSFORM (userid, movieid, rating, unixtime) USING 'python weekday_mapper.py' AS (userid, movieid, rating, weekday)FROM u_data;SELECT weekday, COUNT(*)FROM u_data_newGROUP BY weekday; storage on hiveHive relies on Hadoop. The data in hive is saved in hdfs in fact. And the metadata is saved in mysql (the db can be configured). When check localhost:9870, you can see a new folder /user/hive/warehouse. All tables in hive are dirs in /user/hive/warehouse. Hive Partitionhive中简单介绍分区表(partition table)，含动态分区(dynamic partition)与静态分区(static partition) Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.Tables or partitions are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table.For example, a table named Tab1 contains employee data such as id, name, dept, and yoj (i.e., year of joining). Suppose you need to retrieve the details of all employees who joined in 2012. A query searches the whole table for the required information. However, if you partition the employee data with the year and store it in a separate file, it reduces the query processing time. The following example shows how to partition a file and its data: Hive buckethive partitioning vs bucket with examples stack-overflow: hive partition vs bucket 123456789CREATE TABLE zipcodes(RecordNumber int,Country string,City string,Zipcode int)PARTITIONED BY(state string)CLUSTERED BY Zipcode INTO 10 BUCKETSROW FORMAT DELIMITEDFIELDS TERMINATED BY ','; PARTITIONING BUCKETING Directory is created on HDFS for each partition. File is created on HDFS for each bucket. You can have one or more Partition columns You can have only one Bucketing column You can’t manage the number of partitions to create You can manage the number of buckets to create by specifying the count NA Bucketing can be created on a partitioned table Uses PARTITIONED BY Uses CLUSTERED BY partition 和 bucket 都是将大数据集拆成更小的数据集，加速查询处理的方式。比如按日期拆分区，很多分析只拿当天的分区，处理的数据量、读取的 hdfs 文件很少，就快。 最大的区别是 partition 拆数据就是按 column 值拆，bucket 拆数据是按 column hash 值拆，所以 bucket 最终的桶的数目是固定的，同时一个桶里可能有多个 column 值（parition 每个分区只会存一种 column 的值） 相对来讲，bucket 粒度可能更细。比如一个场景，我们将 order 按 date 分区，分区后每天的数据量还是特别大，如果我们很多查询/join是基于 employee，此时可以基于 employe_id 再分成更多的小集合，即按 employe_id 字段 hash 到 n 个桶里，这种拆桶方式特别有利于宏宇今天说的 map-side join，而且相比 partition，可以控制文件数量（有时想用的 partition 字段可能会分成特别特别多小分区，这个时候 bucket 就更合适些） 上边那个例子，假如 order 按 date+employee_id partition，分区就会特别多（对 hdfs namenode 造成大压力，hive metadata 也有压力），所以按 date partition, 按 employee_id bucket 就比较合适 ORC vs Parquetorc vs Parquet ORCFile in HDP 2: Better Compression, Better Performance Hive Transactionalhive transaction Close: 12345678set hive.support.concurrency &#x3D; false;set hive.optimize.index.filter &#x3D; false;set hive.txn.manager &#x3D; org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;set hive.compactor.initiator.on &#x3D; false;set hive.compactor.worker.threads &#x3D; 0;set hive.strict.managed.tables &#x3D; false;TBLPROPERTIES (&#39;transactional&#39;&#x3D;&#39;false&#39;) architecture Hive Data Typeshive data types Common used commands12340: jdbc:hive2://slave1:2181&gt; use dbname;0: jdbc:hive2://slave1:2181&gt; show tables;0: jdbc:hive2://slave1:2181&gt; describe formatted tablename;0: jdbc:hive2://slave1:2181&gt; describe extended tableName auto increment idtwo ways hive auto increment id 123456789101112131415## use row_numberinsert into tbl_dim select row_number() over (order by tbl_stg.id) + t2.sk_max, tbl_stg.* from tbl_stg cross join (select coalesce(max(sk),0) sk_max from tbl_dim) t2; ## use UDFRowSequenceadd jar hdfs:///user/hive-contrib-2.0.0.jar; create temporary function row_sequence as 'org.apache.hadoop.hive.contrib.udf.udfrowsequence'; insert into tbl_dim select row_sequence() + t2.sk_max, tbl_stg.* from tbl_stg cross join (select coalesce(max(sk),0) sk_max from tbl_dim) t2; get latest partition123## will only scan 2-3 partitionsselect max(ingest_date) from db.table_namewhere ingest_date&gt;date_add(current_date,-3) create table from another table12345CREATE TABLE new_test row format delimited fields terminated by '|' STORED AS RCFile AS select * from source where col=1 select all without some columnsblog 123456789set hive.support.quoted.identifiers=none;select`(num|uid)?+.+`from (select row_number() over (partition by uid order by pay_time asc) as num ,* from order) first_orderwhere num = 1 select latest in grouplink use rank 123select * from ( select id, name, starttime, rank() over(partition by name order by unix_timestamp(starttime, 'EEE, dd MMM yyyy hh:mm:ss z') desc) as rnk from hive_table) a where a.rnk=1; hive cli pretty123set hive.cli.print.header=true; // 打印列名set hive.cli.print.row.to.vertical=true; // 开启行转列功能, 前提必须开启打印列名功能set hive.cli.print.row.to.vertical.num=1; // 设置每行显示的列数 Optimization小文件问题和 spark 的小文件问题一样，hive 的运算引擎（mapreduce 或 Tez），为了提高性能，最后都会采用多个 reducer 来写数据，这个时候就会有小文件。不同于 Spark，Hive 本身提供了多种措施来优化小文件存储，我们只需要设置就行 1. 使用 concatenatehive concatenate 主要针对 orc 和 rcfile 文件格式存储的文件，特别是 orc ，可以直接执行 stripe level 的 merge，省掉 deserialize 和 decode 的开销，很高效。（concatenate 可以执行多次，最终文件数量不会变化） 1ALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] CONCATENATE; 2. 使用一些配置，在写文件时，自动 merge输入时合并： 12345678-- 每个Map最大输入大小，决定合并后的文件数set mapred. max .split.size=256000000;-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并set mapred. min .split.size.per.node=100000000;-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并set mapred. min .split.size.per.rack=100000000;-- 执行Map前进行小文件合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 输出时合并： 123456789101112-- hive 输出时合并的配置参数-- 在Map-only的任务结束时合并小文件set hive.merge.mapfiles = true;-- 在Map-Reduce的任务结束时合并小文件, 默认 falseset hive.merge.tezfiles=true;set hive.merge.mapredfiles = true;-- 合并文件的大小, 默认 256000000set hive.merge.size.per.task=256000000;-- 当输出文件的平均大小小于该值时, 启动一个独立的map-reduce任务进行文件merge， 默认 16000000set hive.merge.smallfiles.avgsize=256000000;-- 当这个参数设置为true,orc文件进行stripe Level级别的合并,当设置为false,orc文件进行文件级别的合并。默认 trueset hive.merge.orcfile.stripe.level=true; Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是： 根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开； 结果文件的平均大小需要大于avgsize参数的值。 Issuescount(*) return 0hive count(*) not working hive analyze 1234567# 可以设，但不好0: jdbc:hive2://slave1:2181&gt; set hive.fetch.task.conversion=none;# 或者设0: jdbc:hive2://slave1:2181&gt; set hive.compute.query.using.stats=false# 推荐0: jdbc:hive2://slave1:2181&gt; analyze table t [partition p] compute statistics for [columns c,...]; Its better not to disturb the properties on the statistics usage like hive.compute.query.using.stats. It impacts the way the statistics are used in your query for performance optimization and execution plans. It has tremendous influence on execution plans, the statistics stored depends on the file format as well. Therefore definitely not a solution to change any property with regards to statistics.The real reason for count not working correctly is the statistics not updated in the hive due to which it returns 0. When a table is created first, the statistics is written with no data rows. Thereafter any data append/change happens hive requires to update this statistics in the metadata. Depending on the circumstances hive might not be updating this real time.Therefore running the ANALYZE command recomputes this statistics to make this work correctly. hive not recognizing alias names in select partThe where clause is evaluated before the select clause, which is why you can’t refer to select aliases in your where clause. You can however refer to aliases from a derived table. 1234567select * from ( select user as u1, url as u2 from rank_test) t1 where u1 &lt;&gt; "";select * from ( select user, count(*) as cnt from rank_test group by user) t1 where cnt &gt;= 2; Side note: a more efficient way to write the last query would be 12select user, count(*) as cnt from rank_test group by userhaving count(*) &gt;= 2 In, not in substitutionHive supports sub-query in in , not in only after 0.13. And in may be slow, so we can replace it with join. 1234-- inselect * from a where id in (select id from b)-- in substitutionnselect a.* from a join (select id from b) b1 on a.id = b1.id VERTEX_FAILURE1234set hive.exec.max.dynamic.partitions=8000;set hive.exec.max.dynamic.partitions.pernode=8000;set hive.tez.log.level=DEBUG; explain1explain select sum(id) from my; \xa0(SPACE_SEPARATOR, LINE_SEPARATOR, or PARAGRAPH_SEPARATOR) but is not also a non-breaking space (&#39;\u00A0&#39;, &#39;\u2007&#39;, &#39;\u202F&#39;). java isWhiteSpace() 12print(res.selectExpr(&quot;trim(translate(mobile1, &#39;\u00A0&#39;, &#39; &#39;))&quot;).collect())print(res.selectExpr(&quot;trim(regexp_replace(mobile1, &#39;\u00A0|\u2007|\u202F&#39;, &#39; &#39;))&quot;).collect())]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>hive</tag>
        <tag>data warehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[responsive-web-design]]></title>
    <url>%2F2018%2F12%2F13%2Fresponsive-web-design%2F</url>
    <content type="text"><![CDATA[自适应一般是设定基准值，宽、高、字体大小都指定为基准值的百分比。当基准值改变时，页面元素、宽高也会按比例变化。 自适应宽度不使用绝对宽度网页宽度默认等于屏幕宽度。所以大部分时候只要不适用绝对宽度即可实现自适应宽度： 1234body: &#123; width: 100%; // or width: auto;&#125; 如果元素是图片，也可以使用 max-width 属性，参见responsive web design: image 1234img &#123; max-width: 100%; height: auto;&#125; 使用 media这适用于需要针对不同的屏幕，显示不同的排版。利用 @media 的 css 规则，可实现根据一个或多个基于设备类型、具体特点和环境的媒体查询来应用样式。 123456789101112131415/* Media query */@media screen and (min-width: 900px) &#123; article &#123; padding: 1rem 3rem; &#125;&#125;/* Nested media query */@supports (display: flex) &#123; @media screen and (min-width: 900px) &#123; article &#123; display: flex; &#125; &#125;&#125; css 规则css 规则 相当于给 css 增加的函数。原本 css 只是简单的 json 对象，现在希望 css 更多样化、更容易维护，所以增加了 css 规则。 @media 这种属于条件规则组，即接收两个参数，第一个参数是个 bool 条件，第二个参数是执行的语句。仅当条件为 true 时，其后的语句才会执行。 自适应高度同样的，要使得高度自适应，也必须指定一个基准值。但不同于宽度（默认是屏幕宽度），高度通常是不固定的，要确定基准值也就麻烦些。 父容器的高度可确定 For the height of a div to be responsive, it must be inside a parent element with a defined height to derive it’s relative height from. 如果父容器的高度可确定，则同上，采用百分比即可实现自适应。 1234567parent: &#123; height: 100px; child: &#123; height: 80%; &#125;&#125; 根据宽度变化，同比例改变高度页面宽度的自适应容易实现，可以基于宽度的变化，同比例更改高度。 利用 padding参见responsive height proportional to width，设定 height 为 0，padding 为百分比 In all browsers, when padding is specified in %, it’s calculated relative to the parent element’s width. Html: 1234567891011&lt;div class="content"&gt; Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.&lt;/div&gt;&lt;br/&gt;&lt;div class="wrapper"&gt; &lt;div class="content2"&gt;Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat. &lt;/div&gt;&lt;/div&gt; Css: 123456789101112131415161718.content &#123; width: 100%; height: 0; padding-bottom: 30%; background-color: #7ad;&#125;.wrapper &#123; position: relative; width: 100%; padding-bottom: 30%; background-color: #7ad;&#125;.wrapper .content2 &#123; position: absolute; width: 100%; height: 100%; background-color: #7da;&#125; 利用 viewport units新版本的浏览器支持 viewport units，eg. 12width: 100vw;height: 30vw; /* 30% of width */ 使用 media同宽度，指定不同 media 情况下，高度显示不同。]]></content>
      <tags>
        <tag>css</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[duplicate rows in postgresql]]></title>
    <url>%2F2018%2F11%2F06%2Fduplicate-rows-in-postgresql%2F</url>
    <content type="text"><![CDATA[参见 Search and destroy duplicate rows in PostgreSQL Find duplicatesusing group123456789SELECT firstname, lastname, count(*)FROM peopleGROUP BY firstname, lastnameHAVING count(*) &gt; 1; using partition123456789SELECT * FROM (SELECT *, count(*) OVER (PARTITION BY firstname, lastname ) AS count FROM people) tableWithCount WHERE tableWithCount.count &gt; 1; Using not strict distinct利用 not strict distinct DISTINCT ON 找到唯一的那些条，剩余的就是重复的，可以修改或删除 123456789DELETE FROM people WHERE people.id NOT IN (SELECT id FROM ( SELECT DISTINCT ON (firstname, lastname) * FROM people)); // more readable codeWITH unique AS (SELECT DISTINCT ON (firstname, lastname) * FROM people)DELETE FROM people WHERE people.id NOT IN (SELECT id FROM unique);]]></content>
      <tags>
        <tag>sql</tag>
        <tag>postgres</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[performance for io in java]]></title>
    <url>%2F2018%2F10%2F15%2Fperformance-of-io-in-java%2F</url>
    <content type="text"><![CDATA[java.io.ByteArrayOutputStream这一般在用到字节流是会用到。 java performance tuning guide 这篇文章不建议在 performance-criticted 代码中使用 ByteArrayOutputStream： 同步写入，效率低 ByteArrayOutputStream allows you to write anything to an internal expandable byte array and use that array as a single piece of output afterwards. Default buffer size is 32 bytes, so if you expect to write something longer, provide an explicit buffer size in the ByteArrayOutputStream(int) constructor 注： ByteArrayOutputStream 内部是一个可变长度的 byte[]（通过扩充实现可变）。它有个初始长度（默认 32），可以在 constructor 中指定. ByteArrayOutputStream 是同步写入，比较影响效率 toByteArray() 效率低，使用 toString(charset) toByteArray 执行了一遍拷贝，效率低。toString 则使用 String(bytes[]) 直接将内部 byte 转成了 String inefficient byte[] to String constructor 指出 String(bytes[]) 也是 copy，但 java8 中的源码看了下，似乎不是 copy，待考证……]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test principles]]></title>
    <url>%2F2018%2F10%2F12%2Ftest-principles%2F</url>
    <content type="text"><![CDATA[three reasons why we should not use inheritance in tests 大概意思是： 很多测试里的继承用的不合适。测试也是代码，必须符合继承的原则。 The point of inheritance is to take advantage of polymorphic behavior NOT to reuse code, and people miss that, they see inheritance as a cheap way to add behavior to a class. When I design code I like to think about options. When I inherit, I reduce my options. I am now sub-class of that class and cannot be a sub-class of something else. I have permanently fixed my construction to that of the superclass, and I am at a mercy of the super-class changing APIs. My freedom to change is fixed at compile time. 多态的定义： Polymorphism is the provision of a single interface to entities of different types 可能会影响性能。 并不是所有的测试可能都会用到测试基类里的东西，而 beforeClass、beforeTest 在执行所有测试时都会被1. 查找，2. 执行，这些都导致了 CPU 的浪费。 此外，其他人也提到，比如 springboottest 中，基类中配置 @SpringBootTest ，最终可能导致在每个测试中配置 @DirtyContext等，严重影响性能。 很影响 reading 测试即文档，而很多东西却是需要读多个类 这个问题仁者见仁，智者见智。duplicate code 也是问题。两者相权，如何处理？但对于目前的代码，如果没有问题，还是保持现状比较好。对于符合继承规则这点，倒是需要注意的。 作者在 writing clean test 中提出了些解决方法，但感觉还是有些片面。当然他写了一个系列的文章：writing clean test series。 另外读书：effective unit testing]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gradle test performance]]></title>
    <url>%2F2018%2F10%2F12%2Fgradle-test-performance%2F</url>
    <content type="text"><![CDATA[gradle test configurations one sample config ways to improve performance of gradle build common used properties: jvmArgs: jvm 参数。通常会配置堆栈大小，保证测试对内存的要求。 &#39;-Xms128m&#39;, &#39;-Xmx1024m&#39;, &#39;-XX:MaxMetaspaceSize=128m&#39;。-Xms 是初始堆大小，-Xmx 是最大堆大小，-XX:MaxMetaspaceSize 是 class metadata 可占用的最大本地内存（默认是 unlimited）。具体 jvm 参数参考 java doc. forkEvery: 每个 test process 里跑的 test classes 的最大个数。当次数达到限制后，会自动重启。这定义了一个测试线程什么时候回重启，与并发无关。默认是 0，即无最大限制，就是可以一直跑 maxParalleForks: 能并发跑的最大 test processes 数目 systemProperty: 系统属性 environment：系统环境变量 include: 具体执行的测试。可以通过这个配置不同的测试级别（单元测试、集成测试、functional 测试……）]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python subprocess]]></title>
    <url>%2F2018%2F09%2F03%2Fpython-subprocess%2F</url>
    <content type="text"><![CDATA[Synchronous vs multiprocessing vs multithreading vs async Concurrency vs Parralism. asyncio &amp; threading can run multiple I/O operations at the same time. Async runs one block of code at a time while threading just one line of code at a time. With async, we have better control of when the execution is given to other block of code but we have to release the execution ourselves. IO bound problems: use async if your libraries support it and if not, use threading. CPU bound problems: use multi-processing. None above is a problem: you are probably just fine with synchronous code. You may still want to use async to have the feeling of responsiveness in case your code interacts with a user. Reference: blog: subprocess subprocess.call()父进程等待子进程完成返回退出信息(returncode，相当于exit code，见Linux进程基础) subprocess.check_call() 父进程等待子进程完成 返回0 检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查(见Python错误处理)。 subprocess.check_output() 父进程等待子进程完成 返回子进程向标准输出的输出结果 检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。 official doc: subprocess 1subprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, capture_output=False, shell=False, cwd=None, timeout=None, check=False, encoding=None, errors=None, text=None, env=None) config env in subprojcess 123new_env &#x3D; os.environ.copy()new_env[&#39;MEGAVARIABLE&#39;] &#x3D; &#39;MEGAVALUE&#39;subprocess.Popen(&#39;path&#39;, env&#x3D;new_env) an example to use subprojcess: 123456789101112try: subprocess.check_output(['java', '-DCOMMAND=' + command, '-DTENANT_ID=' + tenant_id, '-DMODEL_ID=' + model_id, '-jar', 'xxx.jar', ]) except subprocess.CalledProcessError as e: db_util.reopx_solver - 1.0 - SNAPSHOT - all.jarport_status(tenant_id, model_id=model_id, msg=f'Failed runner for "&#123;command&#125;"...') logger.exception('Could not start solver jar: ', e.output) raise OPXException(f'Could not start runner for "&#123;command&#125;".', e.stderr) the jar must be in the same directory as parent process dir]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guice]]></title>
    <url>%2F2018%2F08%2F17%2Fguice%2F</url>
    <content type="text"><![CDATA[TEST with GUICE]]></content>
  </entry>
  <entry>
    <title><![CDATA[构建]]></title>
    <url>%2F2018%2F08%2F16%2Fgradle-build%2F</url>
    <content type="text"><![CDATA[Problems? manifest 是干什么用的？ 代码运行时，如何找到 dependency 的包 java -jar 时，classpath 指定？ classpathclasspath 指定的是 java 类所在的目录（包括当前项目的类、依赖的类等）。应该是当打 jar 包的时候，默认会加上当前目录(.)到 classpath，这样就包含了 jar 内部的类？ Thin jargradle lean This plugin depends on JavaPlugin and ApplicationPlugin. for installDist, jars under install/$PROJECT_NAME$/lib/ for distZip, jars under /lib/ inside package 1234567891011plugins &#123; id 'java' // Apply the application plugin to add support for building a CLI application. id 'application' id 'scala' id 'com.github.maiflai.scalatest' version '0.26' id "com.github.gradle-lean" version "0.1.2"&#125; implementation vs compile vs apistackoverflow Dependency Conflictforce some edition12345configurations.all &#123; resolutionStrategy &#123; force 'com.fasterxml.jackson.module:jackson-module-scala_2.11:2.10.3' &#125;&#125;]]></content>
      <tags>
        <tag>gradle</tag>
        <tag>java</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java.lang.UnsatisfiedLinkError: no xxx in java.library.path]]></title>
    <url>%2F2018%2F08%2F14%2Fjava-application-using-third-party-lib%2F</url>
    <content type="text"><![CDATA[背景 项目需要引入 local 第三方包 该第三方包只有 window/linux license，而开发在 macos 开发时，通过 gradle dependency compile files(&#39;path/to/thejar.jar&#39;) 来引入包 问题运行时，报错误 java.lang.UnsatisfiedLinkError: no thejar in java.library.path 原因引入 .dll 或 .so 失败造成。 solution 把 thejar 加入到 path 中 ————— not work 加入 path，并 loadLibrary ——————— not work should work（配置 .dll 或 .so 路径）： 配置 PATH 或 jar 包启动时，设置 ‘-Djava.library.path’ 有关 PATH, -classpath, java.library.path 的区别，再 google。java 在使用这三个 path 时： PATH：用来寻找 java, javac 等 command 并执行 classpath：jvm 在执行时用来寻找 java class。classpath 一般指向 jar 包的位置，即 jdk 的 lib 目录 java.library.path: 指向非 java 类包的位置，如 .dll, .so 等。在 java System.loadLibrary() 时从这找 java.library.path 的设定可参照 stackoverflow java -jar xxx.jar -Djava.library.path=xxx Set PATH Set programmatically (NOT RECOMMENDED, may cause unpredictable result): 12345System.setProperty("java.library.path", path);//set sys_paths to nullfinal Field sysPathsField = ClassLoader.class.getDeclaredField("sys_paths");sysPathsField.setAccessible(true);sysPathsField.set(null, null); linux 下可通过 LD_LIBRARY_PATH 设置 java.library.path。windows 下似乎是通过 PATH 设定（待确定）]]></content>
      <tags>
        <tag>jvm</tag>
        <tag>problems</tag>
        <tag>ini</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis 工作原理]]></title>
    <url>%2F2018%2F08%2F10%2Fmybatis-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[几个核心类参见: java api 入门 - 介绍核心使用组件和最佳实践 SqlSessionFactorymybatis 应用以一个 sqlSessionFactory 实例为核心，即一个应用中有一个单例 SqlSessionFactory，所以数据库 session 都从这里获得。 SqlSessionFactory 可以通过 SqlSessionFactoryBuilder 获得，builder 负责从 xml 配置或 java configuration 类获得。xml (或相应的 java configuration 类) 配置了 datasource（数据库连接信息）、mappers 等信息 SqlSessionFactoryBuilder它主要就是用来获取 SqlSessionFactory，可以从 xml 或 Java Configuration 类加载配置并构建。提供如下几种方式来获取（参见java api）： 1234567891011121314// 从 xml 获取，其中配置了 environment，datasource，mappers SqlSessionFactory build(InputStream inputStream);// 从 xml 获取，但当 xml 配置了多个 env 中的 datasource 等时，通过 env 指定加载的环境SqlSessionFactory build(InputStream inputStream, String environment);// 从 xml 获取，但可以指定其中使用到的 properties(详见下文的解释)SqlSessionFactory build(InputStream inputStream, Properties properties); // 从 xml 获取，并指定 env 和使用的 propsSqlSessionFactory build(InputStream inputStream, String env, Properties props);// 从 Java Configuration 获取SqlSessionFactory build(Configuration config) SqlSessionFactoryBuilder 只是为了创建 SqlSessionFactory，创建完成就可以丢弃 builder 了。所以一般它的生命周期是方法级，是其中的一个局部变量 使用 xml先配置一个 config.xml： 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;driver&#125;"/&gt; &lt;property name="url" value="$&#123;url&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource="org/mybatis/example/BlogMapper.xml"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 上边的配置中，environments 配置的是各个环境下的数据库配置。每个环境下，都可以配置 TransactionManager、datasource 等，连接数据库、包括操作数据库的 driver 等信息都是在这里配置的)。 ${driver} 这种写法是用的 property。property 可以直接在这个 xml 中配置（使用 &lt;properties&gt; 标签），也是 java 中的 System.getProperties() 中的 property，还可以是在 SqlSessionBuilder 中传递的 props 参数。 构建 SqlSessionFactory： 123String resource = "org/mybatis/example/mybatis-config.xml";InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 使用 java Configuration 类123456DataSource dataSource = BlogDataSourceFactory.getBlogDataSource();TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment("development", transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.addMapper(BlogMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); SqlSessionSqlSession 是执行 sql 命令的接口： 1234567SqlSession session = sqlSessionFactory.openSession();try &#123; BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlog(101);&#125; finally &#123; session.close();&#125; SqlSession 可以执行所有的 sql 命令、做事务提交、回滚、获取 Mapper 实例等，非常强大。SqlSession 也是方法作用域级别的，并且必须被正确关闭： 每个线程都应该有它自己的 SqlSession 实例。SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。也绝不能将 SqlSession 实例的引用放在任何类型的管理作用域中，比如 Servlet 架构中的 HttpSession。如果你现在正在使用一种 Web 框架，要考虑 SqlSession 放在一个和 HTTP 请求对象相似的作用域中。换句话说，每次收到的 HTTP 请求，就可以打开一个 SqlSession，返回一个响应，就关闭它。这个关闭操作是很重要的，你应该把这个关闭操作放到 finally 块中以确保每次都能执行关闭。下面的示例就是一个确保 SqlSession 关闭的标准模式： 123456SqlSession session = sqlSessionFactory.openSession();try &#123; // do work&#125; finally &#123; session.close();&#125; MapperMapper 是资源和数据库实例的映射，提供了相关操作来做转换。可以用两种方式写：xml 或 annotation。 mapper 实例从 SqlSession 获得，所以生命周期和 SqlSession 相同。 xml123456789&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="org.mybatis.example.BlogMapper"&gt; &lt;select id="selectBlog" resultType="Blog"&gt; select * from Blog where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 访问： 1Blog blog = (Blog) session.selectOne("org.mybatis.example.BlogMapper.selectBlog", 101); 当然，我们希望通过 java 接口来调用这些方法，所以可以写相应的 mapper 接口 ，只要保证 namespace 是一致的即可。namespace 是实现接口绑定的方式。mybatis 基于命名空间的命名解析规则如下： 完全限定名（比如“com.mypackage.MyMapper.selectAllThings”）将被直接查找并且找到即用。 短名称（比如“selectAllThings”）如果全局唯一也可以作为一个单独的引用。如果不唯一，有两个或两个以上的相同名称（比如“com.foo.selectAllThings ”和“com.bar.selectAllThings”），那么使用时就会收到错误报告说短名称是不唯一的，这种情况下就必须使用完全限定名。 一旦绑定了接口，就可以用如下方式访问 mapper 方法了： 12BlogMapper mapper = session.getMapper(BlogMapper.class);Blog blog = mapper.selectBlog(101); annotation也可以不依赖于 xml，直接在 mapper 接口上通过 annotation 定义 sql： 12345package org.mybatis.example;public interface BlogMapper &#123; @Select("SELECT * FROM blog WHERE id = #&#123;id&#125;") Blog selectBlog(int id);&#125; annotation 可能更简洁一些，但是 mybatis 目前还是 xml 更强大。大家可以依据需求自由的在两种方式之间切换，mybatis 会自己检测。 由于 Java 注解的一些限制加之某些 MyBatis 映射的复杂性，XML 映射对于大多数高级映射（比如：嵌套 Join 映射）来说仍然是必须的。有鉴于此，如果存在一个对等的 XML 配置文件的话，MyBatis 会自动查找并加载它（这种情况下， BlogMapper.xml 将会基于类路径和 BlogMapper.class 的类名被加载进来）]]></content>
      <tags>
        <tag>orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth]]></title>
    <url>%2F2018%2F07%2F27%2Foauth%2F</url>
    <content type="text"><![CDATA[traditional authentication传统认证使用 session: client 发送 username、password 给 server server 查数据库，检查信息，是否正确。正确就把用户登录信息(即用户状态)写到 session 里（即服务器内存中），并将 sessionId 返回给 client。 client 在请求 api 时，在 cookie 中传递 sessionId。server 端根据 sessionId 获取用户登录信息，如果已认证，返回正常响应；反之，401 这种方式有个缺陷：如果做分布式服务部署，那么需要每个服务器都要同步相同的登录信息，这不是一个好的方式。所以一般 rest 微服务都要求的是 stateless，即 server 端不保存任何用户信息，请求中包含所有需要的信息。 oauthoauth 是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 OAuth允许用户提供一个 令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容。 其令牌可以是 JWT 或其他形式。 ref: oauth 2.0 的用途 Auth0autho0 实现了很多开放标准，包括 oauth。(学习视频) 要使用 Auth0，首先需要创建一个 App（被称作 client），其中定义了 clientId、domain name、callbackUrl、secret 等。 前端交互： 当访问某个页面时，查看 localstorage，看用户是否登录； 如果未登录，利用 Auth0 sdk 或 api 登录认证（提供前边 App 中的 clientId、secret 等信息），认证通过将认证信息（token 等）存入 localstorage，并跳转到 callback url 如果登录，直接访问 后端交互： 前端携带 token 访问 API server 利用 Auth0 sdk 或 api 验证 token 的有效性；认证通过返回资源，否则 401]]></content>
      <tags>
        <tag>security</tag>
        <tag>oauth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asure storage]]></title>
    <url>%2F2018%2F07%2F27%2Fasure-storage%2F</url>
    <content type="text"><![CDATA[asure storage 提供四种存储支持（asure storage overview (youtube)）： blob (binary large object)：二进制数据存储。有两种：page blog（只能新增/删除/向已有数据附加数据，不能修改数据）；block blog（可以更新） table：存储表格（nosql） queue：有库，有 rest api files：好像主要用在文件共享的时候，就是类似于 windows server 上的文件共享（smb(server message block)），实现和使用方式都和 windows server 的文件共享一样。所以需要支持例如按 /servername/filename 等方式来 share 和 使用文件。它是构建于 blob 之上的。 几个概念Storage Accoutstorage 的所有存储都必须在一个 storage account 内发生。这有点类似于一个 database。 安全也是在这里实现： key：创建 account 时，就会生成俩 key，primary key 就是你用来登录访问数据的 key。不过这种方式对于有 client 时不太方便，因为可能不能 share key saas token：就是可以登录认证获得 token，然后拿 token 访问数据。 数据容器 + 数据在每个 storage account 里，你可以创建上述四种类型的存储容器，去存放数据。 Blob： 在 storage account 里可以创建 blob container，在 blog container 中存放 blob table：在 storage account 里创建 table，在 table 中存放 data entity queue：在 storage account 里创建 queue，在 queue 中存放 message file：在 storage account 里创建 asure files (类似于 smb(server message block))??? 感觉应该也是先创建一个 files container，再创建一个个的 asure file（即 smb）]]></content>
      <tags>
        <tag>cloud</tag>
        <tag>asure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mime type]]></title>
    <url>%2F2018%2F07%2F26%2Fmime-type%2F</url>
    <content type="text"><![CDATA[MIME 类型 是用一种标准化的方式来表示文档的性质和格式。浏览器一般通过 MIME 类型（而不是文档扩展名）来确定如何处理文档。因此服务器传输数据时，必须设置正确的 MIME 类型。 通用结构1type&#x2F;subtype 不允许空格 大小写不敏感，一般都是小写 独立类型type 可以是独立类型，表示文件的分类，可以是如下值： 类型 描述 典型示例 text 表明文件是普通文本，理论上是可读的语言 text/plain, text/html, text/css, text/javascript image 表明是某种图像。不包括视频，但是动态图（比如动态gif）也使用image类型 image/gif, image/png, image/jpeg, image/bmp, image/webp audio 表明是某种音频文件 audio/midi, audio/mpeg, audio/webm, audio/ogg, audio/wav video 表明是某种视频文件 video/webm, video/ogg application 表明是某种二进制数据 application/octet-stream, application/pkcs12, application/vnd.mspowerpoint, application/xhtml+xml, application/xml, application/pdf,`application/json` 一般是文本，但是具体类型不确定时，就用 test/plain；是二进制数据，而类型不确定时，用 application/octet-stream application/octet-stream这是应用程序文件的默认值。意思是 未知的应用程序文件 ，浏览器一般不会自动执行或询问执行。浏览器会将它作为附件来处理，附件类型等信息通过HTTP头Content-Disposition 设置。 Multipart 类型12multipart&#x2F;form-datamultipart&#x2F;byteranges 顾名思义，这是复合文件的一种表现形式，即传递过来的数据有多重类型。典型的如果表单数据可能有 string、文件、视频、音频等。 它由边界线（一个由&#39;--&#39;开始的字符串）划分出的不同部分组成。每一部分有自己的实体，以及自己的 HTTP 请求头，Content-Disposition和 Content-Type 用于文件上传领域，最常用的 (Content-Length 因为边界线作为分隔符而被忽略）。 例如，如下表单: 123456&lt;form action="http://localhost:8000/" method="post" enctype="multipart/form-data"&gt; &lt;input type="text" name="myTextField"&gt; &lt;input type="checkbox" name="myCheckBox"&gt;Check&lt;/input&gt; &lt;input type="file" name="myFile"&gt; &lt;button&gt;Send the file&lt;/button&gt;&lt;/form&gt; 请求是： 12345678910111213141516171819202122232425POST &#x2F; HTTP&#x2F;1.1Host: localhost:8000User-Agent: Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko&#x2F;20100101 Firefox&#x2F;50.0Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,*&#x2F;*;q&#x3D;0.8Accept-Language: en-US,en;q&#x3D;0.5Accept-Encoding: gzip, deflateConnection: keep-aliveUpgrade-Insecure-Requests: 1Content-Type: multipart&#x2F;form-data; boundary&#x3D;---------------------------8721656041911415653955004498Content-Length: 465-----------------------------8721656041911415653955004498Content-Disposition: form-data; name&#x3D;&quot;myTextField&quot;Test-----------------------------8721656041911415653955004498Content-Disposition: form-data; name&#x3D;&quot;myCheckBox&quot;on-----------------------------8721656041911415653955004498Content-Disposition: form-data; name&#x3D;&quot;myFile&quot;; filename&#x3D;&quot;test.txt&quot;Content-Type: text&#x2F;plainSimple file.-----------------------------8721656041911415653955004498--]]></content>
      <tags>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[code license]]></title>
    <url>%2F2018%2F07%2F26%2Fcode-license%2F</url>
    <content type="text"><![CDATA[license 是软件的授权许可。 对于开源软件来说，虽然别人可以用，但是用的时候希望别人遵循一些要求，比如，使用时必须标明原作者是谁、可以做怎样的修改、软件被用作不正规用途原作者是否要负责……这些其实就是一个协议。 对于作者来说，自己为开源代码写符合法律条规的繁冗的 license 太麻烦，所以就可以采用广为流传的开源协议（eg. MIT, CC…），在 license 文件中标明 “Licnse under the MIT license” 快速选择详细的协议选择可以从 github choose license 项目中选。下边列些常用的（参见 如何为你的代码选择一个开源协议） MIT 协议宽松但覆盖一般要点。此协议允许别人以任何方式使用你的代码同时署名原作者，但原作者不承担代码使用后的风险，当然也没有技术支持的义务。jQuery和Rails就是MIT协议 apache 协议作品涉及到专利，可以考虑这个。也比较宽松，但考虑了专利，简单指明了作品归属者对用户专利上的一些授权（我的理解是软件作品中含有专利，但它授权你可以免费使用）。Apache服务器，SVN还有NuGet等是使用的Apache协议。 GPL对作品的传播和修改有约束的，可以使用这个。GPL（V2或V3）是一种版本自由的协议（可以参照copy right来理解，后者是版本保留，那copyleft便是版权自由，或者无版权，但无版权不代表你可以不遵守软件中声明的协议）。此协议要求代码分发者或者以此代码为基础开发出来的衍生作品需要以同样的协议来发布。此协议的版本3与版本2相近，只是多3中加了条对于不支持修改后代码运行的硬件的限制（没太明白此句话的内涵）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java api lib for excel]]></title>
    <url>%2F2018%2F07%2F25%2Fjava-api-for-excel%2F</url>
    <content type="text"><![CDATA[可选 lib apache POI：java 中最大众的 ，支持 xls、xlsx，提供接口来创建、读写 excel文件。 apache openOffice uno： JExcel app：这个功能强大，什么都可以做，但是可能收费，而且仅基于 windows + installed excel JExcelAPI：轻量更易用，但似乎仅支持 xls，而且不支持复杂的 range、formular 计算操作。 javascript 版本的 JExcel docx4j：其中 xlsx4j 是处理 excel 的，不过看起来功能比较简单。 microsoft excel VBA：vba 是微软写的操作 office 软件的语言。所以可以利用这个 vba 写代码…… ref doc: baeldung example for apache POI &amp; JExcelAPI Mkyong example for JExcelAPI 核心操作支持 lib apache poi apache openoffice JExcelAPI read/create excel file ✔️ 可以 ✔️ read/write excel cells 1. 获取 sheet，遍历 row，遍历 row cells； 可以 1. 获取 sheet，可以根据 cell 行列数获取（getCell(rowIndex, columnIndex)） compute formula for cells 可以（用 formular evaluator，参见stackoverflow） 可以(calculateAll()) 可能可以 write/read named cells of file 可以（workBook.getNamedAt(&quot;name&quot;)，参见 stackoverflow, 官方文档） 可以（namedRange) NO pros 1. 使用广泛；2. 文档清晰；3. 支持很多复杂需求 1. 似乎功能比 poi 更简易，也更丰富些；2. 可以同时操作 openoffice 和 microoffice；3. 语言支持多，只要使用相应语言的 binding 就可以 1. 简单轻量；2. 接口友好 cons 接口不是很友好，访问起来比较麻烦 1. 文档乱七八糟；2. 采用 uno，领域对象不太一致；3. 必须要安装 openoffice 1. 功能太简单，复杂需求实现不了 核心操作 example（POI）quick guide 源代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// load excelXSSFWorkbook workbook = new XSSFWorkbook(getFile("test.xlsx"));// get sheet by nameString sheetName = "Product Mix";XSSFSheet sheet = workbook.getSheet(sheetName);assertThat(sheet.getSheetName(), is(sheetName));// get cell by coordinateXSSFCell tvsetNumber = getCell(sheet, "D4");assertThat(tvsetNumber.getNumericCellValue(), closeTo(100, 0.1));// get formula cell by coordinateXSSFCell total = getCell(sheet, "D13");assertThat(total.getNumericCellValue(), closeTo(16000, 0.1));// change cell valuetvsetNumber.setCellValue(200);assertThat(tvsetNumber.getNumericCellValue(), closeTo(200, 0.1));// refresh formulasXSSFFormulaEvaluator.evaluateAllFormulaCells(workbook);assertThat(total.getNumericCellValue(), closeTo(23500, 0.1));// write back to fileworkbook.write(new FileOutputStream(getFile("update.xlsx")));// check data updatedXSSFWorkbook updateWorkBook = new XSSFWorkbook(getFile("update.xlsx"));assertThat(getCell(updateWorkBook.getSheet(sheetName), "D4").getNumericCellValue(), closeTo(200, 0.1));assertThat(getCell(updateWorkBook.getSheet(sheetName), "D13").getNumericCellValue(), closeTo(23500, 0.1));// get cell by nameXSSFName test_name = workbook.getName("test_cell_name");AreaReference areaReference = new AreaReference(test_name.getRefersToFormula(), SpreadsheetVersion.EXCEL2007);CellReference firstCell = areaReference.getFirstCell();XSSFSheet sheet = workbook.getSheet(firstCell.getSheetName());XSSFRow row = sheet.getRow(firstCell.getRow());XSSFCell cell = row.getCell(firstCell.getCol());// get cell functionprivate XSSFCell getCell(XSSFSheet sheet, String cellCoordinate) &#123; CellReference d4 = new CellReference(cellCoordinate); XSSFRow row = sheet.getRow(d4.getRow()); return row.getCell(d4.getCol());&#125; 其他操作（POI）copy sheetxlsm to xlsx]]></content>
      <tags>
        <tag>java</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hikariCP configuration]]></title>
    <url>%2F2018%2F07%2F19%2FhikariCP-configuration%2F</url>
    <content type="text"><![CDATA[hikariCP 是一个轻量级的数据库连接池。引用 数据库连接池性能对比 的说法（我并没有测试过）： 性能方面 hikariCP&gt;druid&gt;tomcat-jdbc&gt;dbcp&gt;c3p0 。hikariCP的高性能得益于最大限度的避免锁竞争。 druid功能最为全面，sql拦截等功能，统计数据较为全面，具有良好的扩展性。 综合性能，扩展性等方面，可考虑使用druid或者hikariCP连接池。 可开启prepareStatement缓存，对性能会有大概20%的提升。 在使用 spring jpa 时，默认使用的连接池是 hikariCP，所以最终采用了这个连接池。 使用过程中出现了一些坑，总结一下。 java.sql.SQLTransientConnectionException仅使用默认配置，在运行所有测试时，会出现如下异常信息： 这是因为默认的连接池数量是 10，而并行运行测试时，连接池数量不够了。通过设置 maximumPoolSize 解决。 1spring.datasource.hikari.maximum-pool-size:1000 too many connections在进行上述设置后，启动应用，连接数据库，发现数据库无法连接，报 too many connections。 fixed-size pool按 hikariCP owner 的解释，这可能是因为当设置 maximumPoolSize 后，这就变成了 fix-size 的连接池了，即总是会占有 1000（上边的设置）个连接池。idle 连接不会被释放，因为释放了也要创建新的 idle 连接池来保证 fix-size。从而新的连接就无法建立了。 When running as a fixed-size pool (default) the idleTimeout has no effect. Your example is a fixed-size pool – when minimumIdle is not defined it defaults to maximumPoolSize. idleTimeout is meant to shrink the pool from maximumPoolSize down toward minimumIdle when connections are unused in the pool. However, when minimumIdle == maximumPoolSize then closing an “idle” connection makes no sense as it will be replaced immediately in the pool. 所以配置 minimumIdle 可以解决上述问题。 12spring.datasource.hikari.maximum-pool-size:1000spring.datasource.hikari.miniumIdle:10 wait-timeout too long有时就是因为 connection 一直没被释放，这可能是这是的 connection wait_timeout 太长了。参照 change the mysql timeout on a server 来了解 ‘wait_timeout’ 的设置。 引用原文： Choose a reasonable wait_timeout value. Stateless PHP environments do well with a 60 second timeout or less. Stateful applications that use a connection pool (Java, .NET, etc.) will need to adjust wait_timeout to match their connection pool settings. The default 8 hours (wait_timeout = 28800) works well with properly configured connection pools. Configure the wait_timeout to be slightly longer than the application connection pool’s expected connection lifetime. This is a good safety check. 在设置了合理的 mysql wait_timeout 后，同样也设置 hikariCP 的连接池空闲时间，参考FAQ If you set the MySQL wait_timeout = 28800 (seconds = 8 hours), you should set HikariCP idleTimeout and maxLifetime to the slightly shorter 28000000 (milliseconds = 7 hours 46 minutes). 1234spring.datasource.hikari.maximum-pool-size:1000spring.datasource.hikari.miniumIdle:10spring.datasource.hikari.idleTimeout:28000000spring.datasource.hikari.maxLifetime:28000000 性能优化配置按照前边的说法，合理启用 prepareStatement 缓存，可以大幅提升性能。官方推荐的配置可参考 mysql configuration 一个典型配置如下： 12345678910111213jdbcUrl=jdbc:mysql://localhost:3306/simpsonsuser=testpassword=testdataSource.cachePrepStmts=truedataSource.prepStmtCacheSize=250dataSource.prepStmtCacheSqlLimit=2048dataSource.useServerPrepStmts=truedataSource.useLocalSessionState=truedataSource.rewriteBatchedStatements=truedataSource.cacheResultSetMetadata=truedataSource.cacheServerConfiguration=truedataSource.elideSetAutoCommits=truedataSource.maintainTimeStats=false]]></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bean validator]]></title>
    <url>%2F2018%2F07%2F18%2Fbean-validator%2F</url>
    <content type="text"><![CDATA[bean validator 主要是验证一个 bean 的各字段是否满足一些约束，例如 @NotNull bean validation 有个规范 jsr 380，里边定义了一堆 api。有很多规范的实现，最常用的是 hibernate validator，jersey 出的 jersey-bean-validation 也是基于 hibernate validator 做的。 bean validator 一般是应用在 web 框架（如 spring、jersey）上，框架在反序列化 rest 请求到 bean 对象时，框架会调用 validator 根据 bean 对象的 annotation 对 bean 进行验证。 这个过程也可以手动进行。可参考 hibernate validator: get started。 引入依赖12345678// jsr 380 apicompile "javax.validation:validation-api:2.0.1.Final"// hibernate vaidator 实现testCompile "org.hibernate.validator:hibernate-validator:6.0.10.Final"// hibernate validator 依赖的 JSR 341 实现testCompile "org.glassfish:javax.el:3.0.0" 创建 bean，标明约束123456789101112131415161718192021222324package org.hibernate.validator.referenceguide.chapter01;import javax.validation.constraints.Min;import javax.validation.constraints.NotNull;import javax.validation.constraints.Size;public class Car &#123; @NotNull private String manufacturer; @NotNull @Size(min = 2, max = 14) private String licensePlate; @Min(2) private int seatCount; public Car(String manufacturer, String licencePlate, int seatCount)&#123; this.manufacturer = manufacturer; this.licensePlate = licencePlate; this.seatCount = seatCount; &#125;&#125; 验证 利用 Validation 获取默认的 ValidatorFactory：Validation.buildDefaultValidatorFactory()； 从 ValidatorFacotry 获取 Validator：factory.getValidator() 利用 Validator 验证 bean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package org.hibernate.validator.referenceguide.chapter01;import java.util.Set;import javax.validation.ConstraintViolation;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.ValidatorFactory;import org.junit.BeforeClass;import org.junit.Test;import static org.junit.Assert.assertEquals;public class CarTest &#123;private static Validator validator; @BeforeClass public static void setUp() &#123; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); validator = factory.getValidator(); &#125; @Test public void manufacturerIsNull() &#123; Car car = new Car( null, "DD-AB-123", 4 ); Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations = validator.validate( car ); assertEquals( 1, constraintViolations.size() ); assertEquals( "may not be null", constraintViolations.iterator().next().getMessage() ); &#125; @Test public void licensePlateTooShort() &#123; Car car = new Car( "Morris", "D", 4 ); Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations = validator.validate( car ); assertEquals( 1, constraintViolations.size() ); assertEquals( "size must be between 2 and 14", constraintViolations.iterator().next().getMessage() ); &#125; @Test public void seatCountTooLow() &#123; Car car = new Car( "Morris", "DD-AB-123", 1 ); Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations = validator.validate( car ); assertEquals( 1, constraintViolations.size() ); assertEquals( "must be greater than or equal to 2", constraintViolations.iterator().next().getMessage() ); &#125; @Test public void carIsValid() &#123; Car car = new Car( "Morris", "DD-AB-123", 2 ); Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations = validator.validate( car ); assertEquals( 0, constraintViolations.size() ); &#125;&#125;]]></content>
      <tags>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java reflection]]></title>
    <url>%2F2018%2F07%2F14%2Fjava-reflection%2F</url>
    <content type="text"><![CDATA[泛型java 泛型存在类型擦除（参见 java 泛型） 1234List&lt;String&gt; l1 = new ArrayList&lt;String&gt;();List&lt;Integer&gt; l2 = new ArrayList&lt;Integer&gt;();System.out.println(l1.getClass() == l2.getClass()); // return true, 两个都是 List.class 获取运行时泛型类型类型擦除使得根据类定义获取 runtime 泛型类型是不可能的，一般有几种方法(参见 stackoverflow)： 根据类对象实例获取，可参见 handle java generic types with reflection eg. Class&lt;T&gt; tClass = (Class&lt;T&gt;) ReflectionUtil.getClass(ReflectionUtil.getParameterizedTypes(this)[0]); 从父类中获取（要求父类有相同的泛型参数） eg. Class&lt;T&gt; tClass = (Class&lt;T&gt;) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0] 通过方法存储泛型类型为 field。但这意味着所有的 client 都必须要通过相应方法设置该 field 12345678910111213// 通过方法（constructor）存储泛型类型public class GenericClass&lt;T&gt; &#123; private final Class&lt;T&gt; type; public GenericClass(Class&lt;T&gt; type) &#123; this.type = type; &#125; public Class&lt;T&gt; getMyType() &#123; return this.type; &#125;&#125; 获取一个带泛型信息的 class 变量例如当使用 new ObjectMapper().readValue(string, someClass)，而 someClass 是包含泛型参数的类型（eg. List），如何获取这样的 class 变量？ 参见 stackOverflow，总结如下： 1234567891011121314151617import com.fasterxml.jackson.databind.ObjectMapper;ObjectMapper mapper = new ObjectMapper();// as ArrayMyClass[] myObjects = mapper.readValue(json, MyClass[].class);// as ListList&lt;MyClass&gt; myObjects = mapper.readValue(jsonInput, new TypeReference&lt;List&lt;MyClass&gt;&gt;()&#123;&#125;);// as List (another more generic way)List&lt;MyClass&gt; myObjects = mapper.readValue(jsonInput, mapper.getTypeFactory().constructCollectionType(List.class, MyClass.class));// as List (using TypeToken in Gson)Class&lt;List&lt;MyClass&gt;&gt; tClass = new TypeToken&lt;List&lt;MyClass&gt;() &#123; &#125;.getRawType(); // use c]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 aws lambda 中应用 jersey]]></title>
    <url>%2F2018%2F06%2F27%2Faws-lambda-jersey%2F</url>
    <content type="text"><![CDATA[使用 aws serverless java container 实现。 使用 aws cli创建项目，配置 aws cli123456789101112131415161718# 利用原型创建项目$ mvn archetype:generate -DgroupId=my.service -DartifactId=my-service -Dversion=1.0-SNAPSHOT \ -DarchetypeGroupId=com.amazonaws.serverless.archetypes \ -DarchetypeArtifactId=aws-serverless-jersey-archetype \ -DarchetypeVersion=1.1.3# 安装 aws cli$ pip install awscli# 配置 credentials$ aws configureAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLEAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEYDefault region name [None]: us-west-2Default output format [None]: json# 写代码，测试…… aws 安装之后一般要配置 credentials，详情可参见 aws cli 配置 打包部署1234567891011# package$ mvn clean package# 上传 package 到 s3（需要先创建 s3 bucket）$ aws s3 mb s3://BUCKET_NAME$ aws cloudformation package --template-file sam.yaml --output-template-file output-sam.yaml --s3-bucket &lt;YOUR S3 BUCKET NAME&gt;# 部署到 aws lambda$ aws cloudformation deploy --template-file output-sam.yaml --stack-name your-stack-name --capabilities CAPABILITY_IAM 查看部署结果1234567891011121314151617181920212223242526272829# 可查看 stack 详情$ aws cloudformation describe-stacks --stack-name your-stack-name&#123; "Stacks": [ &#123; "StackId": "arn:aws:cloudformation:us-west-2:xxxxxxxx:stack/ServerlessJerseyApi/xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx", "Description": "AWS Serverless Jersey API - learning.aws::aws-lambda-jersey", "Tags": [], "Outputs": [ &#123; "Description": "URL for application", "ExportName": "AwsLambdaJerseyApi", "OutputKey": "AwsLambdaJerseyApi", "OutputValue": "https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping" &#125; ], "CreationTime": "2016-12-13T22:59:31.552Z", "Capabilities": [ "CAPABILITY_IAM" ], "StackName": "ServerlessJerseyApi", "NotificationARNs": [], "StackStatus": "UPDATE_COMPLETE" &#125; ]&#125;# 根据生成的链接访问 api$ curl https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping 使用 sam-cliaws-cli 只能部署到远端去查看运行效果。而 sam-cli 则可以本地 simulate 一个 lambda 环境，从而实现本地调试。 创建项目aws-sam-cli 有个 sam init --runtime java，可以创建一个 sample 项目，但是目前支持的 template 并不包含 jersey 的。所以要创建项目还是通过 mvn archetype。 （所有支持的 runtime 可以查看 project status） 但是 sam init 创建了一个 template.yaml，这是之后所有 sam 命令的依据。这个文件会被翻译为 mvn archetype 创建的 sam.yaml。因为其后台是利用 aws cli 运行的。 所以我们需要在 mvn archetype 创建了原型之后，手动依据 sam.yaml 创建 template.yaml 1234# 利用原型创建项目，配置 aws-cli，同上# 创建 template.yaml$ cp sam.yaml template.yaml 部署项目123456789101112131415161718192021# package$ mvn clean package# 本地测试运行$ sam local start-api# 上传 package 到 s3（需要先创建 s3 bucket）$ aws s3 mb s3://BUCKET_NAME$ sam package \ --template-file template.yaml \ --output-template-file packaged.yaml \ --s3-bucket YOUR_S3_BUCKET_NAME# 部署到 aws lambda$ sam deploy \ --template-file packaged.yaml \ --stack-name your-stack-name \ --capabilities CAPABILITY_IAM \ --parameter-overrides MyParameterSample=MySampleValue 如果在本地运行时，报错 No class found....，一般是因为 docker 的原因，我最后是通过重装 docker 解决的。由于 aws-sam-cli 使用 docker 去起一个 lambda 容器环境，所以可能是由于 credentials 等获取不到的原因？，总之可能会挂掉。可参见 stackoverflow ticket， 查看部署结果这里跟上边是一样的，我们可以只看一部分 1234567891011121314151617# 可查看 stack 详情$ aws cloudformation describe-stacks \ --stack-name your-stack-name \ --query 'Stacks[].Outputs'[ [ &#123; "Description": "URL for application", "ExportName": "AwsLambdaJerseyApi", "OutputKey": "AwsLambdaJerseyApi", "OutputValue": "https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping" &#125; ]]# 根据生成的链接访问 api$ curl https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping¡]]></content>
      <tags>
        <tag>aws</tag>
        <tag>aws lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js global namespace]]></title>
    <url>%2F2018%2F06%2F21%2Fjs-global-namespace%2F</url>
    <content type="text"><![CDATA[最基本的js写法是：不管是要调用项目内或项目外的其他文件的方法，都是直接在当前文件中调用，然后在web文件（html）中利用 &lt;script&gt; 按依赖顺序引入所有的内部和外部文件。 即每个js中声明的变量都是全局变量。js采用{}来定义变量生命周期（或者说namespace），除了被｛｝所包围的其余变量都是全局变量。有个god object即window－－－全局对象，所有的变量、函数都是这个god object的member－－－全局变量。 利用 &lt;script&gt; 引入所有js，效果类似于将所有文件的内容组装到一个大文件运行。因此声明顺序受依赖关系约束。 即browser运行每个html时，它始终是将这个html中的所有script作为一个文件来运行。即browser是个解释执行器，它总是执行一个文件。而不同于后端（例如java 的jre）的编译运行， 前后端的区别： browser是解释执行器，而js不提供import机制，所以只能人工解决依赖确定关系－－－需要提供一种机制来解决这中依赖确定关系 后端执行是通过server来执行的，服务器会下载所需lib存放到服务器容器中，运行是直接从容器拿。而前端执行是通过browser来执行，每次执行js browser都需要下载依赖的各种js文件，然后将这些文件组合成一个来运行。当然有jquery之类的会利用缓存使得不需要每次执行都下载，然而这依然不是一个最佳解决方案－－－需要提供一种机制来解决依赖下载和组装。 前端 后端 开发生命周期 1. code 2. 简单组合所有文件 3. 解释一行为可执行码－－即时性 4. 执行一行 1. code（提供import） 2. 编译：确定依赖关系，据此依次编译各个文件为汇编码／机器码（其中有预编译、预处理等步骤） 3. 连接：将外部函数代码添加到上述文件中，组合成一个完整可执行文件。 4. 运行整个文件]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js ecosystem]]></title>
    <url>%2F2018%2F06%2F21%2Fjs-ecosystem-md%2F</url>
    <content type="text"><![CDATA[我在学习 react，一直在使用 create-react-app 创建项目。create-react-app 其实包括两个核心： create-react-app：主要提供了 command-line 工具，方便用户创建 react 项目 react-scripts：这才是核心。它封装了所有开发 react 项目的配置，使得用户可以零配置直接开始开发 react。用户基本不需要更新 create-react-app，因为它总是拉最新的 react-scripts，而 react-scripts 才是简化用户配置的核心。 问题来了，我在写测试的时候发现，react-scripts 默认配置使用的是 jest，而且版本较低（可运行 npm ls jest 查看依赖树，结果如下图所示）。而我需要使用的 data-driven-test 依赖包 jest-each 是 jest 23.0.0 以后的版本才有的。 所以我需要在测试的时候不使用默认安装的 jest， 读了一个 post: 自己写样板，不使用 create-react-app，受它的启发要自己配置 react 项目，那么就有必要了解 js ecosystem 中的一些工程。 webpack一个开源的前端打包工具，支持用户进行模块化开发（即用户开发很多 module，然后不同的 mudule 之间可通过 import, export 进行相互引用）。原本 js 是不支持的，可参见 js global namespace。 ESLint是一个 javascript 的语法检查器。类似于在 IDE 中写 java 时的即时编译检查。它是可以配置的，以完成你所需要的检查。 ECMAScriptES5, ES6, ES2016, ES.Next: What’s going on with JavaScript versioning? 翻译的这个文章讲得很好，我直接引用原文： ECMAScript：一个由 ECMA International 进行标准化，TC39 委员会进行监督的语言。通常用于指代标准本身。 JavaScript：ECMAScript 标准的各种实现的最常用称呼。这个术语并不局限于某个特定版本的 ECMAScript 规范，并且可能被用于任何不同程度的任意版本的 ECMAScript 的实现。 ECMAScript 5 (ES5)：ECMAScript 的第五版修订，于 2009 年完成标准化。这个规范在所有现代浏览器中都相当完全的实现了。 ECMAScript 6 (ES6) / ECMAScript 2015 (ES2015)：ECMAScript 的第六版修订，于 2015 年完成标准化。这个标准被部分实现于大部分现代浏览器。可以查阅这张兼容性表来查看不同浏览器和工具的实现情况。 ECMAScript 2016：预计的第七版 ECMAScript 修订，计划于明年夏季发布。这份规范具体将包含哪些特性还没有最终确定 ECMAScript Proposals：被考虑加入未来版本 ECMAScript 标准的特性与语法提案，他们需要经历五个阶段：Strawman（稻草人），Proposal（提议），Draft（草案），Candidate（候选）以及 Finished （完成）。 blog 里也讲了 ecmascript 的历史。简答总结一下： 很久以前（1996），网景浏览器把他们写的 javascript 交给 ECMA International（欧洲计算机制造协会）进行标准化 ECMA 1，2，3 版本很快发布，而且被各大浏览器厂商支持 ECMA 一直没有变化，各大浏览器厂商自行扩展（所以 javascript 其实是 ECMAScript 的实现和扩展） 某一年，ECMA 4 出来，太激进，被废弃了（只有 Adobe 实现了） 2009 年，ECMA 5（es5） 出来。但直到 2012 年才逐渐被公众接受 2015 年，es6 出来。与此同时，相关委员会决定每年定义一次新标准，避免等待整个草案完成。因此 ES6 也被命名为 ECMAScript 2015 一些资源： 如果你还不熟悉 ES6，Babel 有一个很不错的特性概览 如果你希望深入 ES6，这里有两本很不错的书： Axel Rauschmayer 的 Exploring ES6和 Nicholas Zakas 的 Understanding ECMAScript 6。Axel 的博客 2ality 也是很不错的 ES6 资源 babelbabel 是一个 javascript 编译器。其核心是一个语法转换器，能将使用 es6、flow、jsx 的代码转换为浏览器兼容的 javascript。所以这些项目 build 后其实是生成了浏览器兼容的 javascript。 为啥会有这个，原因就是上边说的，新版本 js 出来了，但是大家还不支持。]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js export]]></title>
    <url>%2F2018%2F06%2F15%2Fjs-export%2F</url>
    <content type="text"><![CDATA[export 用于从 module 中导出函数、对象、原始值，在其他地方通过 import 使用这些函数、对象、原始值。 有两种导出方式：命名导出（export），默认导出（export default） 1. 命名导出就是导出 module 中有命名的函数、对象、原始值。相应的，import 时，必须使用相同的命名引入（当然可以使用 import a as b from &#39;./module&#39; 来修改名称） 举例： 1234567891011121314151617// 定义时导出export const a = 'a', b = 'b'; // 适用于 var, letexport function a()&#123;&#125;;export class a&#123;&#125;;// 定义后导出const a = 'a', b = 'b';export &#123;a, b as newB&#125;;// 导出其他模块的导出。// 此时仅会导出 otherModule 的命名导出，所以这里最终导出的还是有命名的export * from 'otherModule';export &#123;a, b as newB&#125; from 'otherModule';// 如果需要导出 otherModule 的默认导出，只能这样写：import defaultExport from 'otherModule';export [default] defaultExport; 2. 默认导出默认导出（export default）的函数、类可以认为没有固定名称，即 import 时，可以用任何名称导入 一个 module 中只能有一个默认导出。 export default 后不能跟 const, let, var 举例： 123456789101112131415// 默认导出函数（定义时）export default function()&#123;&#125;import myname from 'module';export default function funcName()&#123;&#125;import funcNameMy from 'module2';// 默认导出类（定义时）export default class className&#123;&#125;// 默认导出 expressionexport default a = 'a'// 默认导出（定义后）export &#123;a as default, b, c as newC&#125;]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next 设置]]></title>
    <url>%2F2018%2F06%2F14%2Fhexo-next%2F</url>
    <content type="text"><![CDATA[使用 hexo + github 部署博客 介绍了怎么部署自己的博客，然后就开始无休止的调整主题。 我选定的主题是 next：有目录，也有集成搜索的文档，这是一个 example，参照 第三方集成 集成搜索等功能. next 优化配置可参考 这篇文章 配置文件我是采用两个配置文件的写法，即在 source/_data/next.yml 中写 next 相关的配置。 code highlight 配置按照主题设定教程，我设置的是 scheme: Mist。默认的代码 highlight 是用 tomorrow theme，按照 代码高亮设置教程，可以有五种选项。但是很多 code grammar 高亮显示无效，比如 jsx。所以想换一个 highlight 主题。 找了个 code highlight theme 配置教程 ，开始动手。]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[react setState 的坑]]></title>
    <url>%2F2018%2F06%2F05%2Freact-set-state%2F</url>
    <content type="text"><![CDATA[问题setState 是更新 state 的 API，进而会引发组件的重新渲染。然而在使用过程中发现有些坑(参见 react 正确使用状态)： setState(obj)一般情况下不会立即更新 state 的值； 同一 cycle 的多次 setState 调用可能会合并（性能考虑） 对于第一点，引用下边的例子： 12345function incrementMultiple() &#123; this.setState(&#123;count: this.state.count + 1&#125;); this.setState(&#123;count: this.state.count + 1&#125;); this.setState(&#123;count: this.state.count + 1&#125;);&#125; 代码运行时，虽然是对 state 加了三次，但是每次加操作都是针对初始的 state，所以最终相当于仅加了一次。即上述代码等同于下边的代码： 123456Object.assign( previousState, &#123;quantity: state.quantity + 1&#125;, &#123;quantity: state.quantity + 1&#125;, ...) 为什么从 react 生命周期看 setState 生效时间要理解其原因，我们要先看一下 react 生命周期： setState 引起状态更新涉及四个函数： shouldComponentUpdate（默认 true） componentWillUpdate render componentDidUpdate 但直到 render 被调用时，state 才被更新。 （如果 shouldComponentUpdate 返回 false，则 render 不会被执行，但是 state 会被更新） 因此多次调用 setState 时，不能依靠 this.state 来计算下一个 state 的值，因为 this.state 一直没更新。 setState() does not always immediately update the component. It may batch or defer the update until later. This makes reading this.state right after calling setState() a potential pitfall. 但是可以使用 setState(func) api 来解决上述问题 setState apisetState api 如下： 1setState(updater[, callback]) 其中，callback 是在 setState 完成且组件 re-render 完成后执行（一般建议用 componentDidUpdate 来实现类似逻辑）。 而 updater 可以是： obj：异步将 obj shallow merge 到旧 state，构建新 state func：形式如下： 1(prevState, props) =&gt; stateChange 解决方案上述例子做如下修改即可实现真正的累加： 123456789function increment(state) &#123; return &#123;count: state.count + 1&#125;;&#125;function incrementMultiple() &#123; this.setState(increment); this.setState(increment); this.setState(increment);&#125; 更详细请点击参考这篇文章]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript 表达式和操作符]]></title>
    <url>%2F2018%2F05%2F31%2Fjavascript-spread-operator%2F</url>
    <content type="text"><![CDATA[... 扩展运算符...obj 是 js 的扩展运算符，可以将一个可迭代的对象在函数调用的位置展开成为多个参数,或者在数组字面量中展开成多个数组元素。(其他可参见运算符介绍，运算符和表达式清单 reference) eg. 12345678910// 在数组字面量中展开// 利用扩展运算符，实现了数组合并var parts = ['shoulder', 'knees'];var lyrics = ['head', ...parts, 'and', 'toes'];// 在函数调用处展开function f(x, y, z) &#123; &#125;var args = [0, 1, 2];f(...args); template literalstemplate literals sql template strings]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript 解构语法]]></title>
    <url>%2F2018%2F05%2F31%2Fjavascript-destructuring-assignment%2F</url>
    <content type="text"><![CDATA[具体可参见 mdn javascript 解构语法. 这里简单总结一下。 解构是做什么的解构就是一种方便变量赋值的语法，由编译器完成真正的变量赋值 数组解构 将数组元素赋值给变量 赋值依据是元素顺序 指定变量名时，可以提供默认值，以避免 undefined 赋值 支持忽略一些元素（添加 , ，但不提供变量名） 支持 rest 数组赋值 eg. 123456789101112131415161718192021222324252627282930// 基本赋值var a, b, rest;[a, b] = [10, 20];console.log(a); // 10console.log(b); // 20// 默认值var a, b;[a=5, b=7] = [1];console.log(a); // 1console.log(b); // 7// 忽略某些元素function f() &#123; return [1, 2, 3];&#125;var [a, , b] = f();console.log(a); // 1console.log(b); // 3// rest 赋值[a, b, ...rest] = [10, 20, 30, 40, 50];console.log(a); // 10console.log(b); // 20console.log(rest); // [30, 40, 50] 对象解构和数组解构差不多： 将对象属性赋值给变量 赋值依据是属性名称 指定属性名时，可以给默认值，避免 undefined 赋值 变量名称默认是属性名，也可以自定义，通过 propertyName: customName 方式定义 支持 rest 对象赋值（在 proposal 中） eg. 1234567891011121314151617181920212223242526272829303132333435// 基本赋值(&#123; a, b &#125; = &#123; a: 10, b: 20 &#125;);console.log(a); // 10console.log(b); // 20// 默认值var &#123;a = 10, b = 5&#125; = &#123;a: 3&#125;;console.log(a); // 3console.log(b); // 5// 自定义变量名 + 默认值var o = &#123;p: 42, q: true&#125;;var &#123;p: foo, q: bar, m: other='haha'&#125; = o; console.log(foo); // 42 console.log(bar); // trueconsole.log(other); // haha// rest 赋值// Stage 3 proposal(&#123;a, b, ...rest&#125; = &#123;a: 10, b: 20, c: 30, d: 40&#125;);console.log(a); // 10console.log(b); // 20console.log(rest); //&#123;c: 30, d: 40&#125;// 无声明赋值// 相当于 var &#123;a,b&#125; = &#123;a:1, b:2&#125;;// 括号去掉， &#123;a,b&#125; 是块代码，不是对象，所以不能去掉var a, b;(&#123;a, b&#125; = &#123;a: 1, b: 2&#125;);]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redux]]></title>
    <url>%2F2018%2F05%2F29%2Fredux%2F</url>
    <content type="text"><![CDATA[what’s reduxredux 中文文档 中的几个关键特性： 状态容器，提供可预测化的状态管理 跨平台，客户端、服务端、原生应用都能用 易于测试 轻量，支持 react 等界面库 其中第一点，讲明了 redux 的主要用途：状态容器 以 react 为例，页面是渲染状态树得到，有静态状态（props）、动态状态（state）。通过在代码中 setState 来修改状态树，状态自上而下传递到各个子组件，最终触发组件树的重新渲染。 使用 redux，我们就将状态的定义和允许的迁移 function 挪出去，放到一个状态容器里，来有效管理组件的所有状态和状态迁移。此时 component 代码中就没有 state、setState 等定义了。 counter without redux以计数器为例，不使用 redux，即直接在本地定义 state，并通过 setState 修改状态，以实现重现渲染。(源代码) 1234567891011121314151617181920212223242526272829import React, &#123;Component&#125; from 'react';export default class Counter extends Component &#123; state = &#123; value: 0, &#125;; render() &#123; return ( &lt;div&gt; &#123;this.state.value&#125; &lt;button onClick=&#123;this.increment&#125;&gt;+&lt;/button&gt; &lt;button onClick=&#123;this.decrement&#125;&gt;-&lt;/button&gt; &lt;/div&gt; ); &#125; decrement = () =&gt; &#123; this.setState(&#123; value: this.state.value - 1 &#125;); &#125; increment = () =&gt; &#123; this.setState(&#123; value: this.state.value + 1 &#125;); &#125;&#125; counter with redux如果使用 redux，就是将原本的 state 存储到一个 store 中，通过 dispatch 一个 action(eg. {type: &#39;INCREMENT&#39;})触发状态变化，action 如何改变 state 是由一个纯函数 reducer 定义的。（源代码） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// counter.jsimport React, &#123;Component&#125; from 'react';export default class Counter extends Component &#123; render() &#123; return ( &lt;div&gt; &#123;this.props.store.getState()&#125; &lt;button onClick=&#123;this.increment&#125;&gt;+&lt;/button&gt; &lt;button onClick=&#123;this.decrement&#125;&gt;-&lt;/button&gt; &lt;/div&gt; ); &#125; decrement = () =&gt; &#123; // 改变内部 state 惟一方法是 dispatch 一个 action。 this.props.store.dispatch(&#123;type: 'DECREMENT'&#125;); &#125; increment = () =&gt; &#123; this.props.store.dispatch(&#123;type: 'INCREMENT'&#125;); &#125;&#125;// reducer.js/** * 这是一个 reducer，形式为 (state, action) =&gt; state 的纯函数。 * 描述了 action 如何把 state 转变成下一个 state。 */export default function counter(state = 0, action) &#123; switch (action.type) &#123; case 'INCREMENT': return state + 1; case 'DECREMENT': return state - 1; default: return state; &#125;&#125;// index.jsimport React from 'react';import * as ReactDOM from "react-dom";import Counter from "./Counter";import counter from "./reducer";import &#123;createStore&#125; from 'redux';const store = createStore(counter);const render = () =&gt; ReactDOM.render( &lt;Counter store=&#123;store&#125;/&gt;, document.getElementById('root'));render();// store 的订阅接口，注册 listener（这里即 render）来订阅 store 的状态更新。store.subscribe(render); why redux不使用 redux 时， 状态（model）、状态如何被改变（controller）、页面模板/html（view）是耦合在一块的。（react 提供的就是一种界面库，render 函数其实写的就是页面模板，将 model 填充进去，渲染得到最后的 view） 组件之间强耦合，难以确定状态的改变是在哪里被谁触发，bug 追踪难。 使用 redux 后， model 被抽出来了（即当前视图所对应的 model 对象），而且这个 model 对象不是一个贫血模型，它提供基本的 action 来保证 model 的完整性。 由于状态的变更只能通过 dispatch 来触发，解耦了父子组件之间的状态变更传递，易于定位. （可以利用 middleware 记录 state 变更日志，即可实现 state 变化过程透明和可预测） 核心概念redux 是一个状态容器，它解决了： 状态在哪：createStore() 状态是什么：store.getState() 状态怎么变： reducer，即 (preState, action) =&gt; nextState 函数 触发状态变更：store.dispatch(action) 谁关心状态变化：store.subscribe(callback) reducerReducers 指定了应用状态的变化如何响应 actions 并发送到 store 的，记住 actions 只是描述了有事情发生了这一事实，并没有描述应用如何更新 state。reducer 函数形式如下： 1(preState, action) &#x3D;&gt; nextState 因为 state 有很多属性，针对属性的处理也有很多，全放在一块就太大、难管理，所以可以拆 reducer（称为 slice reducer），再利用 combineReducer 组合。（redux 总是一个根，多个子：一个 store，多个子状态；一个根 reducer，多个 slice reducers） 使用 combineReducer 后有两个问题： store state 和 slice reducer state 的关系？ slice reducer 如何被调用执行？ store state 和 slice reducer state 的关系？官方 API 描述 combineReducers() 返回的 state 对象，会将传入的每个 reducer 返回的 state 按其传递给 combineReducers() 时对应的 key 进行命名，并将所有 slice reducer 返回的结果合并 举例： 1234567891011const reducer1 = (state='initA', action) =&gt; &#123;...&#125;const reducer2 = (state='initB', action) =&gt; &#123;...&#125;// 这里 `&#123;reducer1, reducer2&#125;` 使用的是 es2015 的 shorthand property names。// 等价于 `&#123;reducer1: reducer1, reducer2: reducer2&#125;`const store = createStore(combineReducer(&#123;reducer1, reducer2&#125;));console.log(store.getState()); //&#123;reducer1: 'initA', reducer2: 'initB'&#125;// 也可以指定 key 名称const store = createStore(combineReducer(&#123;a:reducer1, b:reducer2&#125;));console.log(store.getState()); //&#123;a: 'initA', b: 'initB'&#125; slice reducer 如何被调用执行？官方 API 描述 返回值：(Function)：一个调用 reducers 对象里所有 reducer 的 reducer，并且构造一个与 reducers 对象结构相同的 state 对象。 即 combinedReducer 会调用执行所有的 slice reducer，以实现分层处理 初始化 state有两种方式： 使用 createStore(reducers, [preloadedState], [enhancers]) 中的 preloadedState 使用 reducer 中的默认属性（function someReducer(state={defaultValue}, action){}） 这其中有两条规则： preloadedState 先于 reducer 去填充 state 创建 store 后，redux 会 dispatch 一个虚拟的 action 到 reducer，以触发其中的默认值来填充 state。 使用单一简单 reducer 使用 preloadedState 后，单一 reducer 的默认 state 赋值会失效，因为流程是这样的： 创建 store，并根据 preloadedState 填充 state； dispatch 虚拟 action 来使用 reducer 默认值填充 state。而此时 state 已经不是 undefind，因此，这个默认值填充是无效的 举一个例子： 12345678910// reducerfunction counterReducer(state=0, action) &#123;...&#125;// 仅使用 reducer 初始化const store = createStore(counterReducer);console.log(store.getState()); //0// 同时使用 preloadedState 和 reducerconst store = createStore(counterReducer, 42);console.log(store.getState()); //42 使用 combineReducer() 使用 preloadedState 后，使用了 combineReducers 的 reducer 默认 state 赋值可能会失效，因为流程是这样的： 创建 store，并根据 preloadedState 填充 state； dispatch 虚拟 action 来调用所有子 reducers， 如果 preloadedState 中定义了当前 reducer 的属性，则对当前 reducer，state 已经不是 undefind，此时默认值填充是无效的 如果 preloadedState 中没有定义当前 reducer 的属性，则对当前 reducer，state 是 undefind，默认值填充有效 同样看一个例子： 12345678910111213// reducerconst reducer1 = (state='initA', action) =&gt; &#123;...&#125;const reducer2 = (state='initB', action) =&gt; &#123;...&#125;const combinedReducer = combineReducers(&#123;a: reducer1, b: reducer2&#125;)// 仅使用 reducer 初始化const store = createStore(combinedReducer);console.log(store.getState()); //&#123;a: 'initA', b: 'initB'&#125;// 同时使用 preloadedState 和 reducerconst store = createStore(counterReducer, &#123;a: 'initInPreload'&#125;);console.log(store.getState()); //&#123;a: 'initInPreload', b: 'initB'&#125; 使用 combineReducers 时，preloadedState 必须包含 slice reducer 中的属性，与传入的 keys 保持同样的结构 (参见 createStore API 说明) 举例： 1234567891011const reducer1 = (state='initA', action) =&gt; &#123;...&#125;const combinedReducer = combineReducers(&#123;a: reducer1, b: reducer2&#125;)// 正确用法const store = createStore(combinedReducer, &#123;b: 'bala'&#125;);console.log(store.getState()); //&#123;a: 'initA', b: 'bala'&#125;;// 错误用法. 此时不会包含 c 属性。// 因为所有的 state 都必须映射到 reducer 上，必须保持同样的结构，映射不到的就会被忽略const store = createStore(combinedReducer, &#123;c: 'bala'&#125;);console.log(store.getState()); //&#123;a: 'initA', b: 'initB'&#125;; References usage with react 显示和容器组件 为什么用redux？原理？ 看漫画学redux]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 hexo + github 部署博客]]></title>
    <url>%2F2018%2F05%2F14%2Fbuild-blog-using-hexo%2F</url>
    <content type="text"><![CDATA[安装部署 hexo参考这个知乎文章安装 1. 准备 node、git2. 安装 hexo-cli1$ npm i -g hexo-cli 3. 创建一个网站1$ hexo init xxx.github.io 4. 部署服务器这里选择部署到 git 上。 首先安装 git deployer 然后修改配置文件，选择部署方式为 git 并配置 repo。 hexo 部署 访问 xxx.github.io 即可 12345678910$ npm install hexo-deployer-git --save$ vi _config.ymldeploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message]$ npm d 注意： 可以选择同时部署到多个服务器。多写几个 ‘deploy’ 配置即可 git 用户名、网站用户名（xxx.github.io 中的 xxx）必须相同。因为它相当于使用 github 服务器 设置 theme在 官方 themes 里挑。我比较喜欢以下几款： 带目录结构的： cactus：英文的，有几种颜色可以选，带目录，可以配置搜索，简洁，这是white 版本的 aircloud：英文中文都 ok，有目录，还可以搜索 next：有目录，也有集成搜索的文档，这是一个 example，参照 第三方集成 集成搜索等功能. next 优化配置可参考 这篇文章 yilia：有目录，有搜索，owner 博客 没有目录，没有 tag apollo：blog 是中文的，比较简单，颜色也好看 配置很简单，以 yilia 为例： 12345678910111213141516171819# 1. 找到 theme git，download 到 `themes` 文件夹下$ git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia# 2. 修改 hexo 根目录下的配置文件，指定使用该主题$ vi _config.ymltheme: yilia# 3. customize 主题配置. 可以修改 themes 中的 config，也可以直接在 hexo config 中修改## 修改 themes 中的 _config.yml$ vi themes/yilia/_config.yml.........## 修改 hexo config$ vi _config.ymltheme_config: ... ... ... 发表博客1. 创建博客利用命令创建一个博客，存放在 source/_posts/ 下。然后可以编辑这个文件。刷新页面就可以看到博客有更新了 1$ hexo new titlename 也可以直接把 md 文件 copy 到 source/_posts/ 下，可以添加 frong-matter 指定 category、tag 等。 2. 生成静态文件hexo g 会根据 md 生成 html、css 等静态文件。文章写完后，利用这个命令生成静态文件，然后再 hexo d 部署即可。 也可以使用下述命令（两个命令等价），指同时 generate + deploy 12$ hexo g -d$ hexo d -g hexo clean有时可能需要使用 hexo clean 清除缓存文件 db.json 和已生成的静态文件 public 1$ hexo clean 启动本地服务器（调试用）博客编辑完成后，可以先本地启动 server，看一下效果 1$ hexo s -s 参数指定仅仅启动静态模式，即创建博客后，必须要 hexo g 去生成 index.html 等（相当于发布），网站才会真正的更新。否则网站不会更新。这一般用于 production mode 一般编辑完文章后，可以先本地启动服务器，调试一下样式可不可以，然后再部署到服务器上]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
</search>
