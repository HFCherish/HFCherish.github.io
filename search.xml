<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>nginx 502 vs 504</title>
    <url>/2020/04/27/502-vs-504/</url>
    <content><![CDATA[<p><a href="https://juejin.im/post/5b54635ae51d451951133d85" target="_blank" rel="noopener">nginx 502 和 504 超时演示</a></p>
<blockquote>
<p>502 Bad Gateway: The server was acting as a <a href="https://www.wikiwand.com/en/Gateway_(telecommunications" target="_blank" rel="noopener">gateway</a>) or proxy and received an invalid response from the upstream server.</p>
<p>504: he server was acting as a gateway or proxy and did not receive a timely response from the upstream server.</p>
</blockquote>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p><strong>504 是 nginx 没有及时从上游服务获取响应，超时了：</strong></p>
<ul>
<li>上游服务响应慢，读取 response / 发送 request 超时（<code>upstream timed out (110: Operation timed out) **while** reading response header from upstream</code>）<ul>
<li>某些请求处理就是慢。此时就应该调大 <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_read_timeout" target="_blank" rel="noopener"><strong>proxy_read_timeout</strong></a> (默认 60s)</li>
<li>上游服务压力太大，响应变慢。此时可以增加上游服务的响应能力，也可以适当提升 <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_send_timeout" target="_blank" rel="noopener"><strong>proxy_send_timeout</strong></a>, <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_read_timeout" target="_blank" rel="noopener"><strong>proxy_read_timeout</strong></a></li>
</ul>
</li>
<li>连接上游服务超时。可能是上游服务已经断了，但由于 keepalive，nginx 依然保有 tcp 连接，但实际操作时，却连不上，就超时了。</li>
</ul>
<p><strong>502 是一般是上游服务器故障导致的</strong>。比如停机，进程被杀死，上游服务 reset 了连接，进程僵死等各种原因。在 nginx 的日志中我们能够发现 502 错误的具体原因，分别为：<code>104: Connection reset by peer</code>，<code>113: Host is unreachable</code>，<code>111: Connection refused</code></p>
]]></content>
  </entry>
  <entry>
    <title>Cache - MicroService</title>
    <url>/2020/06/03/Cache-MicroService/</url>
    <content><![CDATA[<h1 id="Where-is-my-cache-for-a-service"><a href="#Where-is-my-cache-for-a-service" class="headerlink" title="Where is my cache for a service"></a>Where is my cache for a service</h1><p><a href="Architectural Patterns for Caching Microservices">Architectural Patterns for Caching Microservices</a></p>
<p>Patterns:</p>
<ol>
<li><strong>embedded</strong>: save cache in the service</li>
<li><strong>client-server</strong>: a completely separate cache server</li>
<li><strong>reverse-proxy</strong>: put the cache in front of each service</li>
<li><strong>Sidecar</strong>: put the cache as a sidecar container that belongs to the service</li>
</ol>
<p>How does cache work? The application receives the request and checks if <strong>the same request</strong> was already executed (and stored in the cache)</p>
<h2 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h2><p><strong>Embedded Distributed Cache</strong></p>
<p>Why distributed?</p>
<ol>
<li>Same requests happen on different instances, and we want to use cache on the second same request no matter whether they reached the same instance.</li>
<li>An update request reached one instance, and we want all the cache of this resource on all instances should be updated.</li>
</ol>
<h2 id="Client-Server"><a href="#Client-Server" class="headerlink" title="Client-Server"></a>Client-Server</h2><p>pros:</p>
<ul>
<li>sperarate server, so you can use any programming language you want</li>
<li>separate management. you can scale up/down, do backup, design security separatly on need.</li>
</ul>
<p>cons:</p>
<ul>
<li>a new deployment &amp; related ops work (cloud can make this simple)</li>
</ul>
<h2 id="Sidecar"><a href="#Sidecar" class="headerlink" title="Sidecar"></a>Sidecar</h2><p>A mixture of Embedded &amp; client-server. (<a href="https://hazelcast.com/blog/hazelcast-sidecar-container-pattern/" target="_blank" rel="noopener">sidecar container pattern</a>)</p>
<p>Take k8s as example (now most of the sidecar pattern is implemented on k8s, becaused it supports this pattern inborn), we put the cache server with the service as separate containers on the same pod. The request reach the service container first, and then the service can access the cache server by <code>localhost</code> .</p>
<blockquote>
<p>The <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar" target="_blank" rel="noopener">sidecar pattern</a> is a technique of attaching an additional container to the main parent container so that both would share the same lifecycle and the same resources. You may think of it as a perfect tool for decomposing your application into reusable modules, in which each part is written in a different technology or programming language.</p>
</blockquote>
<h2 id="Reverse-proxy"><a href="#Reverse-proxy" class="headerlink" title="Reverse proxy"></a>Reverse proxy</h2><ol>
<li>Everytime you try to access the servcie, you go to the reverse proxy (e.g. nginx). </li>
<li>And the proxy will first check the cache for the request. If cache exists, return immediately, else, forward to the service and write the cache.</li>
</ol>
<p>pros: </p>
<ul>
<li>In this way, the service has no idea about the cache, so nothing will change on the service when cache introduced.</li>
</ul>
<p>cons:</p>
<ul>
<li>cannot use any application-based code to invalidate the cache.</li>
</ul>
<p>Also, you can put the reverse proxy into the side-car, that is, <strong>reverse-proxy-sidecar</strong> pattern.</p>
<p>Now, there is no mature <strong>HTTP Reverse Proxy Cache Sidecar</strong> solution at all. Nginx can do it, but it’s not a good choice since it’s not mature.</p>
<h1 id="Caching-Practices"><a href="#Caching-Practices" class="headerlink" title="Caching Practices"></a>Caching Practices</h1><ul>
<li>Always use caching in one place for a service. Mutliple caches will make the cache invalication and error-prone difficult.</li>
</ul>
]]></content>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title>Cache Memory</title>
    <url>/2020/06/28/Cache-Memory/</url>
    <content><![CDATA[<h1 id="General-Concept"><a href="#General-Concept" class="headerlink" title="General Concept"></a>General Concept</h1><p><img src="/images/cache-memory-simple.003.jpeg" alt="cache-memory-simple.003"></p>
<p><img src="/images/cache-memory-simple.004.jpeg" alt="cache-memory-simple.004"></p>
<p><img src="/images/cache-memory-simple.005.jpeg" alt="cache-memory-simple.005"></p>
<p><img src="/images/cache-memory-simple.006.jpeg" alt="cache-memory-simple.006"></p>
<h1 id="CPU-Core-Caching"><a href="#CPU-Core-Caching" class="headerlink" title="CPU Core Caching"></a>CPU Core Caching</h1><p><img src="/images/cache-memory-simple.008.jpeg" alt="cache-memory-simple.008"></p>
<p><img src="/images/cache-memory-simple.009.jpeg" alt="cache-memory-simple.009"></p>
<p><img src="/images/cache-memory-simple.010.jpeg" alt="cache-memory-simple.010"></p>
<p><img src="/images/cache-memory-simple.011.jpeg" alt="cache-memory-simple.011"></p>
<p><img src="/images/cache-memory-simple.012.jpeg" alt="cache-memory-simple.012"></p>
<p><img src="/images/cache-memory-simple.013.jpeg" alt="cache-memory-simple.013"></p>
<h1 id="Cache-Lines"><a href="#Cache-Lines" class="headerlink" title="Cache Lines"></a>Cache Lines</h1><p><img src="/images/cache-memory-simple.015.jpeg" alt="cache-memory-simple.015"></p>
<p><img src="/images/cache-memory-simple.016.jpeg" alt="cache-memory-simple.016"></p>
<h1 id="Cache-Memory"><a href="#Cache-Memory" class="headerlink" title="Cache Memory"></a>Cache Memory</h1><h2 id="Associative-Memory"><a href="#Associative-Memory" class="headerlink" title="Associative Memory"></a>Associative Memory</h2><p><img src="/images/cache-memory-simple.019.jpeg" alt="cache-memory-simple.019"></p>
<p><img src="/images/cache-memory-simple.020.jpeg" alt="cache-memory-simple.020"></p>
<p><img src="/images/cache-memory-simple.021.jpeg" alt="cache-memory-simple.021"></p>
<p><img src="/images/cache-memory-simple.022.jpeg" alt="cache-memory-simple.022"></p>
<h2 id="Direct-Mapped-Memory"><a href="#Direct-Mapped-Memory" class="headerlink" title="Direct-Mapped Memory"></a>Direct-Mapped Memory</h2><p><img src="/images/cache-memory-simple.024.jpeg" alt="cache-memory-simple.024"></p>
<p><img src="/images/cache-memory-simple.025.jpeg" alt="cache-memory-simple.025"></p>
<p><img src="/images/cache-memory-simple.026.jpeg" alt="cache-memory-simple.026"></p>
<p><img src="/images/cache-memory-simple.027.jpeg" alt="cache-memory-simple.027"></p>
<h2 id="Set-Associative-Memory"><a href="#Set-Associative-Memory" class="headerlink" title="Set Associative Memory"></a>Set Associative Memory</h2><p><img src="/images/cache-memory-simple.029.jpeg" alt="cache-memory-simple.029"></p>
<p><img src="/images/cache-memory-simple.030.jpeg" alt="cache-memory-simple.030"></p>
<p><img src="/images/cache-memory-simple.031.jpeg" alt="cache-memory-simple.031"></p>
<h1 id="Cache-Read-Write-Policies"><a href="#Cache-Read-Write-Policies" class="headerlink" title="Cache Read/Write Policies"></a>Cache Read/Write Policies</h1><p><img src="/images/cache-memory-simple.033.jpeg" alt="cache-memory-simple.033"></p>
<p><img src="/images/cache-memory-simple.034.jpeg" alt="cache-memory-simple.034"></p>
<p><img src="/images/cache-memory-simple.035.jpeg" alt="cache-memory-simple.035"></p>
<p><img src="/images/cache-memory-simple.036.jpeg" alt="cache-memory-simple.036"></p>
<p><a href="https://www.infoq.cn/article/cache-coherency-primer" target="_blank" rel="noopener">cache coherency</a></p>
<p>MESI protocol: (Modified, Exclusive, Shared, Invalid)</p>
<blockquote>
<ul>
<li><strong>I</strong>nvalid lines are cache lines that are either not present in the cache, or whose contents are known to be stale. For the purposes of caching, these are ignored. Once a cache line is invalidated, it’s as if it wasn’t in the cache in the first place.</li>
<li><strong>S</strong>hared lines are clean copies of the contents of main memory. Cache lines in the shared state can be used to serve reads but they can’t be written to. Multiple caches are allowed to have a copy of the same memory location in “shared” state at the same time, hence the name.</li>
<li><strong>E</strong>xclusive lines are also clean copies of the contents of main memory, just like the S state. The difference is that when one core holds a line in E state, no other core may hold it at the same time, hence “exclusive”. That is, the same line must be in the I state in the caches of all other cores.</li>
<li><strong>M</strong>odified lines are dirty; they have been locally modified. If a line is in the M state, it must be in the I state for all other cores, same as E. In addition, modified cache lines need to be written back to memory when they get evicted or invalidated – same as the regular dirty state in a write-back cache.</li>
</ul>
</blockquote>
<h1 id="Principle-Of-Locality"><a href="#Principle-Of-Locality" class="headerlink" title="Principle Of Locality"></a>Principle Of Locality</h1><p><img src="/images/cache-memory-simple.038.jpeg" alt="cache-memory-simple.038"></p>
<p><img src="/images/cache-memory-simple.039.jpeg" alt="cache-memory-simple.039"></p>
<p><img src="/images/cache-memory-simple.040.jpeg" alt="cache-memory-simple.040"></p>
<p><img src="/images/cache-memory-simple.041.jpeg" alt="cache-memory-simple.041"></p>
<p><img src="/images/cache-memory-simple.042.jpeg" alt="cache-memory-simple.042"></p>
<p><img src="/images/cache-memory-simple.043.jpeg" alt="cache-memory-simple.043"></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="/images/cache-memory-simple.045.jpeg" alt="cache-memory-simple.045"></p>
]]></content>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title>MSSQL: multiple cascade paths</title>
    <url>/2020/05/27/MSSQL-multiple-cascade-paths/</url>
    <content><![CDATA[<h1 id="Symptoms"><a href="#Symptoms" class="headerlink" title="Symptoms"></a>Symptoms</h1><p>You may receive the following error message when you create a FOREIGN KEY constraint: (<a href="https://support.microsoft.com/en-us/help/321843/error-message-1785-occurs-when-you-create-a-foreign-key-constraint-tha" target="_blank" rel="noopener">microsoft report</a>)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Server: Msg 1785, Level 16, State 1, Line 1 Introducing FOREIGN KEY constraint &#39;fk_two&#39; on table &#39;table2&#39; may cause cycles or multiple cascade paths. Specify ON DELETE NO ACTION or ON UPDATE NO ACTION, or modify other FOREIGN KEY constraints. Server: Msg 1750, Level 16, State 1, Line 1 Could not create constraint. See previous errors.</span><br></pre></td></tr></table></figure>
<p>For example, the table definition is like this:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">Table t1:</span></span><br><span class="line">	<span class="attr">Id:</span> <span class="string">primaryKey</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Table t2:</span></span><br><span class="line">	<span class="attr">Id:</span> <span class="string">primaryKey</span></span><br><span class="line">	<span class="attr">parent:</span> <span class="string">ForeignKey(t1,</span> <span class="string">Id)</span> <span class="string">on</span> <span class="string">cascade</span> <span class="string">delete</span></span><br><span class="line">	<span class="attr">child:</span> <span class="string">ForeignKey(t1,</span> <span class="string">Id)</span> <span class="string">on</span> <span class="string">cascade</span> <span class="string">delete</span></span><br><span class="line"><span class="comment"># this would raise the above exception</span></span><br></pre></td></tr></table></figure>
<h1 id="Cause"><a href="#Cause" class="headerlink" title="Cause"></a>Cause</h1><p>Basically, you can’t create multiple cascade paths to same table with cascade delete/update. Since you may define <code>t2.parent</code> with cascade delete, <code>t2.child</code> with cascade update(e.g. update as null), this will make sql server ambiguous when t1 is deleted. MSSQL doesn’t detect whether there’re actual circle or not, it just forbids the operation to make the design simple.</p>
<blockquote>
<p>A table cannot appear more than one time in a list of all the cascading referential actions that are started by either a DELETE or an UPDATE statement. For example, the tree of cascading referential actions must only have one path to a particular table on the cascading referential actions tree.</p>
</blockquote>
<p><a href="https://stackoverflow.com/questions/851625/foreign-key-constraint-may-cause-cycles-or-multiple-cascade-paths" target="_blank" rel="noopener">stackoverflow</a></p>
<blockquote>
<p>SQL Server does simple counting of cascade paths and, rather than trying to work out whether any cycles actually exist, it assumes the worst and refuses to create the referential actions (CASCADE): you can and should still create the constraints without the referential actions. If you can’t alter your design (or doing so would compromise things) then you should consider using triggers as a last resort.</p>
<p>FWIW resolving cascade paths is a complex problem. Other SQL products will simply ignore the problem and allow you to create cycles, in which case it will be a race to see which will overwrite the value last, probably to the ignorance of the designer (e.g. ACE/Jet does this). I understand some SQL products will attempt to resolve simple cases. Fact remains, SQL Server doesn’t even try, plays it ultra safe by disallowing more than one path and at least it tells you so.<br>Microsoft themselves <a href="https://support.microsoft.com/en-us/help/321843/error-message-1785-occurs-when-you-create-a-foreign-key-constraint-tha" target="_blank" rel="noopener">advises</a> the use of triggers instead of FK constraints.</p>
</blockquote>
<h1 id="Workaround"><a href="#Workaround" class="headerlink" title="Workaround"></a>Workaround</h1><p>Use trigger instead of ForeignKey to keep the integrity and avoid the exceptions.</p>
<ol>
<li>Set cascade delete on t2.child</li>
<li>Use <code>instead of</code> trigger to cascade delete on t2.parent (you can also use trigger for all fields instead of foreign key constraints)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TRIGGER [DELETE_t2]</span><br><span class="line">   ON dbo.[t1]</span><br><span class="line">   INSTEAD OF DELETE</span><br><span class="line">AS </span><br><span class="line">BEGIN</span><br><span class="line"> SET NOCOUNT ON;</span><br><span class="line"> DELETE FROM [t2] WHERE parent IN (SELECT Id FROM DELETED)</span><br><span class="line"> DELETE FROM [t1] WHERE Id IN (SELECT Id FROM DELETED)</span><br><span class="line">END</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title>FluentValidator</title>
    <url>/2020/01/17/FluentValidator/</url>
    <content><![CDATA[<p><a href="https://fluentvalidation.net/start#setting-the-cascade-mode" target="_blank" rel="noopener">FluentValidation</a></p>
<h1 id="Knowledge"><a href="#Knowledge" class="headerlink" title="Knowledge"></a>Knowledge</h1><ul>
<li>The <code>RuleFor</code> method create a validation rule.</li>
</ul>
<blockquote>
<p>To specify a validation rule for a particular property, call the <code>RuleFor</code> method, passing a lambda expression that indicates the property that you wish to validate. </p>
</blockquote>
<ul>
<li>Rules are run syncronously</li>
</ul>
<blockquote>
<p>By default, all rules in FluentValidation are separate and cannot influence one another. This is intentional and necessary for asynchronous validation to work.</p>
</blockquote>
<ul>
<li><code>Must</code>, <code>NotNull</code>…. are <a href="https://fluentvalidation.net/built-in-validators" target="_blank" rel="noopener">built-in validators</a>. <code>WithMessage</code> is a method on a validator. <code>When</code> defines condition for validator(s).</li>
<li>Append multiple validators on a same property are called chaining validators.</li>
<li>Chainning Validators are executed by sequence. <code>CascadeMode</code> can define how these chaining validators are exectued. <code>Continue</code> is default which means even a validator fail, the latter validators will still be invoked. <code>StopOnFirstFailure</code> will stop at the first failure of these chaining validators.</li>
<li><code>When</code> defines condition for validator(s)/rules. <code>ApplyConditionTo.AllValidators</code> is the default setting. <code>ApplyConditionTo.CurrentValidator</code>  will make the condition only work to the preceding validator.</li>
</ul>
<blockquote>
<p>By default FluentValidation will apply the condition to all preceding validators in the same call to <code>RuleFor</code></p>
</blockquote>
]]></content>
      <tags>
        <tag>DTO validator</tag>
      </tags>
  </entry>
  <entry>
    <title>MPP (Massively Parallel Processing)</title>
    <url>/2020/11/10/MPP/</url>
    <content><![CDATA[<h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://zhuanlan.zhihu.com/p/148621151" target="_blank" rel="noopener">5分钟了解MPP数据库</a></p>
<p>MPP (Massively Parallel Processing)，即大规模并行处理。简单来说，MPP是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果(与Hadoop相似，但主要针对大规模关系型数据的分析计算)。</p>
<h2 id="MPP架构特征"><a href="#MPP架构特征" class="headerlink" title="MPP架构特征"></a>MPP架构特征</h2><ul>
<li>任务并行执行;</li>
<li>数据分布式存储(本地化);</li>
<li>分布式计算;</li>
<li>私有资源;</li>
<li>横向扩展;</li>
<li><a href="#share-nothing">Shared Nothing</a>架构。</li>
</ul>
<h1 id="MPPDB-v-s-Hadoop"><a href="#MPPDB-v-s-Hadoop" class="headerlink" title="MPPDB v.s. Hadoop"></a>MPPDB v.s. Hadoop</h1><p><a href="https://www.zhihu.com/question/22799482/answer/81615602" target="_blank" rel="noopener">知乎-为什么说HADOOP扩展性优于MPP架构的关系型数据库？</a></p>
<p>hadoop 和 MPPDB <strong>最大的区别在于：对数据管理理念的不同。</strong> </p>
<ol>
<li>HDFS/Hadoop 对于数据管理是<strong>粗放型管理</strong>，以一个文件系统的模式，让用户根据文件夹层级，把文件直接塞到池子里。处理也<strong>以批处理为主，就是拼命 scan</strong>。如果想在一大堆数据里找符合条件的数据，hadoop 就是粗暴的把所有文件从头到尾 scan 一遍，因为对于这些文件他没有索引、分类等，他管的少，知道的也少，用的时候每次就要全 scan。</li>
<li>数据库的本质在于数据管理，对外提供在线访问、增删改查等一系列操作。数据库的<strong>内存管理比较精细</strong>，有一套很完善的数据管理和分布体系。如果想在一大堆数据里找符合条件的数据，他可以根据分区信息先落到某个机器，再根据多维分区落到某个文件，再在文件里通过索引数据页的树形结构查询，可以直接定位到记录。</li>
<li>因为这样的基本理念不同，使得 hadoop 的扩展只需要简单的增加机器，内部平衡和迁移 data block；而数据库的扩充则涉及到数据拓扑结构的变更、或者不同机器间数据的迁移，当变化迁移的时候，依然需要维护分区、索引等，这种复杂度就比粗放的 HDFS 要高很多了。目前两者的存储模型不同。hadoop 用的是 HDFS，而 MPP 需要自己切分扩展。HDFS 扩展是通过元数据做的，name node 存元数据，增加一个节点，修改元数据就行，所以 HDFS 的扩展能力受到管理元数据的机器（name node） 的性能限制，一般来说可以到 10k 的规模。但 MPP 采用没有中心节点的存储模型，比如 hash，每次增加节点，都需要 rehash，规模增加到几百台的时候，扩展能力就有下降下来了。</li>
</ol>
<p>通常来讲，MPP数据库有对SQL的完整兼容和一些事务的处理能力，Hadoop在处理非结构化和半结构化数据上具备优势，所以MPP适合多维度数据自助分析、数据集市等；Hadoop适合海量数据存储查询、批量数据ETL、非结构化数据分析(日志分析、文本分析)等海量<strong>数据批处理</strong>应用要求。但通过 sql 还是 map-reduce 来查，其实只是一种查询形式。目前也有很多 sql on hadoop 的方案，例如 impala （sql on hadoop，其实是一个 MPP engine，所以它的查询性能会更好，提供更低的延迟和更少的处理时间）、spark Sql 等。现在Spark的重点都在Spark SQL，因为它已经不仅仅是SQL了，而是新的 “spark core”。（详见最后链接中Reynold Xin对此的解释）</p>
<blockquote>
<p>Spark SQL is not just about SQL. It turns out the primitives required for general data processing (eg ETL) are not that different from the relational operators, and that is what Spark SQL is. Spark SQL is the new Spark core with the Catalyst optimizer and the Tungsten execution engine, which powers the DataFrame, Dataset, and last but not least SQL.</p>
</blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-2195887a063e35952ffb9c94d3e18755_720w.jpg" alt="img"></p>
<h1 id="常用的MPP数据库有哪些"><a href="#常用的MPP数据库有哪些" class="headerlink" title="常用的MPP数据库有哪些"></a>常用的MPP数据库有哪些</h1><ul>
<li>自我管理的数据仓库<ul>
<li>HPE vertica</li>
<li>MemSql</li>
<li>Teradata</li>
</ul>
</li>
<li>按需 MPP 数据库<ul>
<li>aws redshift</li>
<li>azure sql 数据仓库</li>
<li>google bigQuery</li>
</ul>
</li>
<li><strong>GreenPlum</strong></li>
<li>Sybase IQ</li>
<li>TD Aster Data</li>
</ul>
<h1 id="Share-Nothing"><a href="#Share-Nothing" class="headerlink" title="Share Nothing"></a>Share Nothing<a name="share-nothing" /></h1><p><a href="https://www.cnblogs.com/kzwrcom/p/6397709.html" target="_blank" rel="noopener">数据库架构设计的三种模式：share nothing , share everythong , share disk</a></p>
<p>数据库构架设计中主要有Shared Everthting、Shared Nothing、和Shared Disk：</p>
<ol>
<li><p>Shared Everthting:一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer</p>
</li>
<li><p>Shared Disk：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。</p>
</li>
<li>Shared Nothing：各个处理单元都有自己私有的CPU/内存/硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表DB2 DPF和Hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。<br>我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。</li>
</ol>
<h1 id="GreenPlum"><a href="#GreenPlum" class="headerlink" title="GreenPlum"></a>GreenPlum</h1><p><a href="https://gpdb.docs.pivotal.io/5280/admin_guide/intro/arch_overview.html#arch_segments" target="_blank" rel="noopener">greenplum</a></p>
]]></content>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>NHibernate: inverse, cascade</title>
    <url>/2020/05/25/NHibernate-inverse-cascade/</url>
    <content><![CDATA[<p><a href="https://dzone.com/articles/playing-nhibernate-inverse-and" target="_blank" rel="noopener">playing-nhibernate-inverse-and-cascade</a>,</p>
<p><a href="https://web.archive.org/web/20080415101633/http://simoes.org/docs/hibernate-2.1/155.html" target="_blank" rel="noopener">nhibernate-inverse</a></p>
<p><a href="https://nhibernate.info/doc/nhibernate-reference/collections.html#collections-bidirectional" target="_blank" rel="noopener">bidirectional associations</a></p>
<p>In database, there may be biodirectional relationships, e.g. Parent has multiple child, and Child has a parent.<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#### class definition</span></span><br><span class="line"><span class="attr">class Parent:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">String</span> <span class="string">id</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">IList&lt;Child&gt;</span> <span class="string">childs</span>	</span><br><span class="line"><span class="attr">class Child:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">String</span> <span class="string">id</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">Parent</span> <span class="string">parent</span></span><br><span class="line">	</span><br><span class="line"><span class="comment">#### db definition</span></span><br><span class="line"><span class="attr">table Parent:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">id</span></span><br><span class="line"><span class="attr">table Child:</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">id</span></span><br><span class="line">	<span class="bullet">-</span> <span class="string">parentId</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Inverse"><a href="#Inverse" class="headerlink" title="Inverse"></a>Inverse</h2><p><code>Inverse</code> focus on the association. It defines which side is responsible of the association maintenance (create, update, delete), that is, the assignment of <code>parentId</code> column. It doesn’t care about the maintenance of associated objects which is what <code>cascade</code> cares).</p>
<p>By default, <code>invert=false</code>, then the assignment of <code>parentId</code> is maintened when parent is created/updated/deleted. If we set <code>parent.child.inverse=true</code> and the <code>child.parent is not-null</code>, then the assignment of <code>parentId</code> is maintened when child is created/updated/deleted. </p>
<blockquote>
<p><code>many-to-one</code> is <strong>always</strong> <code>inverse=&quot;false&quot;</code> (the attribute does not exist), that is, it means nothing to set <code>child.parent.inverse=true</code></p>
</blockquote>
<h2 id="Cascade"><a href="#Cascade" class="headerlink" title="Cascade"></a>Cascade</h2><p><code>cascade</code> instead will focus on the associated objects. It defines if the current object is responsible of the maintenance of associated objects.</p>
<p>By default, <code>cascade=None</code>, that is, when saving parent, the childs on parent won’t be saved cascadelly.</p>
<p>See <a href="https://stackoverflow.com/questions/1994433/nhibernate-cascade" target="_blank" rel="noopener">cascade stackoverflow</a></p>
<blockquote>
<ul>
<li>none - do not do any cascades, let users handle them by themselves.</li>
<li>save-update - when the object is saved/updated, check the associations and save/update any object that requires it (including save/update the associations in many-to-many scenario).</li>
<li>delete - when the object is deleted, delete all the objects in the association.</li>
<li>delete-orphan - when the object is deleted, delete all the objects in the association. In addition, when an object is removed from the association and not associated with another object (orphaned), also delete it.</li>
<li>all - when an object is save/update/delete, check the associations and save/update/delete all the objects found.</li>
<li>all-delete-orphan - when an object is save/update/delete, check the associations and save/update/delete all the objects found. In additional to that, when an object is removed from the association and not associated with another object (orphaned), also delete it.</li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title>VPN</title>
    <url>/2020/05/06/VPN/</url>
    <content><![CDATA[<p><a href="https://www.youtube.com/watch?v=q4P4BjjXghQ" target="_blank" rel="noopener">the great video</a></p>
<h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><h2 id="Why-does-we-create-internet"><a href="#Why-does-we-create-internet" class="headerlink" title="Why does we create internet?"></a>Why does we create internet?</h2><p>It’s created from the need of American military, to protect the communication in the wars. </p>
<p>The old communication system, phone, connects Lily with Tom through fixed central offices. If some of the central offices are destroyed by nuclear, then the rerouting of the communication line is difficult, that the commnunication will fail.</p>
<p>Ta Da.. Internet comes. It communicates through millions of routers. Even half of the routers are destroyed, there may still be the way to communicate.</p>
<h2 id="Why-does-we-create-VPN-virtual-private-network"><a href="#Why-does-we-create-VPN-virtual-private-network" class="headerlink" title="Why does we create VPN (virtual private network)?"></a>Why does we create VPN (virtual private network)?</h2><p>Internet way secure the communication from high level, however, the data from a computer are in danger now. If a hacker can get all the data from a router, then data of users are leaked since it will go through the router.</p>
<p>To protect the communication from one to another, you can build physical private line. However, it’s too expensive, and <a href="https://en.wikipedia.org/wiki/Virtual_private_network" target="_blank" rel="noopener">VPN</a> provides another way. The VPN secure the data in the following ways:</p>
<ol>
<li>creates tunnel between two communicators, </li>
<li>the data in the tunnel is encrepted</li>
<li>if the tunnel detects attack, it will destroy the old tunnel &amp; create a new one.</li>
</ol>
<p>Client-Server infrastructure. We need to know:</p>
<ol>
<li><p>external address (the address of vpn server)</p>
</li>
<li><p>username &amp; password</p>
</li>
</ol>
<p>These factors may influence the VPN quality:</p>
<ol>
<li><p>hacker penetrate. (like said above, during the rebuilt of tunnel, packets maybe lost)</p>
</li>
<li><p>old wiring, which will cause frequent tunnel rebuilt, too</p>
</li>
<li><p>old routers that not allow vpn passing through</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>networking</tag>
      </tags>
  </entry>
  <entry>
    <title>asure storage</title>
    <url>/2018/07/27/asure-storage/</url>
    <content><![CDATA[<p>asure storage 提供四种存储支持（<a href="https://www.youtube.com/watch?v=y6bIUHtdp6Y" target="_blank" rel="noopener">asure storage overview (youtube)</a>）：</p>
<ul>
<li><a href="https://azure.microsoft.com/zh-cn/services/storage/blobs/" target="_blank" rel="noopener">blob</a> (binary large object)：二进制数据存储。有两种：page blog（只能新增/删除/向已有数据附加数据，不能修改数据）；block blog（可以更新）</li>
<li><a href="https://azure.microsoft.com/zh-cn/services/storage/tables/" target="_blank" rel="noopener">table</a>：存储表格（nosql）</li>
<li><a href="https://azure.microsoft.com/zh-cn/services/storage/queues/" target="_blank" rel="noopener">queue</a>：有库，有 rest api</li>
<li><a href="https://azure.microsoft.com/en-us/services/storage/files/" target="_blank" rel="noopener">files</a>：好像主要用在文件共享的时候，就是类似于 windows server 上的文件共享（<a href="https://zh.wikipedia.org/wiki/%E4%BC%BA%E6%9C%8D%E5%99%A8%E8%A8%8A%E6%81%AF%E5%8D%80%E5%A1%8A" target="_blank" rel="noopener">smb(server message block)</a>），实现和使用方式都和 windows server 的文件共享一样。所以需要支持例如按 /servername/filename 等方式来 share 和 使用文件。它是构建于 blob 之上的。</li>
</ul>
<h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><h2 id="Storage-Accout"><a href="#Storage-Accout" class="headerlink" title="Storage Accout"></a>Storage Accout</h2><p>storage 的所有存储都必须在一个 storage account 内发生。这有点类似于一个 database。</p>
<p>安全也是在这里实现：</p>
<ol>
<li><strong><em>key</em></strong>：创建 account 时，就会生成俩 key，primary key 就是你用来登录访问数据的 key。不过这种方式对于有 client 时不太方便，因为可能不能 share key</li>
<li><strong><em>saas token</em></strong>：就是可以登录认证获得 token，然后拿 token 访问数据。</li>
</ol>
<h2 id="数据容器-数据"><a href="#数据容器-数据" class="headerlink" title="数据容器 + 数据"></a>数据容器 + 数据</h2><p>在每个 storage account 里，你可以创建上述四种类型的存储容器，去存放数据。</p>
<ul>
<li>Blob： 在 storage account 里可以创建 <strong><em>blob container</em></strong>，在 <strong><em>blog container</em></strong> 中存放 <strong><em>blob</em></strong></li>
<li>table：在 storage account 里创建 <strong><em>table</em></strong>，在 <strong><em>table</em></strong> 中存放 <strong><em>data entity</em></strong></li>
<li>queue：在 storage account 里创建 <strong><em>queue</em></strong>，在 <strong><em>queue</em></strong> 中存放 <strong><em>message</em></strong></li>
<li>file：在 storage account 里创建 <strong><em>asure files</em></strong> (类似于 <a href="https://zh.wikipedia.org/wiki/%E4%BC%BA%E6%9C%8D%E5%99%A8%E8%A8%8A%E6%81%AF%E5%8D%80%E5%A1%8A" target="_blank" rel="noopener">smb(server message block)</a>)??? 感觉应该也是先创建一个 files container，再创建一个个的 asure file（即 smb）</li>
</ul>
]]></content>
      <tags>
        <tag>cloud</tag>
        <tag>asure</tag>
      </tags>
  </entry>
  <entry>
    <title>airflow</title>
    <url>/2020/12/18/airflow/</url>
    <content><![CDATA[<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><p><a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html" target="_blank" rel="noopener">quickstart</a></p>
<blockquote>
<p>Airflow is published as <code>apache-airflow</code> package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open and applications usually pin them, but we should do neither and both at the same time. We decided to keep our dependencies as open as possible (in <code>setup.cfg</code> and <code>setup.py</code>) so users can install different version of libraries if needed. This means that from time to time plain <code>pip install apache-airflow</code> will not work or will produce unusable Airflow installation.</p>
<p>In order to have repeatable installation, however, starting from <strong>Airflow 1.10.10</strong> and updated in <strong>Airflow 1.10.13</strong> we also keep a set of “known-to-be-working” constraint files in the <code>constraints-master</code> and <code>constraints-1-10</code> orphan branches. Those “known-to-be-working” constraints are per major/minor python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow version and python versions in the URL.</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip3 install --use-deprecated legacy-resolver <span class="string">"apache-airflow==1.10.14"</span> --constraint <span class="string">"https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt"</span> </span><br><span class="line"></span><br><span class="line">pip3 install <span class="string">"apache-airflow==1.10.14"</span> --constraint <span class="string">"https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>On November 2020, new version of PIP (20.3) has been released with a new, 2020 resolver. This resolver does not yet work with Apache Airflow and might leads to errors in installation - depends on your choice of extras. In order to install Airflow you need to either downgrade pip to version 20.2.4 <code>pip upgrade --pip==20.2.4</code> or, in case you use Pip 20.3, you need to add option <code>--use-deprecated legacy-resolver</code> to your pip install command.</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># airflow needs a home, ~/airflow is the default,</span></span><br><span class="line"><span class="comment"># but you can lay foundation somewhere else if you prefer</span></span><br><span class="line"><span class="comment"># (optional)</span></span><br><span class="line"><span class="built_in">export</span> AIRFLOW_HOME=~/airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># install from pypi using pip</span></span><br><span class="line">pip install apache-airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the database</span></span><br><span class="line">airflow db init</span><br><span class="line"></span><br><span class="line">airflow users create \</span><br><span class="line">    --username admin \</span><br><span class="line">    --firstname petrina \</span><br><span class="line">    --lastname zheng \</span><br><span class="line">    --role Admin \</span><br><span class="line">    --email spiderman@superhero.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the web server, default port is 8080</span></span><br><span class="line">airflow webserver --port 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the scheduler</span></span><br><span class="line"><span class="comment"># open a new terminal or else run webserver with ``-D`` option to run it as a daemon</span></span><br><span class="line">airflow scheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># visit localhost:8080 in the browser and use the admin account you just</span></span><br><span class="line"><span class="comment"># created to login. Enable the example_bash_operator dag in the home page</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>schedule</tag>
        <tag>biodata</tag>
      </tags>
  </entry>
  <entry>
    <title>在 aws lambda 中应用 jersey</title>
    <url>/2018/06/27/aws-lambda-jersey/</url>
    <content><![CDATA[<p>使用 <a href="https://github.com/awslabs/aws-serverless-java-container" target="_blank" rel="noopener">aws serverless java container</a> 实现。</p>
<h1 id="使用-aws-cli"><a href="#使用-aws-cli" class="headerlink" title="使用 aws cli"></a>使用 <a href="https://aws.amazon.com/cn/cli/" target="_blank" rel="noopener">aws cli</a></h1><h2 id="创建项目，配置-aws-cli"><a href="#创建项目，配置-aws-cli" class="headerlink" title="创建项目，配置 aws cli"></a>创建项目，配置 aws cli</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用原型创建项目</span></span><br><span class="line">$ mvn archetype:generate -DgroupId=my.service -DartifactId=my-service -Dversion=1.0-SNAPSHOT \</span><br><span class="line">       -DarchetypeGroupId=com.amazonaws.serverless.archetypes \</span><br><span class="line">       -DarchetypeArtifactId=aws-serverless-jersey-archetype \</span><br><span class="line">       -DarchetypeVersion=1.1.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 aws cli</span></span><br><span class="line">$ pip install awscli</span><br><span class="line"><span class="comment"># 配置 credentials</span></span><br><span class="line">$ aws configure</span><br><span class="line">AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE</span><br><span class="line">AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</span><br><span class="line">Default region name [None]: us-west-2</span><br><span class="line">Default output format [None]: json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写代码，测试……</span></span><br></pre></td></tr></table></figure>
<p>aws 安装之后一般要配置 credentials，详情可参见 <a href="https://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-getting-started.html" target="_blank" rel="noopener">aws cli 配置</a></p>
<h2 id="打包部署"><a href="#打包部署" class="headerlink" title="打包部署"></a>打包部署</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># package</span></span><br><span class="line">$ mvn clean package</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传 package 到 s3（需要先创建 s3 bucket）</span></span><br><span class="line">$ aws s3 mb s3://BUCKET_NAME</span><br><span class="line">$ aws cloudformation package --template-file sam.yaml --output-template-file output-sam.yaml --s3-bucket &lt;YOUR S3 BUCKET NAME&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署到 aws lambda</span></span><br><span class="line">$ aws cloudformation deploy --template-file output-sam.yaml --stack-name your-stack-name --capabilities CAPABILITY_IAM</span><br></pre></td></tr></table></figure>
<h2 id="查看部署结果"><a href="#查看部署结果" class="headerlink" title="查看部署结果"></a>查看部署结果</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可查看 stack 详情</span></span><br><span class="line">$ aws cloudformation describe-stacks --stack-name your-stack-name</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"Stacks"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"StackId"</span>: <span class="string">"arn:aws:cloudformation:us-west-2:xxxxxxxx:stack/ServerlessJerseyApi/xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx"</span>, </span><br><span class="line">            <span class="string">"Description"</span>: <span class="string">"AWS Serverless Jersey API - learning.aws::aws-lambda-jersey"</span>, </span><br><span class="line">            <span class="string">"Tags"</span>: [], </span><br><span class="line">            <span class="string">"Outputs"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"Description"</span>: <span class="string">"URL for application"</span>,</span><br><span class="line">                    <span class="string">"ExportName"</span>: <span class="string">"AwsLambdaJerseyApi"</span>,  </span><br><span class="line">                    <span class="string">"OutputKey"</span>: <span class="string">"AwsLambdaJerseyApi"</span>,</span><br><span class="line">                    <span class="string">"OutputValue"</span>: <span class="string">"https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping"</span></span><br><span class="line">                &#125;</span><br><span class="line">            ], </span><br><span class="line">            <span class="string">"CreationTime"</span>: <span class="string">"2016-12-13T22:59:31.552Z"</span>, </span><br><span class="line">            <span class="string">"Capabilities"</span>: [</span><br><span class="line">                <span class="string">"CAPABILITY_IAM"</span></span><br><span class="line">            ], </span><br><span class="line">            <span class="string">"StackName"</span>: <span class="string">"ServerlessJerseyApi"</span>, </span><br><span class="line">            <span class="string">"NotificationARNs"</span>: [], </span><br><span class="line">            <span class="string">"StackStatus"</span>: <span class="string">"UPDATE_COMPLETE"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据生成的链接访问 api</span></span><br><span class="line">$ curl https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping</span><br></pre></td></tr></table></figure>
<h1 id="使用-sam-cli"><a href="#使用-sam-cli" class="headerlink" title="使用 sam-cli"></a>使用 <a href="https://github.com/awslabs/aws-sam-cli" target="_blank" rel="noopener">sam-cli</a></h1><p>aws-cli 只能部署到远端去查看运行效果。而 sam-cli 则可以本地 simulate 一个 lambda 环境，从而实现本地调试。</p>
<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>aws-sam-cli 有个 <code>sam init --runtime java</code>，可以创建一个 sample 项目，但是目前支持的 template 并不包含 jersey 的。所以要创建项目还是通过 mvn archetype。</p>
<p>（所有支持的 runtime 可以查看 <a href="https://github.com/awslabs/aws-sam-cli#project-status" target="_blank" rel="noopener">project status</a>）</p>
<p>但是 <code>sam init</code> 创建了一个 <code>template.yaml</code>，这是之后所有 sam 命令的依据。这个文件会被翻译为 <code>mvn archetype</code> 创建的 <code>sam.yaml</code>。因为其后台是利用 <code>aws cli</code> 运行的。</p>
<p>所以我们需要在 <code>mvn archetype</code> 创建了原型之后，手动依据 <code>sam.yaml</code> 创建 <code>template.yaml</code></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用原型创建项目，配置 aws-cli，同上</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 template.yaml</span></span><br><span class="line">$ cp sam.yaml template.yaml</span><br></pre></td></tr></table></figure>
<h2 id="部署项目"><a href="#部署项目" class="headerlink" title="部署项目"></a>部署项目</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># package</span></span><br><span class="line">$ mvn clean package</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地测试运行</span></span><br><span class="line">$ sam <span class="built_in">local</span> start-api</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传 package 到 s3（需要先创建 s3 bucket）</span></span><br><span class="line">$ aws s3 mb s3://BUCKET_NAME</span><br><span class="line">$ sam package \</span><br><span class="line">    --template-file template.yaml \</span><br><span class="line">    --output-template-file packaged.yaml \</span><br><span class="line">    --s3-bucket YOUR_S3_BUCKET_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署到 aws lambda</span></span><br><span class="line">$ sam deploy \</span><br><span class="line">    --template-file packaged.yaml \</span><br><span class="line">    --stack-name your-stack-name \</span><br><span class="line">    --capabilities CAPABILITY_IAM \</span><br><span class="line">    --parameter-overrides MyParameterSample=MySampleValue</span><br></pre></td></tr></table></figure>
<p>如果在本地运行时，报错 <code>No class found....</code>，一般是因为 docker 的原因，我最后是通过重装 docker 解决的。由于 aws-sam-cli 使用 docker 去起一个 lambda 容器环境，所以可能是由于 credentials 等获取不到的原因？，总之可能会挂掉。可参见 <a href="https://github.com/docker/for-mac/issues/488" target="_blank" rel="noopener">stackoverflow ticket</a>，</p>
<h2 id="查看部署结果-1"><a href="#查看部署结果-1" class="headerlink" title="查看部署结果"></a>查看部署结果</h2><p>这里跟上边是一样的，我们可以只看一部分</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可查看 stack 详情</span></span><br><span class="line">$ aws cloudformation describe-stacks \</span><br><span class="line">	--stack-name your-stack-name \</span><br><span class="line">	--query <span class="string">'Stacks[].Outputs'</span></span><br><span class="line">[</span><br><span class="line">	[</span><br><span class="line">	    &#123;</span><br><span class="line">	        <span class="string">"Description"</span>: <span class="string">"URL for application"</span>,</span><br><span class="line">	        <span class="string">"ExportName"</span>: <span class="string">"AwsLambdaJerseyApi"</span>,  </span><br><span class="line">	        <span class="string">"OutputKey"</span>: <span class="string">"AwsLambdaJerseyApi"</span>,</span><br><span class="line">	        <span class="string">"OutputValue"</span>: <span class="string">"https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping"</span></span><br><span class="line">	    &#125;</span><br><span class="line">	]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据生成的链接访问 api</span></span><br><span class="line">$ curl https://xxxxxxx.execute-api.us-west-2.amazonaws.com/Prod/ping¡</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>aws</tag>
        <tag>aws lambda</tag>
      </tags>
  </entry>
  <entry>
    <title>automatic drive</title>
    <url>/2019/01/15/automatic-drive/</url>
    <content><![CDATA[<p>reference:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/29393415" target="_blank" rel="noopener">coco</a>: one format for data labelling</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5e6217ea2a76689a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="auto-drive system"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-6a7eac9f5586b04a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="labelling system"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5e82ce0148c621a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="training system.png"></p>
]]></content>
      <tags>
        <tag>automatic drive</tag>
      </tags>
  </entry>
  <entry>
    <title>atlas</title>
    <url>/2020/11/13/atlas/</url>
    <content><![CDATA[<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="http://atlas.apache.org/public/images/twiki/architecture.png" alt="img"></p>
<h1 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h1><p><a href="https://atlas.apache.org/2.0.0/InstallationSteps.html" target="_blank" rel="noopener">install steps</a></p>
<p>Access Apache Atlas UI using a browser: <a href="http://localhost:21000/" target="_blank" rel="noopener">http://localhost:21000</a> </p>
<p>You can also access the rest api <code>http://localhost:21000/api/atlas/v2</code></p>
<p>默认的用户名密码是 (admin, admin)</p>
<h1 id="Atlas-Features"><a href="#Atlas-Features" class="headerlink" title="Atlas Features"></a>Atlas Features</h1><h2 id="定义元模型，规范元数据"><a href="#定义元模型，规范元数据" class="headerlink" title="定义元模型，规范元数据"></a>定义元模型，规范元数据</h2><p>atlas 可以维护（增删改查） metadata types，支持</p>
<ul>
<li>创建多种类型的 metadata types<ul>
<li>businessmetadatadef：业务元数据的元模型</li>
<li>classificationdef：标签数据的元模型</li>
<li>entitydef：一般元数据的元模型</li>
<li>enumdef</li>
<li>relationshipdef：关系元数据的元模型</li>
<li>structdef</li>
</ul>
</li>
<li>元模型支持定义属性约束、索引、唯一性等</li>
<li>按 id/typename/query 来检索</li>
</ul>
<blockquote>
<p><a href="http://atlas.apache.org/api/v2/resource_TypesREST.html#resource_TypesREST_createAtlasTypeDefs_POST" target="_blank" rel="noopener">相关 API 定义</a></p>
<p><a href="http://atlas.apache.org/api/v2/json_AtlasTypesDef.html" target="_blank" rel="noopener">typedef request schema object</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># DELETE&#x2F;GET&#x2F;POST&#x2F;PUT</span><br><span class="line">&#x2F;v2&#x2F;types&#x2F;typedef</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="约束"><a href="#约束" class="headerlink" title="约束"></a>约束</h3><ul>
<li>typename 全局唯一</li>
</ul>
<h2 id="可以维护元数据"><a href="#可以维护元数据" class="headerlink" title="可以维护元数据"></a>可以维护元数据</h2><h3 id="import-metadata"><a href="#import-metadata" class="headerlink" title="import metadata"></a>import metadata</h3><p>atlas 提供以下途径将元数据引入系统：</p>
<ol>
<li>REST API：atlas 提供 api 可以 bulk saveOrUpdate 某个 type 的元数据</li>
<li>文件：atlas 可以上传文件，并 saveOrUpdate 文件中所定义的元模型、元数据等</li>
<li>atlas hook：atlas 通过监听 kafka topic <code>ATLAS_HOOK</code> ，来实时引入数据源中的元数据。目前已提供 Apache Hive/Apache HBase/Apache Storm/Apache Sqoop 的 hook<ol>
<li>hive hook<ol>
<li>可以 import hive databases &amp; tables 元数据</li>
<li>可以监听以下类型的 hive 操作，capture 其中的元数据：<ol>
<li>create database</li>
<li>create table/view, create table as select</li>
<li>load, import, export</li>
<li>DMLs (insert)</li>
<li>alter database</li>
<li>alter table (skewed table information, stored as, protection is not supported)</li>
<li>alter view</li>
</ol>
</li>
</ol>
</li>
<li>sqoop<ol>
<li>目前仅支持监听 sqoop 的 hiveimport operation 完成后，capture 其中的元数据。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="业务元数据"><a href="#业务元数据" class="headerlink" title="业务元数据"></a>业务元数据</h2><p>atlas 可以:</p>
<ol>
<li>定义业务元数据元模型规范业务元数据</li>
<li>在技术元数据上，添加业务元数据，来实现关联</li>
<li>支持按业务元数据检索</li>
</ol>
<h2 id="血缘"><a href="#血缘" class="headerlink" title="血缘"></a>血缘</h2><p>atlas 可以查询元数据血缘关系。应该是基于关系图实现。</p>
<p>目前 hive 表可以支撑到 column 级别的血缘分析。</p>
<h2 id="可以维护标签"><a href="#可以维护标签" class="headerlink" title="可以维护标签"></a>可以维护标签</h2><p>atlas 中的 classification 即标签，可以打在元数据、术语等地方。</p>
<p>可以基于 classification 检索元数据。</p>
<p>可以做 classification 的传播：</p>
<ul>
<li>基于继承关系链的传播</li>
<li>基于血缘关系链的传播</li>
</ul>
<h3 id="Classification-vs-label"><a href="#Classification-vs-label" class="headerlink" title="Classification vs label"></a>Classification vs label</h3><p>Atlas 中的 classification 和 label 都是标签的概念，label 是轻量级、谁都可以加的简单标签，classification 则有更多的支持。</p>
<p><a href="https://docs.cloudera.com/runtime/7.2.1/atlas-working-with-classifications/topics/atlas-working-with-classifications.html" target="_blank" rel="noopener">atlas classification vs label</a></p>
<h2 id="可以维护企业术语表"><a href="#可以维护企业术语表" class="headerlink" title="可以维护企业术语表"></a>可以维护企业术语表</h2><p>atlas 可以维护企业的术语，并将术语与元数据关联，支持按术语检索元数据，通过以下三个概念来实现细粒度的术语管理：</p>
<ol>
<li>glossary：术语表，最高级别，所有的术语都必须属于某个术语表</li>
<li>category：术语分类，必须挂在某个 glossary。可以拥有 childCategories</li>
<li>term：术语。必须挂在某个 glossary。可以挂在某个 category</li>
</ol>
<h2 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h2><p>atlas 通过 kafka 实现 hook，引入元数据；也通过 kafka 广播元数据修改，供消费者使用。</p>
<ul>
<li><p>notifications to atlas：<code>ATLAS_HOOK</code>. 目前已提供 Apache Hive/Apache HBase/Apache Storm/Apache Sqoop 的 hook，来监听这些数据源的元数据</p>
</li>
<li><p>notifications from atlas: <code>ATLAS_ENTITIES</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 监听并发布以下事件的通知</span><br><span class="line">ENTITY_CREATE:         sent when an entity instance is created</span><br><span class="line">ENTITY_UPDATE:         sent when an entity instance is updated</span><br><span class="line">ENTITY_DELETE:         sent when an entity instance is deleted</span><br><span class="line">CLASSIFICATION_ADD:    sent when classifications are added to an entity instance</span><br><span class="line">CLASSIFICATION_UPDATE: sent when classifications of an entity instance are updated</span><br><span class="line">CLASSIFICATION_DELETE: sent when classifications are removed from an entity instance</span><br><span class="line"></span><br><span class="line"># notification data</span><br><span class="line">AtlasEntity  entity;</span><br><span class="line">OperationType operationType;</span><br><span class="line">List&lt;AtlasClassification&gt;  classifications</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h2><p>atlas 所有的元数据存储在图数据库 JanusGraph，而索引数据则存储在 index store（solr / elasticsearch）来做全文检索</p>
<p>atlas 支持以下检索方式：</p>
<ol>
<li>唯一定位元数据：通过 id</li>
<li>basic 检索：基于 type、attributes、classifciation、terms 等 query parameter 做全文检索</li>
<li>advance 检索：可以使用 dsl 语言做全文检索</li>
</ol>
<h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><ol>
<li>基础设施高可用。<ol>
<li>atlas 使用 JanusGraph 存储元数据，并将 HBase（默认，可采用其他数据库）作为 backing store。HBase 本身的高可用特性支撑了 metadata store 的高可用</li>
<li>atlas 使用 solr / elasticsearch 存储元数据索引。这些组件同样支持高可用</li>
</ol>
</li>
<li>Web Service 高可用。<ol>
<li>目前 atlas 的 web service 同一时间只能有一个 active instance 响应，以实现元数据维护、缓存等的一致性问题。高可用模式即有多个备用（passive）instances，当 active instance down 后，可以自动切换某个 passive instance，作为新的 active instance。</li>
</ol>
</li>
</ol>
<h2 id="访问控制"><a href="#访问控制" class="headerlink" title="访问控制"></a>访问控制</h2><p>atlas 支持非常细粒度的访问控制：</p>
<ul>
<li><p>元模型：基于某个元模型或某类元模型的访问控制。典型 example：</p>
<blockquote>
<ul>
<li>Admin users can create/update/delete types of all categories</li>
<li>Data stewards can create/update/delete classification types</li>
<li>Healthcare data stewards can create/update/delete types having names start with “hc”</li>
</ul>
</blockquote>
</li>
<li><p>元数据：基于元模型、标签、元数据 id 的元数据访问控制。典型 example：</p>
<blockquote>
<ul>
<li>Admin users can perform all entity operations on entities of all types</li>
<li>Data stewards can perform all entity operations, except delete, on entities of all types</li>
<li>Data quality admins can add/update/remove DATA_QUALITY classification</li>
<li>Users in specific groups can read/update entities with PII classification or its sub-classification</li>
<li>Finance users can read/update entities whose ID start with ‘finance’</li>
</ul>
</blockquote>
</li>
<li><p>admin 操作：可以控制 user/group 来 import/export entities</p>
</li>
</ul>
<h2 id="UI"><a href="#UI" class="headerlink" title="UI"></a>UI</h2><p>有个 ui 可以管理元数据、标签、术语</p>
<h1 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h1><ol>
<li>web service 只有一个 active instance</li>
<li>Typename 全局唯一</li>
<li>ui 挺慢的</li>
</ol>
<h1 id="基于-atlas-我们可以做什么"><a href="#基于-atlas-我们可以做什么" class="headerlink" title="基于 atlas 我们可以做什么"></a>基于 atlas 我们可以做什么</h1><ol>
<li>数据集发现<ol>
<li>数据字典浏览和检索</li>
</ol>
</li>
<li>数据集导入和导出<ol>
<li>导出数据集，供系统之间交互使用</li>
<li>导入数据集（数据标准、指标口径等）</li>
</ol>
</li>
<li>标签管理：<ol>
<li>维护用户画像标签 (增删改查)</li>
<li>基于标签检索数据集</li>
</ol>
</li>
<li>数据标准维护和浏览：（可以通过术语做？）<ol>
<li>维护数据标准（增删改查）</li>
<li>可以为数据标准加标签</li>
</ol>
</li>
<li>指标和口径维护和浏览<ol>
<li>维护指标口径（增删改查）</li>
<li>可以为指标口径加标签</li>
</ol>
</li>
<li>数据集 staticstics<ol>
<li>数据接入和使用情况统计</li>
</ol>
</li>
<li>辅助数据分析师生成分析：<ol>
<li>通过关联技术元数据和业务元数据，当数据分析师按业务语言定义分析后，可以快速查找相关的表、字段等</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>big data</tag>
        <tag>metadata</tag>
      </tags>
  </entry>
  <entry>
    <title>HDP install (offline using ambari)</title>
    <url>/2020/11/23/ambari-install-offline/</url>
    <content><![CDATA[<p><a href="https://wbaseurlww.cnblogs.com/shook/p/12409759.html" target="_blank" rel="noopener">reference</a></p>
<p><a href="https://docs.cloudera.com/HDPDocuments/Ambari-latest/bk_ambari-installation/content/set_up_password-less_ssh.html" target="_blank" rel="noopener">官方安装指导</a></p>
<h1 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h1><p>除非说明，默认以下操作都是在所有节点上执行</p>
<h2 id="修改-host"><a href="#修改-host" class="headerlink" title="修改 host"></a>修改 host</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># vi /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1             localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.105.137 master</span><br><span class="line">192.168.105.191 slave1</span><br><span class="line">192.168.105.13 slave2</span><br></pre></td></tr></table></figure>
<h2 id="修改-network-config"><a href="#修改-network-config" class="headerlink" title="修改 network config"></a>修改 network config</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># vi /etc/sysconfig/network</span></span><br><span class="line"><span class="comment"># Created by anaconda</span></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master</span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># hostnamectl set-hostname master</span></span><br><span class="line">[root@master ~]<span class="comment"># hostname</span></span><br><span class="line">master</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping 各个节点，查看是否可连通</span></span><br><span class="line">[root@master ~]<span class="comment"># ping slave1</span></span><br><span class="line">PING slave1 (192.168.105.191) 56(84) bytes of data.</span><br></pre></td></tr></table></figure>
<h2 id="同步时间-ntp"><a href="#同步时间-ntp" class="headerlink" title="同步时间 ntp"></a>同步时间 ntp</h2><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><h2 id="关闭-Selinux-和-THP"><a href="#关闭-Selinux-和-THP" class="headerlink" title="关闭 Selinux 和 THP"></a>关闭 Selinux 和 THP</h2><h2 id="修改文件打开最大限制"><a href="#修改文件打开最大限制" class="headerlink" title="修改文件打开最大限制"></a><del>修改文件打开最大限制</del></h2><h2 id="SSH-无密码登录（主节点）"><a href="#SSH-无密码登录（主节点）" class="headerlink" title="SSH 无密码登录（主节点）"></a>SSH 无密码登录（主节点）</h2><h2 id="Reboot"><a href="#Reboot" class="headerlink" title="Reboot"></a>Reboot</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ shutdown -r now</span><br></pre></td></tr></table></figure>
<h1 id="制作本地源（离线安装）"><a href="#制作本地源（离线安装）" class="headerlink" title="制作本地源（离线安装）"></a>制作本地源（离线安装）</h1><h2 id="文件目录访问（http-服务方式）"><a href="#文件目录访问（http-服务方式）" class="headerlink" title="文件目录访问（http 服务方式）"></a>文件目录访问（http 服务方式）</h2><h2 id="制作本地源（主节点）"><a href="#制作本地源（主节点）" class="headerlink" title="制作本地源（主节点）"></a>制作本地源（主节点）</h2><h3 id="安装本地源制作相关工具"><a href="#安装本地源制作相关工具" class="headerlink" title="安装本地源制作相关工具"></a>安装本地源制作相关工具</h3><h3 id="修改文件里面的源地址"><a href="#修改文件里面的源地址" class="headerlink" title="修改文件里面的源地址"></a>修改文件里面的源地址</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[root@master ambari]<span class="comment"># vi ambari/centos7/2.7.4.0-118/ambari.repo</span></span><br><span class="line"><span class="comment">#VERSION_NUMBER=2.7.4.0-118</span></span><br><span class="line">[ambari-2.7.4.0]</span><br><span class="line"><span class="comment">#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json</span></span><br><span class="line">name=ambari Version - ambari-2.7.4.0</span><br><span class="line">baseurl=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">[root@master ambari]<span class="comment"># cp ambari/centos7/2.7.4.0-118/ambari.repo /etc/yum.repos.d/</span></span><br><span class="line">[root@master ambari]<span class="comment"># vi HDP/centos7/3.1.4.0-315/hdp.repo</span></span><br><span class="line"><span class="comment">#VERSION_NUMBER=3.1.4.0-315</span></span><br><span class="line">[HDP-3.1.4.0]</span><br><span class="line">name=HDP Version - HDP-3.1.4.0</span><br><span class="line">baseurl=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[HDP-UTILS-1.1.0.22]</span><br><span class="line">name=HDP-UTILS Version - HDP-UTILS-1.1.0.22</span><br><span class="line">baseurl=http://192.168.105.137/ambari/HDP-UTILS/centos7/1.1.0.22</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/HDP-UTILS/centowwws7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">[root@master ambari]<span class="comment"># cp HDP/centos7/3.1.4.0-315/hdp.repo /etc/yum.repos.d/</span></span><br></pre></td></tr></table></figure>
<h3 id="将创建好的源文件（-repo）拷贝到子节点"><a href="#将创建好的源文件（-repo）拷贝到子节点" class="headerlink" title="将创建好的源文件（.repo）拷贝到子节点"></a>将创建好的源文件（.repo）拷贝到子节点</h3><h1 id="安装-ambari-server"><a href="#安装-ambari-server" class="headerlink" title="安装 ambari-server"></a>安装 ambari-server</h1><h2 id="安装-ambari-server-1"><a href="#安装-ambari-server-1" class="headerlink" title="安装 ambari-server"></a>安装 ambari-server</h2><p>先安装，然后开始配置</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ yum -y install ambari-server</span><br></pre></td></tr></table></figure>
<h2 id="设置并启动-ambari-server（主节点）"><a href="#设置并启动-ambari-server（主节点）" class="headerlink" title="设置并启动 ambari-server（主节点）"></a>设置并启动 ambari-server（主节点）</h2><p><a href="https://www.baeldung.com/find-java-home" target="_blank" rel="noopener">how to find JAVA_HOME</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ java -XshowSettings:properties -version 2&gt;&amp;1 &gt; /dev/null | grep <span class="string">'java.home'</span></span><br></pre></td></tr></table></figure>
<h3 id="使用默认-postgresql"><a href="#使用默认-postgresql" class="headerlink" title="使用默认 postgresql"></a>使用默认 postgresql</h3><p>Setup server 时，有几个交互式配置：</p>
<ol>
<li><p>是否自定义用户账户：</p>
<ol>
<li>选 n。即默认设置了 Ambari GUI 的登录用户为 admin/admin。并且指定 Ambari Server 的运行用户为 root。</li>
</ol>
<blockquote>
<p>If you want to create a different user to run the Ambari Server, or to assign a previously created user, select <strong><code>y</code></strong> at the <code>Customize user account for ambari-server daemon</code> prompt, then provide a user name.</p>
</blockquote>
</li>
<li><p>JDK：</p>
<ol>
<li>选 2. 因为默认会安装并使用 oracle jdk，但是（1）不能联网下载（2）好像 oracle jdk 不会下载依赖包。所以自己安装好，在这指定 path 即可</li>
</ol>
</li>
<li><p>数据库：</p>
<ol>
<li>按默认配置创建 postgres 数据库</li>
</ol>
</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[root@master yum.repos.d]<span class="comment"># ambari-server setup</span></span><br><span class="line">Using python  /usr/bin/python</span><br><span class="line">Setup ambari-server</span><br><span class="line">Checking SELinux...</span><br><span class="line">SELinux status is <span class="string">'disabled'</span></span><br><span class="line">Customize user account <span class="keyword">for</span> ambari-server daemon [y/n] (n)? n</span><br><span class="line">Adjusting ambari-server permissions and ownership...</span><br><span class="line">Checking firewall status...</span><br><span class="line">Checking JDK...</span><br><span class="line">[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8</span><br><span class="line">[2] Custom JDK</span><br><span class="line">Enter choice (1): 2</span><br><span class="line">WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.</span><br><span class="line">WARNING: JCE Policy files are required <span class="keyword">for</span> configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.</span><br><span class="line">Path to JAVA_HOME: /usr/java/jdk1.8.0_202</span><br><span class="line">Validating JDK on Ambari Server...done.</span><br><span class="line">Completing setup...</span><br><span class="line">Configuring database...</span><br><span class="line">Enter advanced database configuration [y/n] (n)? n</span><br><span class="line">Configuring database...</span><br><span class="line">Default properties detected. Using built-in database.</span><br><span class="line">Configuring ambari database...</span><br><span class="line">Checking PostgreSQL...</span><br><span class="line">Running initdb: This may take up to a minute.</span><br><span class="line">Initializing database ... OK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">About to start PostgreSQL</span><br><span class="line">Configuring <span class="built_in">local</span> database...</span><br><span class="line">Configuring PostgreSQL...</span><br><span class="line">Restarting PostgreSQL</span><br><span class="line">Creating schema and user...</span><br><span class="line"><span class="keyword">done</span>.</span><br><span class="line">Creating tables...</span><br><span class="line"><span class="keyword">done</span>.</span><br><span class="line">Extracting system views...</span><br><span class="line">ambari-admin-2.7.4.0-118.jar</span><br><span class="line">...........</span><br><span class="line">Adjusting ambari-server permissions and ownership...</span><br><span class="line">Ambari Server <span class="string">'setup'</span> completed successfully.</span><br></pre></td></tr></table></figure>
<h3 id="使用-mysql"><a href="#使用-mysql" class="headerlink" title="使用 mysql"></a>使用 mysql</h3><p><a href="https://programmer.group/linux-centos-7-mysql-5.7-offline-installation.html" target="_blank" rel="noopener">离线安装 mysql</a></p>
<h4 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h4><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建用户组</span></span><br><span class="line">$ groupadd hdp</span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">$ mkdir /home/mysql</span><br><span class="line">$ useradd -g hdp hive -d /home/mysql/hive</span><br><span class="line">$ passwd hive</span><br><span class="line">yourPassword</span><br><span class="line"><span class="comment"># 创建临时目录</span></span><br><span class="line">$ mkdir /home/mysql/hive/3306/data</span><br><span class="line">$ mkdir /home/mysql/hive/3306/<span class="built_in">log</span></span><br><span class="line">$ mkdir /home/mysql/hive/3306/tmp</span><br><span class="line">$ chown -R hive:hdp /home/mysql/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">$ mv mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz /usr/<span class="built_in">local</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">$ tar -xzvf mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz</span><br><span class="line"><span class="comment"># Establish soft links for future upgrades</span></span><br><span class="line">$ ln -s mysql-5.7.27-linux-glibc2.12-x86_64 mysql</span><br><span class="line"><span class="comment"># Modify users and user groups of all files under mysql folder</span></span><br><span class="line">$ chown -R mysql:mysql mysql/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc</span><br><span class="line">$ vi my.cnf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 mysql</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/mysql/bin</span><br><span class="line">$ ./mysqld --initialize --user=hive</span><br></pre></td></tr></table></figure>
<h4 id="安装-driver-并配置与-ambari-server-的-jdbc-连接"><a href="#安装-driver-并配置与-ambari-server-的-jdbc-连接" class="headerlink" title="安装 driver, 并配置与 ambari-server 的 jdbc 连接"></a>安装 driver, 并配置与 ambari-server 的 jdbc 连接</h4><p><a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">mysql-connector-driver 下载</a> （选 RedHat 8 那个操作系统）</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解压</span></span><br><span class="line">$ rpm -ivh mysql80-community-release-el7-3.noarch.rpm</span><br><span class="line">$ cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar</span><br><span class="line"><span class="comment"># 配置 driver path</span></span><br><span class="line">$ vi /etc/ambari-server/conf/ambari.properties</span><br><span class="line">添加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
<h4 id="配置-mysql"><a href="#配置-mysql" class="headerlink" title="配置 mysql"></a>配置 mysql</h4><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置开机启动，并启动 mysql</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/mysql</span><br><span class="line"><span class="comment"># Copy the startup script to the resource directory and modify mysql.server. It's better to modify mysqld as well. These two files are best synchronized.</span></span><br><span class="line">$ cp ./support-files/mysql.server /etc/rc.d/init.d/mysqld</span><br><span class="line"><span class="comment"># Increase the execution privileges of mysqld service control scripts</span></span><br><span class="line">$ chmod +x /etc/rc.d/init.d/mysqld</span><br><span class="line"><span class="comment"># Add mysqld service to system service</span></span><br><span class="line">$ chkconfig --add mysqld</span><br><span class="line"><span class="comment"># Check whether the mysqld service is in effect</span></span><br><span class="line">$ chkconfig --list mysqld </span><br><span class="line"><span class="comment"># mysql start</span></span><br><span class="line">$ service mysqld start</span><br><span class="line"><span class="comment"># View mysql status</span></span><br><span class="line">$ service mysqld status</span><br><span class="line"><span class="comment"># Check mysql related processes</span></span><br><span class="line">$ ps aux|grep mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">$ vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/mysql/bin</span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新 root 密码</span></span><br><span class="line">$ mysql -uroot -p</span><br><span class="line">$ mysql&gt; <span class="built_in">set</span> password <span class="keyword">for</span> root@localhost=password(<span class="string">"root"</span>);</span><br><span class="line"><span class="comment"># 配置 remote access to the mysql</span></span><br><span class="line">$ mysql&gt; use mysql</span><br><span class="line">$ mysql&gt; update user <span class="built_in">set</span> host=<span class="string">'%'</span> <span class="built_in">where</span> user=<span class="string">'root'</span>;</span><br><span class="line">$ mysql&gt; select host,user from user;</span><br><span class="line">$ mysql&gt; grant all privileges on *.* to <span class="string">'root'</span>@<span class="string">'%'</span> identified by <span class="string">'yourPassword'</span>;</span><br><span class="line">$ mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h4 id="创建-database"><a href="#创建-database" class="headerlink" title="创建 database"></a>创建 database</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE ambari;  </span><br><span class="line">use ambari;  </span><br><span class="line">CREATE USER &#39;ambari&#39;@&#39;%&#39; IDENTIFIED BY &#39;ambari&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;%&#39;;  </span><br><span class="line">CREATE USER &#39;ambari&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ambar&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;localhost&#39;;  </span><br><span class="line">CREATE USER &#39;ambari&#39;@&#39;master&#39; IDENTIFIED BY &#39;ambari&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;ambari&#39;@&#39;master&#39;;  </span><br><span class="line">FLUSH PRIVILEGES;  </span><br><span class="line">source &#x2F;var&#x2F;lib&#x2F;ambari-server&#x2F;resources&#x2F;Ambari-DDL-MySQL-CREATE.sql  </span><br><span class="line"></span><br><span class="line">CREATE DATABASE hive;  </span><br><span class="line">use hive;  </span><br><span class="line">CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;%&#39;;  </span><br><span class="line">CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;localhost&#39;;  </span><br><span class="line">CREATE USER &#39;hive&#39;@&#39;master&#39; IDENTIFIED BY &#39;hive&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;master&#39;;  </span><br><span class="line">FLUSH PRIVILEGES;  </span><br><span class="line">CREATE DATABASE oozie;  </span><br><span class="line">use oozie;  </span><br><span class="line">CREATE USER &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;%&#39;;  </span><br><span class="line">CREATE USER &#39;oozie&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;oozie&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;localhost&#39;;  </span><br><span class="line">CREATE USER &#39;oozie&#39;@&#39;master&#39; IDENTIFIED BY &#39;oozie&#39;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;oozie&#39;@&#39;master&#39;;  </span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>
<h4 id="Mysql-conf"><a href="#Mysql-conf" class="headerlink" title="Mysql conf"></a>Mysql conf</h4><p>上边 cnf 的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]                                        # Client settings, the default connection parameters for the client</span><br><span class="line">port = 3306                                    # Default connection port</span><br><span class="line">socket = /home/mysql/hive/3306/tmp/mysql.sock                        # For socket sockets for local connections, the mysqld daemon generates this file</span><br><span class="line"></span><br><span class="line">[mysqld]                                        # Server Basic Settings</span><br><span class="line"><span class="meta">#</span><span class="bash"> Foundation setup</span></span><br><span class="line">user = hive</span><br><span class="line">bind-address = 0.0.0.0                         # Allow any ip host to access this database</span><br><span class="line">server-id = 1                                  # The unique number of Mysql service Each MySQL service Id needs to be unique</span><br><span class="line">port = 3306                                    # MySQL listening port</span><br><span class="line">basedir = /usr/local/mysql                      # MySQL installation root directory</span><br><span class="line">datadir = /home/mysql/hive/3306/data                      # MySQL Data File Location</span><br><span class="line">tmpdir  = /home/mysql/hive/3306/tmp                                  # Temporary directories, such as load data infile, will be used</span><br><span class="line">socket = /home/mysql/hive/3306/tmp/mysql.sock        # Specify a socket file for local communication between MySQL client program and server</span><br><span class="line">pid-file = /home/mysql/hive/3306/log/mysql.pid      # The directory where the pid file is located</span><br><span class="line">skip_name_resolve = 1                          # Only use IP address to check the client's login, not the host name.</span><br><span class="line">character-set-server = utf8mb4                  # Database default character set, mainstream character set support some special emoticons (special emoticons occupy 4 bytes)</span><br><span class="line">transaction_isolation = READ-COMMITTED          # Transaction isolation level, which is repeatable by default. MySQL is repeatable by default.</span><br><span class="line">collation-server = utf8mb4_general_ci          # The character set of database corresponds to some sort rules, etc. Be careful to correspond to character-set-server.</span><br><span class="line">init_connect='SET NAMES utf8mb4'                # Set up the character set when client connects mysql to prevent scrambling</span><br><span class="line">lower_case_table_names = 1                      # Is it case sensitive to sql statements, 1 means insensitive</span><br><span class="line">max_connections = 400                          # maximum connection</span><br><span class="line">max_connect_errors = 1000                      # Maximum number of false connections</span><br><span class="line">explicit_defaults_for_timestamp = true          # TIMESTAMP allows NULL values if no declaration NOT NULL is displayed</span><br><span class="line">max_allowed_packet = 128M                      # The size of the SQL packet sent, if there is a BLOB object suggested to be modified to 1G</span><br><span class="line">interactive_timeout = 1800                      # MySQL connection will be forcibly closed after it has been idle for more than a certain period of time (in seconds)</span><br><span class="line">wait_timeout = 1800                            # The default value of MySQL wait_timeout is 8 hours. The interactive_timeout parameter needs to be configured concurrently to take effect.</span><br><span class="line">tmp_table_size = 16M                            # The maximum value of interior memory temporary table is set to 128M; for example, group by, order by with large amount of data may be used as temporary table; if this value is exceeded, it will be written to disk, and the IO pressure of the system will increase.</span><br><span class="line">max_heap_table_size = 128M                      # Defines the size of memory tables that users can create</span><br><span class="line">query_cache_size = 0                            # Disable mysql's cached query result set function; later test to determine whether to turn on or not based on business conditions; in most cases, close the following two items</span><br><span class="line">query_cache_type = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Memory settings allocated by user processes, and each session will allocate memory size <span class="keyword">for</span> parameter settings</span></span><br><span class="line">read_buffer_size = 2M                          # MySQL read buffer size. Requests for sequential table scans allocate a read buffer for which MySQL allocates a memory buffer.</span><br><span class="line">read_rnd_buffer_size = 8M                      # Random Read Buffer Size of MySQL</span><br><span class="line">sort_buffer_size = 8M                          # Buffer size used for MySQL execution sort</span><br><span class="line">binlog_cache_size = 1M                          # A transaction produces a log that is recorded in Cache when it is not committed, and persists the log to disk when it needs to be committed. Default binlog_cache_size 32K</span><br><span class="line"></span><br><span class="line">back_log = 130                                  # How many requests can be stored on the stack in a short time before MySQL temporarily stops responding to new requests; the official recommendation is back_log = 50 + (max_connections/5), with a cap of 900</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">log</span> setting</span></span><br><span class="line">log_error = /home/mysql/hive/3306/log/error.log                          # Database Error Log File</span><br><span class="line">slow_query_log = 1                              # Slow Query sql Log Settings</span><br><span class="line">long_query_time = 1                            # Slow query time; Slow query over 1 second</span><br><span class="line">slow_query_log_file = /home/mysql/hive/3306/log/slow.log                  # Slow Query Log Files</span><br><span class="line">log_queries_not_using_indexes = 1              # Check sql that is not used in the index</span><br><span class="line">log_throttle_queries_not_using_indexes = 5      # Represents the number of SQL statements per minute that are allowed to be logged to a slow log and are not indexed. The default value is 0, indicating that there is no limit.</span><br><span class="line">min_examined_row_limit = 100                    # The number of rows retrieved must reach this value before they can be recorded as slow queries. SQL that returns fewer than the rows specified by this parameter is not recorded in the slow query log.</span><br><span class="line">expire_logs_days = 5                            # MySQL binlog log log file saved expiration time, automatically deleted after expiration</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Master-slave replication settings</span></span><br><span class="line">log-bin = mysql-bin                            # Open mysql binlog function</span><br><span class="line">binlog_format = ROW                            # The way a binlog records content, recording each row being manipulated</span><br><span class="line">binlog_row_image = minimal                      # For binlog_format = ROW mode, reduce the content of the log and record only the affected columns</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Innodb settings</span></span><br><span class="line">innodb_open_files = 500                        # Restrict the data of tables Innodb can open. If there are too many tables in the library, add this. This value defaults to 300</span><br><span class="line">innodb_buffer_pool_size = 64M                  # InnoDB uses a buffer pool to store indexes and raw data, usually 60% to 70% of physical storage; the larger the settings here, the less disk I/O you need to access the data in the table.</span><br><span class="line">innodb_log_buffer_size = 2M                    # This parameter determines the size of memory used to write log files in M. Buffers are larger to improve performance, but unexpected failures can result in data loss. MySQL developers recommend settings between 1 and 8M</span><br><span class="line">innodb_flush_method = O_DIRECT                  # O_DIRECT reduces the conflict between the cache of the operating system level VFS and the buffer cache of Innodb itself.</span><br><span class="line">innodb_write_io_threads = 4                    # CPU multi-core processing capability settings are adjusted according to read-write ratio</span><br><span class="line">innodb_read_io_threads = 4</span><br><span class="line">innodb_lock_wait_timeout = 120                  # InnoDB transactions can wait for a locked timeout second before being rolled back. InnoDB automatically detects transaction deadlocks and rolls back transactions in its own lock table. InnoDB notices the lock settings with the LOCK TABLES statement. The default value is 50 seconds.</span><br><span class="line">innodb_log_file_size = 32M                      # This parameter determines the size of the data log file. Larger settings can improve performance, but also increase the time required to recover the failed database.</span><br></pre></td></tr></table></figure>
<h2 id="停止-ambari-server"><a href="#停止-ambari-server" class="headerlink" title="停止 ambari-server"></a>停止 ambari-server</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[root@master ~] ambari-server stop    <span class="comment">#停止命令</span></span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server reset   #重置命令</span></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server setup   #重新设置 </span></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server start   #启动命令</span></span><br></pre></td></tr></table></figure>
<h1 id="配置集群"><a href="#配置集群" class="headerlink" title="配置集群"></a>配置集群</h1><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>集群名字：ftms_hdp_qat</p>
<p>选择版本：</p>
<ul>
<li>hdp 3.1</li>
<li>redhat7</li>
</ul>
<h2 id="配置服务"><a href="#配置服务" class="headerlink" title="配置服务"></a>配置服务</h2><p>选择服务</p>
<p>选择 master/slave for 各服务</p>
<p><img src="/images/ambari-20201125154733621.png" alt="image-20201125154733621"></p>
<h3 id="配置-hive-ozzie-ranger-database"><a href="#配置-hive-ozzie-ranger-database" class="headerlink" title="配置 hive/ozzie/ranger database"></a>配置 hive/ozzie/ranger database</h3><h3 id="使用-postgresql"><a href="#使用-postgresql" class="headerlink" title="使用 postgresql"></a>使用 postgresql</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 先执行下边这句话，再继续配置</span></span><br><span class="line">$ ambari-server setup --jdbc-db=postgres --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
<h3 id="使用-mysql-1"><a href="#使用-mysql-1" class="headerlink" title="使用 mysql"></a>使用 mysql</h3><p>使用 mysql（生产环境推荐使用），且 mysql 在其他地方也在用，而 postgresql 和 mysql 的语法是有区别的。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先执行下边这句话，再继续配置</span></span><br><span class="line">$ ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
<h3 id="配置-directories"><a href="#配置-directories" class="headerlink" title="配置 directories"></a>配置 directories</h3><p>采用默认的</p>
<h3 id="configurations"><a href="#configurations" class="headerlink" title="configurations"></a>configurations</h3><p>采用默认的</p>
<h1 id="异常调试"><a href="#异常调试" class="headerlink" title="异常调试"></a>异常调试</h1><h2 id="查看-ambari-的配置"><a href="#查看-ambari-的配置" class="headerlink" title="查看 ambari 的配置"></a>查看 ambari 的配置</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 ambari-server 的配置</span></span><br><span class="line">$ vi /etc/ambari-server/conf/ambari.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 hdp 各服务的配置</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/hdp/3.1.4.0-315/hbase/conf</span><br><span class="line"><span class="comment"># 运行服务</span></span><br><span class="line">$ /usr/hdp/3.1.4.0-315/hbase/bin/hbase shell</span><br></pre></td></tr></table></figure>
<h2 id="查看错误日志"><a href="#查看错误日志" class="headerlink" title="查看错误日志"></a>查看错误日志</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 ambari-server 启动的错误日志</span></span><br><span class="line">$ tail /var/<span class="built_in">log</span>/ambari-server/ambari-server.log -n 10 -f | less</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 hdp 各服务的日志</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/hdp/3.1.4.0-315/hbase/logs/hbase/hbase-hbase-regionserver-slave2.log</span><br><span class="line"><span class="comment"># 或者在 var 下看也一样，不知道是放了软 link 还是什么，日志好像是一样的</span></span><br><span class="line">$ <span class="built_in">cd</span> /var/<span class="built_in">log</span>/hbase/hbase-hbase-regionserver-slave2.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ranger service 的配置</span></span><br><span class="line">$ /etc/ranger/</span><br></pre></td></tr></table></figure>
<h2 id="重新安装"><a href="#重新安装" class="headerlink" title="重新安装"></a>重新安装</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ambari-server stop</span><br><span class="line">$ ambari-server reset</span><br><span class="line">$ ambari-server setup</span><br></pre></td></tr></table></figure>
<h2 id="安装找不到包"><a href="#安装找不到包" class="headerlink" title="安装找不到包"></a>安装找不到包</h2><h3 id="找不到-hdp-repo"><a href="#找不到-hdp-repo" class="headerlink" title="找不到 hdp.repo"></a>找不到 hdp.repo</h3><p>在实际安装时，ambari 会生成一个新的 ambari-hdp-1.repo，其中也定义了 hdp 的 baseurl 之类，这里对 hdp 定义的 name 可能是 <code>[HDP-3.1-repo-1]</code> , 而前边准备本地库时，定义的 hdp 的 name 是 <code>[HDP-3.1.4.0]</code>，这两个名字必须保持一致，否则 ambari 就找不到包（这是 ambari 的一个 bug）。</p>
<blockquote>
<p>解决方案：</p>
<p>将 <code>hdp.repo</code> 中的 [HDP-3.1.4.0] 改为 [HDP-3.1-repo-1]，并重新 scp 到各个节点</p>
</blockquote>
<h3 id="postfix找不到libmysqlclient-so-18"><a href="#postfix找不到libmysqlclient-so-18" class="headerlink" title="postfix找不到libmysqlclient.so.18"></a>postfix找不到libmysqlclient.so.18</h3><blockquote>
<p>还有一种简单的方法（没试过）：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">&gt;$ yum reinstall mysql-libs -y</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先删除现在安装的 postfix</span></span><br><span class="line">$ systemctl <span class="built_in">disable</span> postfix</span><br><span class="line">$ systemctl stop postfix</span><br><span class="line">$ yum remove postfix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给 libmysqlclient.so.18 加上软链</span></span><br><span class="line">$ find / -name <span class="string">'*libmysqlclient.so.18*'</span></span><br><span class="line">/usr/lib64/mysql/libmysqlclient.so.18</span><br><span class="line">$ ln -s /usr/lib64/mysql/libmysqlclient.so.18 /usr/lib/libmysqlclient.so.18</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新安装 postfix 并启动</span></span><br><span class="line">$ yum install postfix</span><br><span class="line">$ systemctl <span class="built_in">enable</span> postfix</span><br><span class="line">$ systemctl start postfix</span><br><span class="line">$ systemctl status postfix.service</span><br></pre></td></tr></table></figure>
<h3 id="找不到-libtirpc-devel"><a href="#找不到-libtirpc-devel" class="headerlink" title="找不到 libtirpc-devel"></a>找不到 libtirpc-devel</h3><p>如果出错，可能各个节点都需要做这件事</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不能联网</span></span><br><span class="line"><span class="comment"># 先下载 https://centos.pkgs.org/7/centos-x86_64/libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm.html 包</span></span><br><span class="line">$ yum-config-manager --<span class="built_in">enable</span> rhui-REGION-rhel-server-optional</span><br><span class="line">$ yum install libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果可以联网。这个可以从 CentOs-Base.repo 里下，但不能联网就没办法了</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc/yum.repos.d</span><br><span class="line">$ cp backup/CentOs-Base.repo .</span><br></pre></td></tr></table></figure>
<h2 id="ranger-admin-start-fail"><a href="#ranger-admin-start-fail" class="headerlink" title="ranger-admin start fail"></a>ranger-admin start fail</h2><p>start ranger-admin fail.</p>
<blockquote>
<p>error detail:<br>This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you <em>might</em> want to use the less safe log_bin_trust_function_creators variable)</p>
</blockquote>
<p>Solution (<a href="https://stackoverflow.com/questions/26015160/deterministic-no-sql-or-reads-sql-data-in-its-declaration-and-binary-logging-i" target="_blank" rel="noopener">stackflow</a>):</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Execute the following in the MySQL console:</span></span><br><span class="line">SET GLOBAL log_bin_trust_function_creators = 1;</span><br></pre></td></tr></table></figure>
<h2 id="atlas-server-启动失败"><a href="#atlas-server-启动失败" class="headerlink" title="atlas server 启动失败"></a>atlas server 启动失败</h2><h3 id="Ranger-authorization-失败"><a href="#Ranger-authorization-失败" class="headerlink" title="Ranger authorization 失败"></a>Ranger authorization 失败</h3><blockquote>
<p>执行 <code>cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n</code> 命令时，失败报 404</p>
<p>Error detail:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.coprocessor.CoprocessorException: HTTP 404 Error: HTTP 404</span><br><span class="line">	at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1261)</span><br><span class="line">	at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1072)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos<span class="variable">$AccessControlService</span><span class="variable">$1</span>.grant(AccessControlProtos.java:10023)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos<span class="variable">$AccessControlService</span>.callMethod(AccessControlProtos.java:10187)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:8135)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:2426)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:2408)</span><br><span class="line">	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos<span class="variable">$ClientService</span><span class="variable">$2</span>.callBlockingMethod(ClientProtos.java:42010)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:131)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor<span class="variable">$Handler</span>.run(RpcExecutor.java:324)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor<span class="variable">$Handler</span>.run(RpcExecutor.java:304)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
<p>这个原因是 atlas 通过 ranger 访问 hbase 的时候没有权限。可能是由于安装先后顺序的原因，也有 atlas + ranger 本身需要手动配置的原因，导致 ranger 中没有配置 atlas 对 hbase、kafka 的访问权限。因此需要做做几件事：</p>
<ol>
<li><p>在 ambari 的 ranger config 中，启动 hbase ranger plugin，并重启相关服务</p>
</li>
<li><p>在 ambari 的 ranger config 中，启动 kafka ranger  plugin，并重启相关服务 ===&gt; 暂时没做</p>
</li>
<li><p>在 ranger 添加 hbase 的 service（参照 <a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/authorization-ranger/content/resource_service_configure_an_hbase_service.html" target="_blank" rel="noopener">Configure a Resource-based Service: HBase</a>)</p>
<ol>
<li>这里 service 的名称必须和 <code>/usr/hdp/3.1.4.0-315/ranger-hbase-plugin/install.properties</code> 里配置 <code>REPOSITORY_NAME</code> 保持一致（但是似乎并不需要？）</li>
<li>username，password 说是 The end system username that can be used for connection.（目前来看，我现在是随便写的 admin 的账号）</li>
<li>Zookeeper 和 hbase.authentication 的配置是和  <code>/usr/hdp/3.1.4.0-315/hbase/conf/hbase-site.xml</code> 中的配置保持一致</li>
</ol>
<p><img src="/images/ambari-20201130112641591.png" alt="image-20201130112641591"></p>
<p><img src="/images/ambari-20201130112704671.png" alt="image-20201130112704671"></p>
</li>
<li><p>添加 atlas 对 hbase tables、kafka topic 的访问 policies（可参看：<a href="https://github.com/emaxwell-hw/Atlas-Ranger-Tag-Security/blob/master/README.md" target="_blank" rel="noopener">tag-based security with atlas + ranger</a>）===》 暂时没做 kafka</p>
<ol>
<li><strong>创建 hbase 的 policies 时，必须给 all - table, column-family, column 加上 <code>hbase</code> 这个 user</strong>，否则可能会遇到 403。这个原因是启动 metadata server 时，会执行 <code>cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n</code> 这么一条命令，执行时，会切换到 <code>hbase</code> 这个用户，如果这里不加权限，这条命令就会执行失败</li>
</ol>
<p><img src="/images/ambari-20201130112445208.png" alt="image-20201130112445208"></p>
</li>
</ol>
<p>ranger 的访问链接: <a href="http://ranger-server-host:6080/index.html" target="_blank" rel="noopener">http://ranger-server-host:6080/index.html</a> (可以从 ambari ranger config 中看到)</p>
<h3 id="生成-jar-包失败（报-no-such-file-or-directory"><a href="#生成-jar-包失败（报-no-such-file-or-directory" class="headerlink" title="生成 jar 包失败（报 no such file or directory)"></a>生成 jar 包失败（报 no such file or directory)</h3><blockquote>
<p>在执行 <code>source /usr/hdp/current/atlas-server/conf/atlas-env.sh ; /usr/hdp/current/atlas-server/bin/atlas_start.py</code> 时报错，</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">  File <span class="string">"/usr/hdp/current/atlas-server/bin/atlas_start.py"</span>, line 163, <span class="keyword">in</span> </span><br><span class="line">    returncode = main()</span><br><span class="line">  File <span class="string">"/usr/hdp/current/atlas-server/bin/atlas_start.py"</span>, line 73, <span class="keyword">in</span> main</span><br><span class="line">    mc.expandWebApp(atlas_home)</span><br><span class="line">  File <span class="string">"/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py"</span>, line 160, <span class="keyword">in</span> expandWebApp</span><br><span class="line">    jar(atlasWarPath)</span><br><span class="line">  File <span class="string">"/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py"</span>, line 213, <span class="keyword">in</span> jar</span><br><span class="line">    process = runProcess(commandline)</span><br><span class="line">  File <span class="string">"/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py"</span>, line 249, <span class="keyword">in</span> runProcess</span><br><span class="line">    p = subprocess.Popen(commandline, stdout=stdoutFile, stderr=stderrFile, shell=shell)</span><br><span class="line">  File <span class="string">"/usr/lib64/python2.7/subprocess.py"</span>, line 711, <span class="keyword">in</span> __init__</span><br><span class="line">    errread, errwrite)</span><br><span class="line">  File <span class="string">"/usr/lib64/python2.7/subprocess.py"</span>, line 1327, <span class="keyword">in</span> _execute_child</span><br><span class="line">    raise child_exception</span><br><span class="line">OSError: [Errno 2] No such file or directory</span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过在 <code>atlas_config.py</code> 中添加 log，发现最后是在执行 <code>/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/jar -xf /usr/hdp/3.1.4.0-315/atlas/server/webapp/atlas.war</code> 时报错，找不到的是 jar 命令. 原因是 jar 是 jdk 中的命令，而使用的默认 openjdk 其实只安装了 jre.</p>
<blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看所有的 openjdk 列表</span></span><br><span class="line">$ yum list | grep jdk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 jdk</span></span><br><span class="line">$ yum install java-1.8.0-openjdk.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># copy jar 到指定目录</span></span><br><span class="line">$ cp /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/bin/jar /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Host-Disk-Usage-alert"><a href="#Host-Disk-Usage-alert" class="headerlink" title="Host Disk Usage alert"></a>Host Disk Usage alert</h2><p>安装过程下载了挺多乱七八糟的东西，导致硬盘报警了</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看目前各文件系统的硬盘使用情况，如果设置的 80% 报警，则只要有某个文件系统的使用哪个超了，就会报警</span></span><br><span class="line">$ df -h</span><br><span class="line">文件系统                       容量  已用  可用 已用% 挂载点</span><br><span class="line">devtmpfs                        32G     0   32G    0% /dev</span><br><span class="line">tmpfs                           32G     0   32G    0% /dev/shm</span><br><span class="line">tmpfs                           32G  824M   31G    3% /run</span><br><span class="line">tmpfs                           32G     0   32G    0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/centos-root         50G   31G   20G   61% /</span><br><span class="line">/dev/sda1                     1014M  154M  861M   16% /boot</span><br><span class="line">/dev/mapper/vg_data2-lv_data2  200G   55M  200G    1% /data02</span><br><span class="line">/dev/mapper/centos-home         47G  444M   47G    1% /home</span><br><span class="line">/dev/mapper/vg_data1-lv_data1  200G  4.9G  196G    3% /data01</span><br><span class="line">tmpfs                          6.3G     0  6.3G    0% /run/user/0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看现在系统中的文件占用情况，找大文件去清</span></span><br><span class="line">$ du -hs /*</span><br><span class="line">$ du -hs /root/*</span><br><span class="line">$ du -hs /var/<span class="built_in">log</span>/*</span><br><span class="line">$ du -h /var/* -d 1 | sort -n -r</span><br></pre></td></tr></table></figure>
<h1 id="验证各服务可用"><a href="#验证各服务可用" class="headerlink" title="验证各服务可用"></a>验证各服务可用</h1><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="创建文件夹（可选？？？）"><a href="#创建文件夹（可选？？？）" class="headerlink" title="创建文件夹（可选？？？）"></a>创建文件夹（可选？？？）</h3><p><a href="https://www.tutorialspoint.com/hive/hive_quick_guide.htm" target="_blank" rel="noopener">hive 验证可用</a></p>
<h3 id="配置-ranger-service-amp-policies"><a href="#配置-ranger-service-amp-policies" class="headerlink" title="配置 ranger service &amp; policies"></a>配置 ranger service &amp; policies</h3><p><a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/authorization-ranger/content/resource_service_configure_a_hive_service.html" target="_blank" rel="noopener">ranger-hive-service</a></p>
<p>service 命名</p>
<p>url: get from ambari-hive, or when you connect hive, it will show the whole connect string</p>
<p>policy 中 <code>all-database,table,column</code> 加上 <code>hive</code> user </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 重启 spark 服务</span><br><span class="line">Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br></pre></td></tr></table></figure>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ManagedandExternalTables" target="_blank" rel="noopener">hive managed &amp; external tables</a></p>
<p>acid table not supported now （acid new feature in hive，many others does not support it)</p>
<p><a href="https://github.com/Gowthamsb12/BigData-Blogs/blob/master/Spark_ACID" target="_blank" rel="noopener">spark-acids thoughts</a></p>
<p><img src="/images/ambari-table-create-in-hive.png" alt="image-20201211154708672"></p>
<p><img src="/images/ambari-table-create-in-spark.png" alt="image-20201211154728273"></p>
<p>I figured it out. Just set: <code>mapred.input.dir.recursive</code> and <code>hive.mapred.supports.subdirectories</code> to <code>true</code>. (Hive-site.xml)</p>
<p> /warehouse/tablespace/managed/hive/ftms_ods.db/test_user6/delta_0000001_0000001_0000</p>
<p>/warehouse/tablespace/managed/hive/ftms_ods.db/test_user7/part-00000-2a86feec-be9a-413d-a696-8ff115d14075-c000.snappy.orc</p>
<h2 id="包冲突"><a href="#包冲突" class="headerlink" title="包冲突"></a>包冲突</h2><p>xbean-asm5-shaded-4.4.jar</p>
<p>xmlbeans-3.1.0.jar</p>
<p>xercesImpl-2.9.1.jar</p>
<p>xerces2-xsd11-2.11.1.jar</p>
<p>Xmlapis.jar</p>
<h2 id="Xml"><a href="#Xml" class="headerlink" title="Xml"></a>Xml</h2><p>删除 /user/ftms/lib/xercesImpl-2.9.1.jar 和 /user/ftms/lib/xml-apis-1.3.04.jar</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: javax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:311)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267)</span><br><span class="line">	at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:120)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.asXmlDocument(Configuration.java:3442)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3417)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3388)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3384)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.getConfVarInputStream(HiveConf.java:2410)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:2703)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:2657)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.hiveConf(HiveMetaStoreUtil.scala:18)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.createHiveMetaStoreClient(HiveMetaStoreUtil.scala:23)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveMetaStoreClient(HiveMetaStoreUtil.scala:34)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionCols(HiveMetaStoreUtil.scala:78)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionColNames(HiveMetaStoreUtil.scala:73)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveDataSource$.buildInsertSql(HiveDataSource.scala:7)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveDataSource$.save(HiveDataSource.scala:42)</span><br><span class="line">	at com.ftms.datapipeline.tasks.dwd.DCompany$.main(DCompany.scala:197)</span><br><span class="line">	at com.ftms.datapipeline.tasks.dwd.DCompany.main(DCompany.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)</span><br><span class="line">Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)</span><br><span class="line">	... 23 more</span><br><span class="line">Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Provider org.apache.xerces.jaxp.DocumentBuilderFactoryImpl not found</span><br><span class="line">	at java.util.ServiceLoader.fail(ServiceLoader.java:239)</span><br><span class="line">	at java.util.ServiceLoader.access$300(ServiceLoader.java:185)</span><br><span class="line">	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372)</span><br><span class="line">	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)</span><br><span class="line">	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:294)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)</span><br><span class="line">	... 23 more</span><br></pre></td></tr></table></figure>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="在-yarn-client-模式下运行-spark"><a href="#在-yarn-client-模式下运行-spark" class="headerlink" title="在 yarn-client 模式下运行 spark"></a>在 yarn-client 模式下运行 spark</h3><p>会出现 <code>Library directory &#39;...\assembly\target\scala-2.11\jars&#39; does not exist; make sure Spark is built.</code>，这个大致原因是 yarn-client 模式下</p>
<p><a href="https://www.jianshu.com/p/016fbd2a421b" target="_blank" rel="noopener"><code>spark.yarn.jar</code>和<code>spark.yarn.archive</code>的使用</a></p>
<blockquote>
<p>Running Spark on YARN requires a binary distribution of Spark which is built with YARN support. Binary distributions can be downloaded from the <a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">downloads page</a> of the project website. To build Spark yourself, refer to <a href="https://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="noopener">Building Spark</a>.<br> To make Spark runtime jars accessible from YARN side, you can specify <code>spark.yarn.archive</code> or <code>spark.yarn.jars</code>. For details please refer to <a href="https://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties" target="_blank" rel="noopener">Spark Properties</a>. If neither <code>spark.yarn.archive</code> nor <code>spark.yarn.jars</code> is specified, Spark will create a zip file with all jars under <code>$SPARK_HOME/jars</code> and upload it to the distributed cache</p>
</blockquote>
<h1 id="Install-ClickHouse"><a href="#Install-ClickHouse" class="headerlink" title="Install ClickHouse"></a>Install ClickHouse</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装 clickhouse</span></span><br><span class="line">$ rpm -ivh clickhouse-server-common-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-common-static-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-server-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-client-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">$ service clickhouse-server start</span><br><span class="line">Start clickhouse-server service: Path to data directory <span class="keyword">in</span> /etc/clickhouse-server/config.xml: /var/lib/clickhouse/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 client 验证运行成功</span></span><br><span class="line">$ clickhouse-client</span><br><span class="line">ClickHouse client version 19.4.3.11.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 19.4.3 revision 54416.</span><br><span class="line"></span><br><span class="line">master :) select 1</span><br><span class="line"></span><br><span class="line">SELECT 1</span><br><span class="line"></span><br><span class="line">┌─1─┐</span><br><span class="line">│ 1 │</span><br><span class="line">└───┘</span><br><span class="line"></span><br><span class="line">1 rows <span class="keyword">in</span> <span class="built_in">set</span>. Elapsed: 0.001 sec.</span><br></pre></td></tr></table></figure>
<h2 id="client-启动失败"><a href="#client-启动失败" class="headerlink" title="client 启动失败"></a>client 启动失败</h2><blockquote>
<p>error detail:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ClickHouse client version 20.8.3.18.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Code: 102. DB::NetException: Unexpected packet from server localhost:9000 (expected Hello or Exception, got Unknown packet)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>这个错表示，clickhouse-client 收到返回了，但是返回的结果是非预期错误。这个错一般是由于端口占用。可以通过 <code>netstat -antp|grep LIST|grep 9000</code> 查询。</p>
<h3 id="Solution："><a href="#Solution：" class="headerlink" title="Solution："></a>Solution：</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新 clickHouse 的端口为 9011</span></span><br><span class="line">$ vi /etc/clickhouse-server/config.xml</span><br><span class="line">:%s/9000/9011</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动时带上端口号</span></span><br><span class="line">$ clickhouse-client --port 9011</span><br></pre></td></tr></table></figure>
<h1 id="安装-python3"><a href="#安装-python3" class="headerlink" title="安装 python3"></a>安装 python3</h1><p><a href="https://www.python.org/downloads/release/python-361/" target="_blank" rel="noopener">https://www.python.org/downloads/release/python-361/</a></p>
<p><a href="https://www.jianshu.com/p/758b592387d1" target="_blank" rel="noopener">reference doc</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解压并安装 python3</span></span><br><span class="line">$ tar -xf Python-3.?.?.tar.xz</span><br><span class="line">$ <span class="built_in">cd</span> Python-3.?.?</span><br><span class="line">$ ./configure</span><br><span class="line">$ make altinstall</span><br><span class="line">$ python3.x -V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建软链</span></span><br><span class="line">$ <span class="built_in">which</span> python3.6</span><br><span class="line">$ ln -s /usr/<span class="built_in">local</span>/bin/python3.6.1 /usr/bin/python3</span><br><span class="line">$ python3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 pip</span></span><br><span class="line">$ python3 -m ensurepip</span><br><span class="line">$ pip3</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果安装 pip 时报错 zlib not found：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装 zlib 相关依赖包</span></span><br><span class="line">$ yum -y install zlib*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 python安装包,修改Module路径的setup文件，Modules/Setup.dist （或者 Modules/Setup） 文件</span></span><br><span class="line">$ vi Module/Setup</span><br><span class="line"><span class="comment">#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz</span></span><br><span class="line">去掉注释</span><br><span class="line">     zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz</span><br></pre></td></tr></table></figure>
</blockquote>
<h1 id="安装-airflow"><a href="#安装-airflow" class="headerlink" title="安装 airflow"></a>安装 airflow</h1><p>为了方便管理，可以安装个 mpack，然后就可以从 ambari 安装、管理、监控 airflow</p>
<ul>
<li><a href="https://miho120.medium.com/integrating-apache-airflow-with-apache-ambari-ccab2c90173" target="_blank" rel="noopener">install airflow from ambari</a></li>
<li><a href="https://github.com/miho120/ambari-airflow-mpack" target="_blank" rel="noopener">git</a></li>
</ul>
<p>这个插件在安装/启动时，其实就是执行了 <code>/var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py</code> ，脚本中提供了安装、启动、停止。安装时，本质是执行了以下内容：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ pip install apache-airflow[all]==1.9.0 apache-airflow[celery]==1.9.0</span><br></pre></td></tr></table></figure>
<blockquote>
<p>相关的安装、启动等脚本都在 <code>/var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts</code> 目录下</p>
</blockquote>
<p>但上述过程只能在有线环境执行，离线环境还是得自己下。</p>
<h2 id="install-airflow-offline"><a href="#install-airflow-offline" class="headerlink" title="install airflow offline"></a>install airflow offline</h2><p><a href="https://airflow.apache.org/docs/stable/installation.html" target="_blank" rel="noopener">airflow installation 官方</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先下载相关包</span></span><br><span class="line">$ mkdir airflow-install</span><br><span class="line">$ <span class="built_in">cd</span> airflow-install</span><br><span class="line">$ pip download <span class="string">'apache-airflow[all]==1.10.12'</span> \</span><br><span class="line">--constraint  <span class="string">'https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.6.txt'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来更新上述 `airflow_scheduler_control.py` 中的安装脚本，选择从本地文件安装即可</span></span><br><span class="line">$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py</span><br><span class="line">$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_webserver_control.py</span><br><span class="line"><span class="comment"># 下边这个命令是上述脚本的内容，这里只是介绍一下执行的命令</span></span><br><span class="line">$ pip install apache-airflow==1.10.12 --no-index -f ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过程中可能会有些包缺，例如 docutils、pytest-runner 等，从 https://pypi.org/ 下载相应的 .whl 文件，放到该目录下</span></span><br><span class="line"><span class="comment"># 然后通过 --no-index -f ./ 或者 --no-index -f ./xxx.whl 来安装即可</span></span><br><span class="line">$ pip install pytest-runner --no-index -f ./</span><br></pre></td></tr></table></figure>
<p>实际操作时，下载完 airflow 后，就更新 airflow_scheduler_control.py 和 airflow_webserver_control.py （两个文件的 install method 是一模一样的，按同样的方式修改就行）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">################# 源文件</span></span><br><span class="line">     <span class="number">11</span>         <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></span><br><span class="line">     <span class="number">12</span>                 <span class="keyword">import</span> params</span><br><span class="line">     <span class="number">13</span>                 env.set_params(params)</span><br><span class="line">     <span class="number">14</span>                 print(<span class="string">'*'</span> * <span class="number">30</span>)</span><br><span class="line">     <span class="number">15</span>                 print(env)</span><br><span class="line">     <span class="number">16</span>                 print(<span class="string">'*'</span> * <span class="number">30</span>)</span><br><span class="line">     <span class="number">17</span>                 self.install_packages(env)</span><br><span class="line">     <span class="number">18</span>                 Logger.info(format(<span class="string">"Installing Airflow Service"</span>))</span><br><span class="line">     <span class="number">19</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; pip"</span>))</span><br><span class="line">     <span class="number">20</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; setuptools"</span>))</span><br><span class="line">     <span class="number">21</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython==0.28"</span>))</span><br><span class="line">     <span class="number">22</span>                 Execute(format(<span class="string">"export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-        installed apache-airflow[all]==1.10.0"</span>))</span><br><span class="line">     <span class="number">23</span>                 Execute(format(<span class="string">"export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-        installed apache-airflow[celery]==1.10.0"</span>))</span><br><span class="line">     <span class="number">24</span>                 Execute(format(<span class="string">"chmod 755 /bin/airflow /usr/bin/airflow"</span>))</span><br><span class="line">     <span class="number">25</span>                 Execute(format(<span class="string">"useradd &#123;airflow_user&#125;"</span>), ignore_failures=<span class="literal">True</span>)</span><br><span class="line">     <span class="number">26</span>                 Execute(format(<span class="string">"mkdir -p &#123;airflow_home&#125;"</span>))</span><br><span class="line">     <span class="number">27</span>                 airflow_make_startup_script(env)</span><br><span class="line">     <span class="number">28</span>                 Execute(format(<span class="string">"chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;"</span>))</span><br><span class="line">     <span class="number">29</span>                 Execute(format(<span class="string">"export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb"</span>),</span><br><span class="line">     <span class="number">30</span>                         user=params.airflow_user</span><br><span class="line">     <span class="number">31</span>                 )</span><br><span class="line"></span><br><span class="line"><span class="comment">################### 修改后的</span></span><br><span class="line">     <span class="number">11</span>         <span class="function"><span class="keyword">def</span> <span class="title">install</span><span class="params">(self, env)</span>:</span></span><br><span class="line">     <span class="number">12</span>                 <span class="keyword">import</span> params</span><br><span class="line">     <span class="number">13</span>                 env.set_params(params)</span><br><span class="line">    <span class="comment"># 这里是去安装 pip，已经装好了，就不装了</span></span><br><span class="line">     <span class="number">14</span> <span class="comment">#               self.install_packages(env)</span></span><br><span class="line">     <span class="number">15</span>                 Logger.info(format(<span class="string">"Installing Airflow Service"</span>))</span><br><span class="line">     <span class="number">16</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; pip"</span>))</span><br><span class="line">     <span class="number">17</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; setuptools"</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx</span></span><br><span class="line">     <span class="number">18</span>                 Execute(format(<span class="string">"pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython --no-index -f /install/python_install/airflow-install/"</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12，且仅安装 minimal packages</span></span><br><span class="line">     <span class="number">19</span>                 Execute(format(<span class="string">"export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow==1.10.12 --no-index -f /install/python_install/airflow-install/"</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12</span></span><br><span class="line">     <span class="number">20</span>                 Execute(format(<span class="string">"export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow[celery]==1.10.12 --no-index -f /install/python_install/airflow-install/"</span>))</span><br><span class="line">    <span class="comment"># 目前系统中默认是安装在 /usr/local/bin/airflow</span></span><br><span class="line">     <span class="number">21</span>                 Execute(format(<span class="string">"chmod 755 /usr/local/bin/airflow"</span>))</span><br><span class="line">     <span class="number">22</span>                 Execute(format(<span class="string">"useradd &#123;airflow_user&#125;"</span>), ignore_failures=<span class="literal">True</span>)</span><br><span class="line">     <span class="number">23</span>                 Execute(format(<span class="string">"mkdir -p &#123;airflow_home&#125;"</span>))</span><br><span class="line">     <span class="number">24</span>                 airflow_make_startup_script(env)</span><br><span class="line">     <span class="number">25</span>                 Execute(format(<span class="string">"chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;"</span>))</span><br><span class="line">     <span class="number">26</span>                 Execute(format(<span class="string">"export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb"</span>),</span><br><span class="line">     <span class="number">27</span>                         user=params.airflow_user</span><br><span class="line">     <span class="number">28</span>                 )</span><br></pre></td></tr></table></figure>
<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p><strong>Admin Name</strong> : admin </p>
<p><strong>Cluster Name</strong> : ftms_hdp_qat </p>
<p><strong>Total Hosts</strong> : 3 (3 new) </p>
<p><strong>Repositories</strong>:</p>
<p>redhat7 (HDP-3.1):<br> <a href="http://10.66.18.11/ambari/HDP/centos7/3.1.4.0-315/" target="_blank" rel="noopener">http://10.66.18.11/ambari/HDP/centos7/3.1.4.0-315/</a></p>
<p>redhat7 (HDP-3.1-GPL):<br> <a href="http://10.66.18.11/ambari/HDP-GPL/centos7/3.1.4.0-315/" target="_blank" rel="noopener">http://10.66.18.11/ambari/HDP-GPL/centos7/3.1.4.0-315/</a></p>
<p>redhat7 (HDP-UTILS-1.1.0.22):<br> <a href="http://10.66.18.11/ambari/HDP-UTILS/centos7/1.1.0.22/" target="_blank" rel="noopener">http://10.66.18.11/ambari/HDP-UTILS/centos7/1.1.0.22/</a></p>
<p><strong>Services:</strong></p>
<p><em>HDFS</em> </p>
<p>DataNode : 2 hosts </p>
<p>NameNode : master </p>
<p>NFSGateway : 0 host </p>
<p>SNameNode : slave1 </p>
<p><em>YARN + MapReduce2</em> </p>
<p>Timeline Service V1.5 : slave1 </p>
<p>NodeManager : 1 host </p>
<p>ResourceManager : master </p>
<p>Timeline Service V2.0 Reader : master </p>
<p>Registry DNS : master </p>
<p><em>Tez</em> </p>
<p>Clients : 1 host </p>
<p><em>Hive</em> </p>
<p>Metastore : slave1 </p>
<p>HiveServer2 : slave1 </p>
<p>Database : Existing MySQL / MariaDB Database </p>
<p><em>HBase</em> </p>
<p>Master : master </p>
<p>RegionServer : 1 host </p>
<p>Phoenix Query Server : 0 host </p>
<p><em>Sqoop</em> </p>
<p>Clients : 1 host </p>
<p><em>Oozie</em> </p>
<p>Server : master </p>
<p>Database : Existing MySQL / MariaDB Database </p>
<p><em>ZooKeeper</em> </p>
<p>Server : 3 hosts </p>
<p><em>Infra Solr</em> </p>
<p>Infra Solr Instance : master </p>
<p><em>Ambari Metrics</em> </p>
<p>Metrics Collector : slave2 </p>
<p>Grafana : master </p>
<p><em>Atlas</em> </p>
<p>Metadata Server : slave1 </p>
<p><em>Kafka</em> </p>
<p>Broker : master </p>
<p><em>Ranger</em> </p>
<p>Admin : slave1 </p>
<p>Tagsync : 1 host </p>
<p>Usersync : slave1 </p>
<p><em>SmartSense</em> </p>
<p>Activity Analyzer : master </p>
<p>Activity Explorer : master </p>
<p>HST Server : master </p>
<p><em>Spark2</em> </p>
<p>Livy for Spark2 Server : 0 host </p>
<p>History Server : master </p>
<p>Thrift Server : 0 host </p>
]]></content>
      <tags>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>bean validator</title>
    <url>/2018/07/18/bean-validator/</url>
    <content><![CDATA[<p>bean validator 主要是验证一个 bean 的各字段是否满足一些约束，例如 <code>@NotNull</code></p>
<p>bean validation 有个规范 <a href="https://beanvalidation.org/2.0/" target="_blank" rel="noopener">jsr 380</a>，里边定义了一堆 api。有很多规范的实现，最常用的是 <a href="http://hibernate.org/validator/" target="_blank" rel="noopener">hibernate validator</a>，jersey 出的 <a href="https://jersey.github.io/documentation/latest/bean-validation.html" target="_blank" rel="noopener">jersey-bean-validation</a> 也是基于 hibernate validator 做的。</p>
<p>bean validator 一般是应用在 web 框架（如 spring、jersey）上，框架在反序列化 rest 请求到 bean 对象时，框架会调用 validator 根据 bean 对象的 annotation 对 bean 进行验证。</p>
<p>这个过程也可以手动进行。可参考 <a href="http://hibernate.org/validator/documentation/getting-started/" target="_blank" rel="noopener">hibernate validator: get started</a>。</p>
<h1 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h1><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line"><span class="comment">// jsr 380 api</span></span><br><span class="line">compile <span class="string">"javax.validation:validation-api:2.0.1.Final"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// hibernate vaidator 实现</span></span><br><span class="line">testCompile <span class="string">"org.hibernate.validator:hibernate-validator:6.0.10.Final"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// hibernate validator 依赖的 JSR 341 实现</span></span><br><span class="line">testCompile <span class="string">"org.glassfish:javax.el:3.0.0"</span></span><br></pre></td></tr></table></figure>
<h1 id="创建-bean，标明约束"><a href="#创建-bean，标明约束" class="headerlink" title="创建 bean，标明约束"></a>创建 bean，标明约束</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.hibernate.validator.referenceguide.chapter01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.Min;</span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.NotNull;</span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.Size;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@NotNull</span></span><br><span class="line">   <span class="keyword">private</span> String manufacturer;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@NotNull</span></span><br><span class="line">   <span class="meta">@Size</span>(min = <span class="number">2</span>, max = <span class="number">14</span>)</span><br><span class="line">   <span class="keyword">private</span> String licensePlate;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Min</span>(<span class="number">2</span>)</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">int</span> seatCount;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Car</span><span class="params">(String manufacturer, String licencePlate, <span class="keyword">int</span> seatCount)</span></span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.manufacturer = manufacturer;</span><br><span class="line">      <span class="keyword">this</span>.licensePlate = licencePlate;</span><br><span class="line">      <span class="keyword">this</span>.seatCount = seatCount;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><ol>
<li>利用 <code>Validation</code> 获取默认的 <code>ValidatorFactory</code>：<code>Validation.buildDefaultValidatorFactory()</code>；</li>
<li>从 <code>ValidatorFacotry</code> 获取 <code>Validator</code>：<code>factory.getValidator()</code></li>
<li>利用 <code>Validator</code> 验证 bean</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.hibernate.validator.referenceguide.chapter01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"><span class="keyword">import</span> javax.validation.ConstraintViolation;</span><br><span class="line"><span class="keyword">import</span> javax.validation.Validation;</span><br><span class="line"><span class="keyword">import</span> javax.validation.Validator;</span><br><span class="line"><span class="keyword">import</span> javax.validation.ValidatorFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.junit.BeforeClass;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.junit.Assert.assertEquals;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CarTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Validator validator;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@BeforeClass</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      ValidatorFactory factory = Validation.buildDefaultValidatorFactory();</span><br><span class="line">      validator = factory.getValidator();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">manufacturerIsNull</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      Car car = <span class="keyword">new</span> Car( <span class="keyword">null</span>, <span class="string">"DD-AB-123"</span>, <span class="number">4</span> );</span><br><span class="line"></span><br><span class="line">      Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations =</span><br><span class="line">      validator.validate( car );</span><br><span class="line"></span><br><span class="line">      assertEquals( <span class="number">1</span>, constraintViolations.size() );</span><br><span class="line">      assertEquals(</span><br><span class="line">         <span class="string">"may not be null"</span>,</span><br><span class="line">         constraintViolations.iterator().next().getMessage()</span><br><span class="line">      );</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">licensePlateTooShort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      Car car = <span class="keyword">new</span> Car( <span class="string">"Morris"</span>, <span class="string">"D"</span>, <span class="number">4</span> );</span><br><span class="line"></span><br><span class="line">      Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations =</span><br><span class="line">      validator.validate( car );</span><br><span class="line"></span><br><span class="line">      assertEquals( <span class="number">1</span>, constraintViolations.size() );</span><br><span class="line">      assertEquals(</span><br><span class="line">         <span class="string">"size must be between 2 and 14"</span>,</span><br><span class="line">         constraintViolations.iterator().next().getMessage()</span><br><span class="line">      );</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seatCountTooLow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      Car car = <span class="keyword">new</span> Car( <span class="string">"Morris"</span>, <span class="string">"DD-AB-123"</span>, <span class="number">1</span> );</span><br><span class="line"></span><br><span class="line">      Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations =</span><br><span class="line">      validator.validate( car );</span><br><span class="line"></span><br><span class="line">      assertEquals( <span class="number">1</span>, constraintViolations.size() );</span><br><span class="line">      assertEquals(</span><br><span class="line">         <span class="string">"must be greater than or equal to 2"</span>,</span><br><span class="line">         constraintViolations.iterator().next().getMessage()</span><br><span class="line">      );</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">carIsValid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      Car car = <span class="keyword">new</span> Car( <span class="string">"Morris"</span>, <span class="string">"DD-AB-123"</span>, <span class="number">2</span> );</span><br><span class="line"></span><br><span class="line">      Set&lt;ConstraintViolation&lt;Car&gt;&gt; constraintViolations =</span><br><span class="line">      validator.validate( car );</span><br><span class="line"></span><br><span class="line">      assertEquals( <span class="number">0</span>, constraintViolations.size() );</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 hexo + github 部署博客</title>
    <url>/2018/05/14/build-blog-using-hexo/</url>
    <content><![CDATA[<h1 id="安装部署-hexo"><a href="#安装部署-hexo" class="headerlink" title="安装部署 hexo"></a>安装部署 <a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">hexo</a></h1><p>参考这个<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">知乎文章安装</a></p>
<h2 id="1-准备-node、git"><a href="#1-准备-node、git" class="headerlink" title="1. 准备 node、git"></a>1. 准备 node、git</h2><h2 id="2-安装-hexo-cli"><a href="#2-安装-hexo-cli" class="headerlink" title="2. 安装 hexo-cli"></a>2. 安装 hexo-cli</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ npm i -g hexo-cli</span><br></pre></td></tr></table></figure>
<h2 id="3-创建一个网站"><a href="#3-创建一个网站" class="headerlink" title="3. 创建一个网站"></a>3. 创建一个网站</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo init xxx.github.io</span><br></pre></td></tr></table></figure>
<h2 id="4-部署服务器"><a href="#4-部署服务器" class="headerlink" title="4. 部署服务器"></a>4. <a href="https://hexo.io/zh-cn/docs/deployment.html" target="_blank" rel="noopener">部署服务器</a></h2><p>这里选择部署到 git 上。</p>
<ol>
<li>首先安装 git deployer</li>
<li>然后修改配置文件，选择部署方式为 git 并配置 repo。</li>
<li>hexo 部署</li>
<li>访问 <code>xxx.github.io</code> 即可</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br><span class="line">$ vi _config.yml</span><br><span class="line"></span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt;</span><br><span class="line">  branch: [branch]</span><br><span class="line">  message: [message]</span><br><span class="line"></span><br><span class="line">$ npm d</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<blockquote>
<ol>
<li>可以选择同时部署到多个服务器。多写几个 ‘deploy’ 配置即可</li>
<li>git 用户名、网站用户名（<code>xxx.github.io</code> 中的 <code>xxx</code>）必须相同。因为它相当于使用 github 服务器</li>
</ol>
</blockquote>
<h1 id="设置-theme"><a href="#设置-theme" class="headerlink" title="设置 theme"></a>设置 theme</h1><p>在 <a href="https://hexo.io/themes/index.html" target="_blank" rel="noopener">官方 themes</a> 里挑。我比较喜欢以下几款：</p>
<ol>
<li>带目录结构的：<ol start="2">
<li><a href="https://github.com/probberechts/hexo-theme-cactus/blob/master/README.md" target="_blank" rel="noopener">cactus</a>：英文的，有几种颜色可以选，带目录，可以配置搜索，简洁，这是<a href="https://probberechts.github.io/hexo-theme-cactus/cactus-white/public/archives/" target="_blank" rel="noopener">white 版本的</a></li>
<li><a href="https://github.com/aircloud/hexo-theme-aircloud" target="_blank" rel="noopener">aircloud</a>：英文中文都 ok，有目录，还可以搜索</li>
<li><a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">next</a>：有目录，也有集成搜索的文档，这是一个 <a href="http://www.itfanr.cc/about/" target="_blank" rel="noopener">example</a>，参照 <a href="https://theme-next.iissnan.com/third-party-services.html" target="_blank" rel="noopener">第三方集成</a> 集成搜索等功能. next 优化配置可参考 <a href="http://www.vitah.net/posts/20f300cc/" target="_blank" rel="noopener">这篇文章</a></li>
<li><a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">yilia</a>：有目录，有搜索，<a href="http://litten.me/2017/12/29/diary-2017-1222-1229/" target="_blank" rel="noopener">owner 博客</a></li>
</ol>
</li>
<li>没有目录，没有 tag<ol start="3">
<li><a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank" rel="noopener">apollo</a>：<a href="http://pinggod.com/archives/" target="_blank" rel="noopener">blog</a> 是中文的，比较简单，颜色也好看</li>
</ol>
</li>
</ol>
<p>配置很简单，以 <a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">yilia</a> 为例：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 找到 theme git，download 到 `themes` 文件夹下</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 修改 hexo 根目录下的配置文件，指定使用该主题</span></span><br><span class="line">$ vi _config.yml</span><br><span class="line">theme: yilia</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. customize 主题配置. 可以修改 themes 中的 config，也可以直接在 hexo config 中修改</span></span><br><span class="line"><span class="comment">## 修改 themes 中的 _config.yml</span></span><br><span class="line">$ vi themes/yilia/_config.yml</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">## 修改 hexo config</span></span><br><span class="line">$ vi _config.yml</span><br><span class="line">theme_config:</span><br><span class="line">	...</span><br><span class="line">	...</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>
<h1 id="发表博客"><a href="#发表博客" class="headerlink" title="发表博客"></a>发表博客</h1><h2 id="1-创建博客"><a href="#1-创建博客" class="headerlink" title="1. 创建博客"></a>1. <a href="https://hexo.io/zh-cn/docs/commands.html#new" target="_blank" rel="noopener">创建博客</a></h2><p>利用命令创建一个博客，存放在 <code>source/_posts/</code> 下。然后可以编辑这个文件。刷新页面就可以看到博客有更新了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo new titlename</span><br></pre></td></tr></table></figure>
<p>也可以直接把 md 文件 copy 到 <code>source/_posts/</code> 下，可以添加 <a href="https://hexo.io/zh-cn/docs/front-matter.html" target="_blank" rel="noopener"><code>frong-matter</code></a> 指定 category、tag 等。</p>
<h2 id="2-生成静态文件"><a href="#2-生成静态文件" class="headerlink" title="2. 生成静态文件"></a>2. <a href="https://hexo.io/zh-cn/docs/generating.html" target="_blank" rel="noopener">生成静态文件</a></h2><p><code>hexo g</code> 会根据 md 生成 html、css 等静态文件。文章写完后，利用这个命令生成静态文件，然后再 <code>hexo d</code> 部署即可。</p>
<p>也可以使用下述命令（两个命令等价），指同时 <code>generate</code> + <code>deploy</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g -d</span><br><span class="line">$ hexo d -g</span><br></pre></td></tr></table></figure>
<h3 id="hexo-clean"><a href="#hexo-clean" class="headerlink" title="hexo clean"></a>hexo clean</h3><p>有时可能需要使用 <a href="https://hexo.io/zh-cn/docs/commands.html#clean" target="_blank" rel="noopener"><code>hexo clean</code></a> 清除缓存文件 <code>db.json</code> 和已生成的静态文件 <code>public</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure>
<h3 id="启动本地服务器（调试用）"><a href="#启动本地服务器（调试用）" class="headerlink" title="启动本地服务器（调试用）"></a><a href="https://hexo.io/zh-cn/docs/commands.html#server" target="_blank" rel="noopener">启动本地服务器（调试用）</a></h3><p>博客编辑完成后，可以先本地启动 server，看一下效果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
<p><code>-s</code> 参数指定仅仅启动静态模式，即创建博客后，必须要 <code>hexo g</code> 去生成 <code>index.html</code> 等（相当于发布），网站才会真正的更新。否则网站不会更新。这一般用于 <code>production mode</code></p>
<p>一般编辑完文章后，可以先本地启动服务器，调试一下样式可不可以，然后再部署到服务器上</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>c# basic</title>
    <url>/2019/06/05/c-sharp-basic/</url>
    <content><![CDATA[<h1 id="net-asp-net-c"><a href="#net-asp-net-c" class="headerlink" title=".net, asp.net, c"></a>.net, asp.net, c</h1><p>c# is like java language specification;</p>
<p>.net is like jdk/javase/javaee</p>
<p><a href="https://dotnet.microsoft.com/apps/aspnet" target="_blank" rel="noopener">asp.net</a>: is like springboot</p>
<ol>
<li><a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/statements-expressions-operators/default-value-expressions" target="_blank" rel="noopener">default</a>, as, is</li>
<li>sln: solution ——&gt; csproj: c sharp project ——&gt; files <a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/statements-expressions-operators/default-value-expressions" target="_blank" rel="noopener">.sln vs .csproj</a></li>
</ol>
<h1 id="Key-concepts"><a href="#Key-concepts" class="headerlink" title="Key concepts"></a>Key concepts</h1><p>ref: <a href="https://blog.csdn.net/baidu_32134295/article/details/51285603" target="_blank" rel="noopener">c# concepts</a></p>
<ul>
<li><strong>solution</strong>: a complete application, similar to maven project. It contains several c# project like frontend, backend, library to compose a complete application.</li>
<li><strong>project</strong>: similar to maven module. It can be a web project, a library, a windows program, etc.</li>
<li><strong>assembly</strong>: similar to maven jar. A c# project is corresponding to an assembly. An assembly can be a <code>dll</code>, <code>exe</code>, etc.</li>
<li><strong>namespace</strong>: similar to java package. It’s a logical concept to avoid naming conflicts while assembly is a physical concept. A namespace can be in different assemblies and an assembly can contains multiple namespaces.</li>
</ul>
<h1 id="Accessibility-levels"><a href="#Accessibility-levels" class="headerlink" title="Accessibility levels"></a>Accessibility levels</h1><p>ref: <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/accessibility-levels" target="_blank" rel="noopener">accessibility levels</a></p>
<ul>
<li><strong>public</strong>: access is not restricted. (all)</li>
<li><strong>private</strong>: limited to containing type (only self)</li>
<li><strong>protected</strong>: limited to the containing class and types derived from the containing class. (sub-classes)</li>
<li><strong>internal</strong>: limited to the current assembly (only the same assembly)</li>
<li><strong>protected internal</strong>: limited to the current assembly or types derived from the containing class (same assembly &amp; sub-classes)</li>
<li><strong>protected private</strong>: limited to the containing class and types derived from the containing class in the current assembly (sub-classes in same assembly)</li>
</ul>
<h1 id="data-types"><a href="#data-types" class="headerlink" title="data types"></a>data types</h1><h2 id="Nullable"><a href="#Nullable" class="headerlink" title="Nullable"></a>Nullable</h2><p><a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/nullable-types/" target="_blank" rel="noopener">Nullable<T></a> (T?): similar to Optional in java, but the T can only be <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/introduction#types-and-variables" target="_blank" rel="noopener">value type</a>, which are simple types, enum types, struct types, and nullable types. Because value type has no null value (not alike reference type — — object), and there’re situations when their values are undefined, nullable is born.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int? x &#x3D; null    &#x2F;&#x2F; int? is the shorthand for Nullable&lt;int&gt;</span><br></pre></td></tr></table></figure>
<h2 id="Delegate"><a href="#Delegate" class="headerlink" title="Delegate"></a>Delegate</h2><p><a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/introduction#delegates" target="_blank" rel="noopener">delegate</a> is like <code>@FunctionalInterface</code> in java.</p>
<blockquote>
<p>A <strong><em>delegate type</em></strong> represents references to methods with a particular parameter list and return type. Delegates make it possible to treat methods as entities that can be assigned to variables and passed as parameters. </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">delegate double Function(double x);    &#x2F;&#x2F; functional interface definition</span><br><span class="line"></span><br><span class="line">static double[] Apply(double[] a, Function f) &#123;&#125;  &#x2F;&#x2F; use delegate as a method param</span><br><span class="line"></span><br><span class="line">Apply(&#123;0.0, 0.5, 1.0&#125;, (double x) -&gt; x*x);   &#x2F;&#x2F; define an anoymous function</span><br></pre></td></tr></table></figure>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/introduction#methods" target="_blank" rel="noopener">Method</a> contains parameters &amp; return type &amp; body &amp; modifier &amp; type parameters.</p>
<h2 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h2><p>Argument is where the initial value is from, and parameter is used to pass value/references to methods. </p>
<p>Parameter has modifier (e.g. out, final, this, params) and type</p>
<p>Argument are passed as parameter in 4 ways:</p>
<h3 id="value-parameter"><a href="#value-parameter" class="headerlink" title="value parameter:"></a>value parameter:</h3><p>parameter change won’t affect the argument</p>
<ol>
<li>only for input parameter passing</li>
<li>optional by specify default value</li>
</ol>
<h3 id="reference-parameter"><a href="#reference-parameter" class="headerlink" title="reference parameter"></a>reference parameter</h3><p>parameter change will affect the argument</p>
<ol>
<li>for input/output parameter passing</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void swap(ref int x, ref int y)</span><br></pre></td></tr></table></figure>
<h3 id="output-parameter"><a href="#output-parameter" class="headerlink" title="output parameter"></a>output parameter</h3><p>similar to reference parameter except for that the initial value is unimportant.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void divide(int x, int y, out int res, out int remainder)&#123;&#125;</span><br><span class="line"></span><br><span class="line">divide(1,2,out var res, out var remainder);</span><br></pre></td></tr></table></figure>
<h3 id="parameter-arrays"><a href="#parameter-arrays" class="headerlink" title="parameter arrays"></a>parameter arrays</h3><p>similar to java <code>…</code></p>
<h2 id="extension-methods"><a href="#extension-methods" class="headerlink" title="extension methods"></a>extension methods</h2><p><a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/extension-methods" target="_blank" rel="noopener">extension method</a> is a mechanism that you can “add method” to a class without extending from it.</p>
<p>This method:</p>
<ol>
<li>must be static</li>
<li>works in scope when you explictly import the namespace into you source code with a <code>using</code> directive.</li>
<li>must be the first param of method.</li>
<li>when used, it’s same as the normal instance method.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">namespace ExtensionMethods</span><br><span class="line">&#123;</span><br><span class="line">    public static class MyExtensions</span><br><span class="line">    &#123;</span><br><span class="line">        public static int WordCount(this String str)</span><br><span class="line">        &#123;</span><br><span class="line">            return str.Split(new char[] &#123; &#39; &#39;, &#39;.&#39;, &#39;?&#39; &#125;, </span><br><span class="line">                             StringSplitOptions.RemoveEmptyEntries).Length;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>c#</tag>
      </tags>
  </entry>
  <entry>
    <title>az data engineer certificate</title>
    <url>/2020/10/10/az-data-engineer-certificate/</url>
    <content><![CDATA[<p><a href="https://docs.microsoft.com/zh-cn/learn/certifications/azure-data-engineer?tab=tab-learning-paths" target="_blank" rel="noopener">learning paths</a></p>
<h2 id="On-premises-Env-vs-Cloud"><a href="#On-premises-Env-vs-Cloud" class="headerlink" title="On-premises Env vs Cloud"></a>On-premises Env vs Cloud</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/evolving-world-of-data/3-systems-on-premise-vs-cloud" target="_blank" rel="noopener">link</a></p>
<p>The term <em>total cost of ownership</em> (TCO) describes the final cost of owning a given technology. In <strong>on-premises systems</strong>, TCO includes the following costs:</p>
<ul>
<li>Hardware</li>
<li>Software licensing</li>
<li>Labor (installation, upgrades, maintenance)</li>
<li>Datacenter overhead (power, telecommunications, building, heating and cooling)</li>
</ul>
<p><strong>Cloud systems</strong> like Azure track costs by subscriptions. A subscription can be based on usage that’s measured in compute units, hours, or transactions. The cost includes hardware, software, disk storage, and labor. Because of economies of scale, an on-premises system can rarely compete with the cloud in terms of the measurement of the service usage.</p>
<p>The cost of operating an on-premises server system rarely aligns with the actual usage of the system. In cloud systems, the cost usually aligns more closely with the actual usage.</p>
<h4 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h4><blockquote>
<p>However, many companies use the cloud still in a wasting way. The cost in cloud systems in fact rarely aligns with the actual usage, too.</p>
<p>The main advantage of Cloud is that it can <strong>be charged on usage</strong>. Thus, this advantage only works when using the cloud by need. Otherwise, the cloud advantage evaporates, especially from the aspect of cost.</p>
<p>The advantages of cloud:</p>
<ol>
<li>charge on usage</li>
<li>enjoy the high quality and compresensive services of big company.</li>
</ol>
</blockquote>
<h2 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/survey-the-azure-data-platform/2-structured-vs-non-structured" target="_blank" rel="noopener">link</a></p>
<p>For nonstructured Data, the data <strong>structure is defined only when the data is read</strong>. The difference in the definition point gives you flexibility to use the same source data for different outputs.</p>
<blockquote>
<p>JSON is in fact semistructured data.</p>
</blockquote>
<p>Examples of nonstructured data include binary, audio, and image files. </p>
<p>NoSQL is in fact semistructured data. The open-source world offers four types of NoSQL databases:</p>
<ol>
<li><strong>Key-value store</strong>: Stores key-value pairs of data in a table structure.</li>
<li><strong>Document database</strong>: Stores documents that are <strong>tagged with metadata</strong> to aid document searches.</li>
<li><strong>Graph database</strong>: Finds relationships between data points by using a structure that’s composed of vertices and edges.</li>
<li><strong>Column database</strong>: Stores data based on columns rather than rows. Columns can be defined at the query’s runtime, allowing flexibility in the data that’s returned performantly.</li>
</ol>
<h3 id="Azure-Storage"><a href="#Azure-Storage" class="headerlink" title="Azure Storage"></a>Azure Storage</h3><p><a href="https://docs.microsoft.com/en-us/learn/modules/survey-the-azure-data-platform/3-store-data-using-azure-store-account" target="_blank" rel="noopener">Azure Storage account</a> is the base storage type. It’s mainly used to store data but with poor or no query ability.</p>
<p>Azure Storage offers four configuration options:</p>
<ul>
<li><strong>Azure Blob</strong>: A scalable object store for text and binary data. The cheapst choice to store bot not query data.</li>
<li><strong>Azure Files</strong>: Managed file shares for cloud or on-premises deployments</li>
<li><strong>Azure Queue</strong>: A messaging store for reliable messaging between application components</li>
<li><strong>Azure Table</strong>: A NoSQL store for no-schema storage of structured data</li>
</ul>
<h2 id="Tasks-of-an-Azure-data-engineer"><a href="#Tasks-of-an-Azure-data-engineer" class="headerlink" title="Tasks of an Azure data engineer:"></a>Tasks of an Azure data engineer:</h2><p> <a href="https://docs.microsoft.com/en-us/learn/modules/data-engineering-processes/3-data-engineering-practices" target="_blank" rel="noopener">link</a></p>
<p><a href="https://docs.microsoft.com/zh-cn/learn/modules/data-engineering-processes/4-architecturing-project" target="_blank" rel="noopener">example</a></p>
<p><a href="https://docs.microsoft.com/en-us/learn/modules/data-engineering-processes/2-roles-and-responsibilities" target="_blank" rel="noopener">Data Engineer vs Data Scientist vs AI engineer</a>: Data engineer decide how to organized the data and pre-process the data. Data scientist use the result of Data engineer to create analysis model, and extract value. AI engineer use the existed model &amp; tools to process the data. AI engineer may need the help of Data engineer to store the result, and the help of Data analysis to generate new model.</p>
<p>Here are some of the tasks of an Azure data engineer:</p>
<ul>
<li>Design and develop data storage and data processing solutions for the enterprise.</li>
<li>Set up and deploy cloud-based data services such as blob services, databases, and analytics.</li>
<li>Secure the platform and the stored data. Make sure only the necessary users can access the data.</li>
<li>Ensure business continuity in uncommon conditions by using techniques for high availability and disaster recovery.</li>
<li>Monitor to ensure that the systems run properly and are cost-effective.</li>
</ul>
<h2 id="Plan-the-data-storage-solution"><a href="#Plan-the-data-storage-solution" class="headerlink" title="Plan the data storage solution"></a>Plan the data storage solution</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/3-operations-and-latency" target="_blank" rel="noopener">Determine operational needs</a></p>
<p>What are the main operations you’ll be completing on each data type, and what are the performance requirements?</p>
<p>Ask yourself these questions:</p>
<ul>
<li>Will you be doing simple lookups using an ID?</li>
<li>Do you need to query the database for one or more fields?</li>
<li>How many create, update, and delete operations do you expect?</li>
<li>Do you need to run complex analytical queries?</li>
<li>How quickly do these operations need to complete?</li>
</ul>
<p><a href="https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/5-choose-the-right-azure-service-for-your-data" target="_blank" rel="noopener">a storage solution example on the e-commerce system</a>: </p>
<ul>
<li><p>Product catalog data: cosmosDB</p>
<ul>
<li><p><strong>Data classification:</strong> Semi-structured because of the need to extend or modify the schema for new products</p>
<p><strong>Operations:</strong></p>
<ul>
<li>Customers require a high number of read operations, with the ability to query on many fields within the database.</li>
<li>The business requires a high number of write operations to track the constantly changing inventory.</li>
</ul>
<p><strong>Latency &amp; throughput:</strong> High throughput and low latency</p>
<p><strong>Transactional support:</strong> Required</p>
</li>
</ul>
</li>
<li><p>Photos &amp; videos: azure blob (with azure CDN)</p>
<ul>
<li><p><strong>Data classification:</strong> Unstructured</p>
<p><strong>Operations:</strong></p>
<ul>
<li>Only need to be retrieved by ID.</li>
<li>Customers require a high number of read operations with low latency.</li>
<li>Creates and updates will be somewhat infrequent and can have higher latency than read operations.</li>
</ul>
<p><strong>Latency &amp; throughput:</strong> Retrievals by ID need to support low latency and high throughput. Creates and updates can have higher latency than read operations.</p>
<p><strong>Transactional support:</strong> Not required</p>
</li>
</ul>
</li>
<li><p>Buisiness Data: azure sql (with azure analysis services)</p>
<ul>
<li><p><strong>Data classification:</strong> Structured</p>
<p><strong>Operations:</strong> Read-only, complex analytical queries across multiple databases</p>
<p><strong>Latency &amp; throughput:</strong> Some latency in the results is expected based on the complex nature of the queries.</p>
<p><strong>Transactional support:</strong> Required</p>
</li>
</ul>
</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><h2 id="Private-cloud-vs-Public-Cloud-vs-Specific-Cloud"><a href="#Private-cloud-vs-Public-Cloud-vs-Specific-Cloud" class="headerlink" title="Private cloud vs Public Cloud vs Specific Cloud"></a>Private cloud vs Public Cloud vs Specific Cloud</h2><ul>
<li><p>[x] Are there still private cloud and public cloud??    ===&gt; yes</p>
</li>
<li><p>[ ] What’s the difference? The private cloud will have higher quality cloud? The so-called SLA of public cloud is in fact not assured??</p>
</li>
<li><p>[x] Maybe there’re also specific cloud, like financial cloud (maybe more secure and fast) ===&gt; yes</p>
</li>
</ul>
<h4 id="Points"><a href="#Points" class="headerlink" title="Points:"></a>Points:</h4><blockquote>
<ol>
<li>There is specific cloud. Mayi has financial cloud.</li>
<li>There are both public cloud &amp; private cloud, but they’re closely related. That is, they use the same cloud technology (images), but with different clusters &amp; differnt tariff.</li>
</ol>
</blockquote>
<h2 id="Azure-storage-account"><a href="#Azure-storage-account" class="headerlink" title="Azure storage account"></a>Azure storage account</h2><ol>
<li>how to configure to use different type?</li>
<li>Why they’re all in storage account instead of as independant service??</li>
<li>azure blob vs azure file storage</li>
</ol>
<h2 id="Column-database"><a href="#Column-database" class="headerlink" title="Column database"></a>Column database</h2><ol>
<li>How the data is organized in the hardware?</li>
<li>How should we save the data into column database?</li>
<li>Wide Column Stores</li>
<li>BigTable</li>
</ol>
<p>Cosmosdb(Cassendra) vs HBase: infrastructure, load-balance???</p>
<p>Cassendra: Wide Column Stores</p>
<h2 id="ELT-vs-ELTL"><a href="#ELT-vs-ELTL" class="headerlink" title="ELT vs ELTL"></a>ELT vs ELTL</h2><ul>
<li style="list-style: none"><input type="checkbox"></input> In fact, often ELTL???</li>
<li style="list-style: none"><input type="checkbox"></input> How is the data stored in T(transform)???</li>
</ul>
<h2 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h2><ul>
<li style="list-style: none"><input type="checkbox"></input> what’s markup language???</li>
<li style="list-style: none"><input type="checkbox"></input> why yaml is not markup language but json is???</li>
</ul>
<h2 id="Azure-SQL"><a href="#Azure-SQL" class="headerlink" title="Azure SQL"></a>Azure SQL</h2><ul>
<li style="list-style: none"><input type="checkbox"></input> how azure sql support queries across multiple databases??</li>
<li style="list-style: none"><input type="checkbox"></input> Why does azure sql data warehouse not support???</li>
</ul>
]]></content>
      <tags>
        <tag>big data</tag>
        <tag>certificate</tag>
        <tag>cloud computing</tag>
      </tags>
  </entry>
  <entry>
    <title>data lake</title>
    <url>/2020/11/09/data-lake/</url>
    <content><![CDATA[<h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://zhuanlan.zhihu.com/p/91165577" target="_blank" rel="noopener">数据湖</a></p>
<p>数据湖是：</p>
<ol>
<li>装有一些便于提取、分析、搜索、挖掘的设备（本身不具备分析能力，是其他分析工具可以方便的在湖上运行，而不需要把湖的数据挪出去再分析）</li>
<li>存放各种数据（格式不统一，原始数据）：结构、半结构、非结构化</li>
<li>来源各种各样，能很方便的导入到数据湖</li>
</ol>
<p><strong>数据湖就是原始数据保存区</strong>. 虽然这个概念国内谈的少，但<strong>绝大部分互联网公司都已经有了</strong>。国内一般把整个HDFS叫做数据仓库（广义），<strong>即存放所有数据的地方</strong>，而国外一般叫数据湖（data lake）。把需要的数据导入到数据湖，如果你想结合来自数据湖的信息和客户关系管理系统（CRM）里面的信息，我们就进行连接，<strong>只有需要时才执行这番数据结合</strong>。</p>
<p>数据湖是多结构数据的系统或存储库，它们以原始格式和模式存储，通常作为对象“blob”或文件存储。数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。数据湖中的数据包括结构化数据（关系数据库数据），半结构化数据（CSV、XML、JSON等），非结构化数据（电子邮件，文档，PDF）和二进制数据（图像、音频、视频），从而形成一个容纳所有形式数据的集中式数据存储。</p>
<p>数据湖从本质上来讲，是一种企业数据架构方法，物理实现上则是一个数据存储平台，用来集中化存储企业内海量的、多来源，多种类的数据，<strong>并支持对数据进行快速加工和分析</strong> (支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统）。从实现方式来看，目前Hadoop是最常用的部署数据湖的技术，但并不意味着数据湖就是指Hadoop集群。为了应对不同业务需求的特点，MPP数据库+Hadoop集群+传统数据仓库这种“混搭”架构的数据湖也越来越多出现在企业信息化建设规划中。</p>
<blockquote>
<p><code>Data Lake</code>： 数据湖<br><code>Data Swamp</code>： 数据沼泽<br><code>Data Mart</code>： 数据集市<br><code>Data Warehouse</code>： 数据仓库<br><code>Data Cube</code>：数据立方体<br><code>Data Stream</code>：数据流<br><code>Data Virtualization</code>：数据虚拟化</p>
</blockquote>
<h2 id="错误认知"><a href="#错误认知" class="headerlink" title="错误认知"></a>错误认知</h2><ul>
<li>错误认知1： 数据湖仅用于“存储”数据<ul>
<li>支持对数据进行快速加工和分析。支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统</li>
</ul>
</li>
<li>错误认知2：数据湖仅存储“原始”数据<ul>
<li>需要有定义的机制来编目和保护数据。这些元素并非原始数据，而是对数据湖的管理数据。</li>
</ul>
</li>
</ul>
<h2 id="数据和和分析解决方案的基本要素"><a href="#数据和和分析解决方案的基本要素" class="headerlink" title="数据和和分析解决方案的基本要素"></a>数据和和分析解决方案的基本要素</h2><p>组织构建数据湖和分析平台时，他们需要考虑许多关键功能，包括：</p>
<h4 id="数据移动（支持大规模的数据以原始形式导入）"><a href="#数据移动（支持大规模的数据以原始形式导入）" class="headerlink" title="数据移动（支持大规模的数据以原始形式导入）"></a>数据移动（支持大规模的数据以原始形式导入）</h4><p>数据湖允许您导入任何数量的实时获得的数据。您可以从多个来源收集数据，并以其原始形式将其移入到数据湖中。此过程允许您扩展到任何规模的数据，同时节省定义数据结构、Schema 和转换的时间。</p>
<h4 id="安全地存储和编目数据（编目使得数据是被监督的，可用的）"><a href="#安全地存储和编目数据（编目使得数据是被监督的，可用的）" class="headerlink" title="安全地存储和编目数据（编目使得数据是被监督的，可用的）"></a>安全地存储和编目数据（编目使得数据是被监督的，可用的）</h4><p>数据湖允许您存储关系数据（例如，来自业务线应用程序的运营数据库和数据）和非关系数据（例如，来自移动应用程序、IoT 设备和社交媒体的运营数据库和数据）。它们还使您能够通过<strong>对数据进行爬网、编目和建立索引</strong>来了解湖中的数据。最后，必须保护数据以确保您的数据资产受到保护。</p>
<blockquote>
<p>是数据湖里的数据本身有索引吗？还是基于数据湖做catalog、元数据管理等？catalog 即是对数据湖数据的索引？？？</p>
</blockquote>
<h4 id="分析（可以直接在数据湖上，运行快速加工和分析）"><a href="#分析（可以直接在数据湖上，运行快速加工和分析）" class="headerlink" title="分析（可以直接在数据湖上，运行快速加工和分析）"></a>分析（可以直接在数据湖上，运行快速加工和分析）</h4><p>数据湖允许组织中的各种角色（如数据科学家、数据开发人员和业务分析师）通过各自选择的分析工具和框架来访问数据。这包括 Apache Hadoop、Presto 和 Apache Spark 等开源框架，以及数据仓库和商业智能供应商提供的商业产品。<strong>数据湖允许您运行分析，而无需将数据移至单独的分析系统</strong>。</p>
<h4 id="机器学习（在数据湖上进行机器学习）"><a href="#机器学习（在数据湖上进行机器学习）" class="headerlink" title="机器学习（在数据湖上进行机器学习）"></a>机器学习（在数据湖上进行机器学习）</h4><p>数据湖将允许组织生成不同类型的见解，包括报告历史数据以及进行机器学习（构建模型以预测可能的结果），并建议一系列规定的行动以实现最佳结果。</p>
<h1 id="Data-swamp"><a href="#Data-swamp" class="headerlink" title="Data swamp"></a>Data swamp</h1><p>搭建数据湖容易，但是让数据湖发挥价值是很难。如果只是一直往里面灌数据，而应用场景极少，没有输出或者极少输出，形成<strong>单向湖</strong>。</p>
<p>企业的业务是实时在变化的，这代表着沉积在数据湖中的数据定义、数据格式实时都在发生的转变，企业的大型数据湖对企业数据治理（Data Governance）提升了更高的要求。大部分使用数据湖的企业在数据真的需要使用的时候，往往因为数据湖中的数据质量太差而无法最终使用。数据湖，被企业当成一个大数据的垃圾桶，最终数据湖成为臭气熏天，存储在Hadoop当中的数据成为无人可以清理的数据沼泽.</p>
<p>数据湖架构的主要挑战是存储原始数据而不监督内容。对于使数据可用的数据湖，它需要有定义的机制来编目和保护数据。没有这些元素，就无法找到或信任数据，从而导致出现“<a href="https://www.gartner.com/newsroom/id/2809117" target="_blank" rel="noopener">数据沼泽</a>”。 满足更广泛受众的需求需要数据湖具有管理、语义一致性和访问控制。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7f3ec7c14c46b4e7328f74e44529ad21_720w.jpg" alt="img"></p>
<h1 id="Data-Lake-v-s-Data-Warehouse"><a href="#Data-Lake-v-s-Data-Warehouse" class="headerlink" title="Data Lake v.s. Data Warehouse"></a>Data Lake v.s. Data Warehouse</h1><p><a href="https://aws.amazon.com/cn/big-data/datalakes-and-analytics/what-is-a-data-lake/" target="_blank" rel="noopener">aws 什么是数据湖</a></p>
<p><a href="https://dbaplus.cn/news-141-2924-1.html" target="_blank" rel="noopener">数据仓库、数据湖 -&gt; 数据中台</a></p>
<p>数据仓库里的数据都满足特定的 schema，而数据湖则没有。仓库里的数据是简单整理过的，湖里的则是原始的（但不全是原始的）。仓库里的来源也都是规范的关系数据，而湖里则什么都有。</p>
<p>数据仓库是一个优化的数据库，用于分析来自事务系统和业务线应用程序的关系数据。事先定义数据结构和 Schema 以优化快速 SQL 查询，其中结果通常用于操作报告和分析。数据经过了清理、丰富和转换，因此可以充当用户可信任的“单一信息源”。</p>
<p>数据湖有所不同，因为它存储来自业务线应用程序的关系数据，以及来自移动应用程序、IoT 设备和社交媒体的非关系数据。捕获数据时，未定义数据结构或 Schema。这意味着您可以存储所有数据，而不需要精心设计也无需知道将来您可能需要哪些问题的答案。您可以对数据使用不同类型的分析（如 SQL 查询、大数据分析、全文搜索、实时分析和机器学习）来获得见解。</p>
<p>随着使用数据仓库的组织看到数据湖的优势，他们正在改进其仓库以包括数据湖，并启用各种查询功能、数据科学使用案例和用于发现新信息模型的高级功能。Gartner 将此演变称为“分析型数据管理解决方案”或“<a href="https://www.gartner.com/doc/3614317/magic-quadrant-data-management-solutions" target="_blank" rel="noopener">DMSA</a>”。</p>
<table>
<thead>
<tr>
<th style="text-align:center">特性</th>
<th style="text-align:center">数据仓库</th>
<th style="text-align:center">数据湖</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>数据</strong></td>
<td style="text-align:center">来自事务系统、运营数据库和业务线应用程序的<strong>关系数据</strong></td>
<td style="text-align:center">来自 IoT 设备、网站、移动应用程序、社交媒体和企业应用程序的<strong>非关系和关系数据</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Schema</strong></td>
<td style="text-align:center">设计在数据仓库实施之前（写入型 Schema）</td>
<td style="text-align:center">写入在分析时（读取型 Schema）</td>
</tr>
<tr>
<td style="text-align:center"><strong>性价比</strong></td>
<td style="text-align:center">更快查询结果会带来较高存储成本</td>
<td style="text-align:center">更快查询结果只需较低存储成本</td>
</tr>
<tr>
<td style="text-align:center"><strong>数据质量 </strong></td>
<td style="text-align:center">可作为重要事实依据的高度监管数据</td>
<td style="text-align:center">任何可以或无法进行监管的数据（例如原始数据）</td>
</tr>
<tr>
<td style="text-align:center"><strong>用户</strong></td>
<td style="text-align:center">业务分析师</td>
<td style="text-align:center">数据科学家、数据开发人员和业务分析师（使用监管数据）</td>
</tr>
<tr>
<td style="text-align:center"><strong>分析</strong></td>
<td style="text-align:center">批处理报告、BI 和可视化</td>
<td style="text-align:center">机器学习、预测分析、数据发现和分析</td>
</tr>
</tbody>
</table>
<p><img src="https://pic2.zhimg.com/80/v2-9eb32acb557b22322749f775c17b0079_720w.jpg" alt="img"></p>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>code license</title>
    <url>/2018/07/26/code-license/</url>
    <content><![CDATA[<p>license 是软件的授权许可。</p>
<p>对于开源软件来说，虽然别人可以用，但是用的时候希望别人遵循一些要求，比如，使用时必须标明原作者是谁、可以做怎样的修改、软件被用作不正规用途原作者是否要负责……这些其实就是一个协议。</p>
<p>对于作者来说，自己为开源代码写符合法律条规的繁冗的 license 太麻烦，所以就可以采用广为流传的开源协议（eg. MIT, CC…），在 license 文件中标明 “Licnse under the MIT license”</p>
<h1 id="快速选择"><a href="#快速选择" class="headerlink" title="快速选择"></a>快速选择</h1><p>详细的协议选择可以从 github <a href="https://choosealicense.com/" target="_blank" rel="noopener">choose license</a> 项目中选。下边列些常用的（参见 <a href="https://www.cnblogs.com/Wayou/p/how_to_choose_a_license.html" target="_blank" rel="noopener">如何为你的代码选择一个开源协议</a>）</p>
<h2 id="MIT-协议"><a href="#MIT-协议" class="headerlink" title="MIT 协议"></a>MIT 协议</h2><p>宽松但覆盖一般要点。此协议允许别人以任何方式使用你的代码同时署名原作者，但原作者不承担代码使用后的风险，当然也没有技术支持的义务。jQuery和Rails就是MIT协议</p>
<h2 id="apache-协议"><a href="#apache-协议" class="headerlink" title="apache 协议"></a>apache 协议</h2><p>作品涉及到专利，可以考虑这个。也比较宽松，但考虑了专利，简单指明了作品归属者对用户专利上的一些授权（我的理解是软件作品中含有专利，但它授权你可以免费使用）。Apache服务器，SVN还有NuGet等是使用的Apache协议。</p>
<h2 id="GPL"><a href="#GPL" class="headerlink" title="GPL"></a>GPL</h2><p>对作品的传播和修改有约束的，可以使用这个。GPL（<a href="http://choosealicense.com/licenses/gpl-v2" target="_blank" rel="noopener">V2</a>或<a href="http://choosealicense.com/licenses/gpl-v3" target="_blank" rel="noopener">V3</a>）是一种版本自由的协议（可以参照copy right来理解，后者是版本保留，那copyleft便是版权自由，或者无版权，但无版权不代表你可以不遵守软件中声明的协议）。此协议要求代码分发者或者以此代码为基础开发出来的衍生作品需要以同样的协议来发布。此协议的版本3与版本2相近，只是多3中加了条对于不支持修改后代码运行的硬件的限制（没太明白此句话的内涵）。</p>
]]></content>
  </entry>
  <entry>
    <title>clean mac other storage</title>
    <url>/2021/02/24/clean-mac-other-storage/</url>
    <content><![CDATA[<p>other storage 主要是存的系统的一些缓存、日志等数据。有时会占特别大空间，可以按下列步骤清理</p>
<h2 id="1-暂时关闭-SIP，以能查看和删除系统文件（解决-not-permitted-问题）"><a href="#1-暂时关闭-SIP，以能查看和删除系统文件（解决-not-permitted-问题）" class="headerlink" title="1. 暂时关闭 SIP，以能查看和删除系统文件（解决 not permitted 问题）"></a>1. 暂时关闭 SIP，以能查看和删除系统文件（解决 not permitted 问题）</h2><ol>
<li>以 recover mode 重启电脑：启动时，按 command + R 即可</li>
<li>选择 Utilities -&gt; Terminal</li>
<li>在 Terminal 中输入 <code>csrutil disable</code> 关闭 SIP</li>
<li>重启电脑</li>
</ol>
<p>在完成 clean 后，应该重复 1、2，并在 terminal 中输入 <code>csrutil enable</code> 来启动 SIP</p>
<p>重启电脑后，可以通过 <code>csrutil status</code> 来查看 SIP 服务是否启动（清理完成后应该启动）</p>
<h2 id="2-按-size-查找大文件"><a href="#2-按-size-查找大文件" class="headerlink" title="2. 按 size 查找大文件"></a>2. 按 size 查找大文件</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /</span><br><span class="line">$ sudo du -sh  -- *| sort -hr</span><br></pre></td></tr></table></figure>
<h2 id="3-常见的大文件"><a href="#3-常见的大文件" class="headerlink" title="3.  常见的大文件"></a>3.  常见的大文件</h2><h3 id="Libraray-Caches-和-Library-Caches"><a href="#Libraray-Caches-和-Library-Caches" class="headerlink" title="~/Libraray/Caches 和  /Library/Caches"></a>~/Libraray/Caches 和  /Library/Caches</h3><p><code>~/Libraray</code> 和  <code>/Library</code> 下的 <code>Caches</code> 和 <code>logs</code> 等都是可以安全删除的。可以查看一下大小，把自己不用的 cache 删掉。</p>
<p>当然也可以查看 <code>Library</code> 下的所有大文件，确认是否可以删除</p>
<h3 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h3><p>Docker 的 images、volumes 等可能占很大空间，可以查看到 <code>~/Library/Containers/com.docker.docker</code> 文件夹的大小</p>
<p><a href="https://docs.docker.com/docker-for-mac/space/" target="_blank" rel="noopener">docker space for mac</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/Library/Containers</span><br><span class="line">$ sudo du -sh  -- *| sort -hr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 docker 的系统占用，这是清理后了，占用很小</span></span><br><span class="line">$ docker system df</span><br><span class="line">TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE</span><br><span class="line">Images              1                   1                   100.8kB             0B (0%)</span><br><span class="line">Containers          1                   0                   0B                  0B</span><br><span class="line">Local Volumes       0                   0                   0B                  0B</span><br><span class="line">Build Cache         0                   0                   0B                  0B</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理磁盘，删除关闭的容器、无用的数据卷和网络，以及 dangling 镜像(即无 tag 的镜像)</span></span><br><span class="line">$ docker system prune</span><br><span class="line"><span class="comment"># 清理得更加彻底，可以将没有容器使用 Docker 镜像都删掉</span></span><br><span class="line">$ docker system prune -a</span><br><span class="line"><span class="comment"># 如果需要同时删除未被任何容器引用的数据卷需要显式的指定 --volumns 参数</span></span><br><span class="line">$ docker system prune --all --force --volumes</span><br><span class="line"><span class="comment"># 删除所有 dangling 数据卷(即无用的 volume)</span></span><br><span class="line">$ docker volume rm $(docker volume ls -qf dangling=<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有时删完后，需要一段时间 reclaim space，可以使用以下命令，手动 trigger relamation</span></span><br><span class="line">$ docker run --privileged --pid=host docker/desktop-reclaim-space</span><br></pre></td></tr></table></figure>
<h3 id="Library-Updates"><a href="#Library-Updates" class="headerlink" title="/Library/Updates"></a>/Library/Updates</h3><p>这里可能会有很多文件，都可以删。这里正常应该在执行 App Store 里的 Update 时，才会有文件，但不知道为啥，即使 App Store 没有 Update 也会有。</p>
<p>如果这里有大文件，那么：</p>
<ol>
<li>先尝试去执行 App Store 里的 Update</li>
<li>如果还有文件，可以删除文件夹里的所有文件（建议不要删那几个 <code>.plist</code> 文件），不要删除 <code>/Library/Updates</code> 文件夹本身，删除里边的内容</li>
</ol>
<h3 id="private-var-tmp-WiFiDiagnostics"><a href="#private-var-tmp-WiFiDiagnostics" class="headerlink" title="/private/var/tmp/WiFiDiagnostics*"></a>/private/var/tmp/WiFiDiagnostics*</h3><p><code>/private/var/tmp/</code> 里的文件删除的时候要小心些。</p>
<p><code>WiFiDiagnostics</code> 是 wifi log，这些文件<strong>可以安全删除</strong></p>
<p>但它可以用  <code>command+control+option+shift+w</code> 或 <code>command+control+option+shift+&gt;</code> 触发。如果你正好使用 karabiner 并且 remap 了 <code>command+control+option+shift</code>，在使用过程中可能正好和 +w 或 +&gt; 冲突了，那么每次都会启动 logging。所以需要修改配置。</p>
<ol>
<li>karabiner -&gt; Misc -&gt; Open config folder</li>
<li>open karabiner.json，在其中加入下面的配置</li>
</ol>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"rules": [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"description"</span>: <span class="string">"Disabling command+control+option+shift+w. This triggers wifi logging."</span>,</span><br><span class="line">        <span class="attr">"manipulators"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"from"</span>: &#123;</span><br><span class="line">                    <span class="attr">"key_code"</span>: <span class="string">"w"</span>,</span><br><span class="line">                    <span class="attr">"modifiers"</span>: &#123;</span><br><span class="line">                        <span class="attr">"mandatory"</span>: [</span><br><span class="line">                            <span class="string">"command"</span>,</span><br><span class="line">                            <span class="string">"control"</span>,</span><br><span class="line">                            <span class="string">"option"</span>,</span><br><span class="line">                            <span class="string">"shift"</span></span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"to"</span>: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="attr">"key_code"</span>: <span class="string">"escape"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"basic"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"description"</span>: <span class="string">"Disabling command+control+option+shift+&gt;. This triggers wifi logging also."</span>,</span><br><span class="line">        <span class="attr">"manipulators"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"from"</span>: &#123;</span><br><span class="line">                    <span class="attr">"key_code"</span>: <span class="string">"&gt;"</span>,</span><br><span class="line">                    <span class="attr">"modifiers"</span>: &#123;</span><br><span class="line">                        <span class="attr">"mandatory"</span>: [</span><br><span class="line">                            <span class="string">"command"</span>,</span><br><span class="line">                            <span class="string">"control"</span>,</span><br><span class="line">                            <span class="string">"option"</span>,</span><br><span class="line">                            <span class="string">"shift"</span></span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"to"</span>: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="attr">"key_code"</span>: <span class="string">"escape"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"basic"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">"description"</span>: <span class="string">"Change caps_lock key to command+control+option+shift. (Post escape key when pressed alone)"</span>,</span><br><span class="line">        <span class="attr">"manipulators"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"from"</span>: &#123;</span><br><span class="line">                    <span class="attr">"key_code"</span>: <span class="string">"caps_lock"</span>,</span><br><span class="line">                    <span class="attr">"modifiers"</span>: &#123;</span><br><span class="line">                        <span class="attr">"optional"</span>: [</span><br><span class="line">                            <span class="string">"any"</span></span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"to"</span>: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="attr">"key_code"</span>: <span class="string">"left_shift"</span>,</span><br><span class="line">                        <span class="attr">"modifiers"</span>: [</span><br><span class="line">                            <span class="string">"left_command"</span>,</span><br><span class="line">                            <span class="string">"left_control"</span>,</span><br><span class="line">                            <span class="string">"left_option"</span></span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">                <span class="attr">"to_if_alone"</span>: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="attr">"key_code"</span>: <span class="string">"escape"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"basic"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>/private/var/tmp</code> 文件夹下可能还有很多 <code>sysdiagnose</code> 文件，这个应该是可以删，是系统诊断结果。但不太确定，我没删。</p>
<p>Sysdiagnose 可以通过 <code>command+control+option+shift+.</code> 来启动一次，所以也要注意</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>fantastic tools</title>
    <url>/2022/10/22/fantastic-tools/</url>
    <content><![CDATA[<h1 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h1><p>收集一些在设计时可能会用到的比较好的网站</p>
<ul>
<li><a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">icons from the fontawesome</a>: 各种各样的图标，很漂亮</li>
<li><a href="https://ant.design/docs/spec/introduce-cn" target="_blank" rel="noopener">ant design</a>：里边有 UI 设计价值观及图标资源等，还有前端组件库</li>
</ul>
<h2 id="前端组件库"><a href="#前端组件库" class="headerlink" title="前端组件库"></a>前端组件库</h2><p><a href="https://zhuanlan.zhihu.com/p/24650288" target="_blank" rel="noopener">组件库大合集</a></p>
<p><a href="https://github.com/JingwenTian/awesome-frontend" target="_blank" rel="noopener">组件库大合集2</a></p>
<h1 id="mac"><a href="#mac" class="headerlink" title="mac"></a>mac</h1><p><a href="https://insights.thoughtworks.cn/ocds-guide-to-setting-up-mac/" target="_blank" rel="noopener">强迫症的Mac设置指南</a></p>
<h2 id="terminal"><a href="#terminal" class="headerlink" title="terminal"></a>terminal</h2><p><a href="10 Must know terminal commands and tips for productivity">10 Must know terminal commands and tips for productivity</a> 介绍了一些，简单罗列下：</p>
<ol>
<li><code>iterm2</code>：是一种 terminal，将其作为默认 terminal，可以方便的分屏等</li>
<li><code>oh my zsh</code>：管理 zsh 配置的，使用 <code>~/.zshrc</code>，利用 <code>source ~/.zshrc</code> 可以是配置立马生效<ol>
<li><a href="https://hustyichi.github.io/2018/09/19/oh-my-zsh/" target="_blank" rel="noopener">oh my zsh 插件</a></li>
</ol>
</li>
<li><code>cat</code>：不打开文件的情况下查看内容</li>
<li><code>imgcat</code>：同上，不过查看的是图片</li>
<li><code>less [filename]</code>：同 <code>cat</code>，不过如果长文件，可以用这个，仅显示一部分</li>
<li><code>pbcopy &lt; [filename]</code>：复制文件内容到 clipboard</li>
<li><code>touch</code>：创建各种各样的文件，可以同时创建多个。eg. <code>touch index.html readme.md index.php</code></li>
<li><code>lsof -i :[port]</code>：查看端口占用</li>
<li><code>&amp;&amp;</code>：实现 command chaining，即同时写多个命令，依次执行。eg. <code>npm i &amp;&amp; npm start</code></li>
<li><code>open .</code>：打开当前目录</li>
</ol>
<h3 id="iterm2-oh-my-zsh-theme-配置"><a href="#iterm2-oh-my-zsh-theme-配置" class="headerlink" title="iterm2 + oh my zsh + theme 配置"></a>iterm2 + oh my zsh + theme 配置</h3><ol>
<li><a href="https://www.iterm2.com/" target="_blank" rel="noopener">download iterm2</a>，解压安装</li>
<li>设置 iterm2 为默认 terminal：iterm2 -&gt; make iterm2 default Term</li>
<li>安装 <a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank" rel="noopener">oh my zsh</a>: <ul>
<li><code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</code></li>
</ul>
</li>
<li>配置主题和颜色<ol start="4">
<li>主题，我选用默认的 robbyrussell。其他下载主题然后在 <code>~/.zshrc</code> 中配置 <code>ZSH_THEME</code></li>
<li>颜色，下载 <a href="https://github.com/mbadolato/iTerm2-Color-Schemes" target="_blank" rel="noopener">iterm2 color schemes</a>，然后在 iterm2 perferences -&gt; profiles -&gt; colors -&gt; color presets -&gt; import -&gt; 从前边下载的库中选择自己喜欢的 scheme</li>
</ol>
</li>
<li>配置高亮<ul>
<li><code>brew install zsh-syntax-highlighting</code></li>
<li>在 <code>~/.zshrc</code> 中添加：<code>source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh</code></li>
<li>刷新配置使其生效：<code>source ~/.zshrc</code></li>
</ul>
</li>
<li>显示快捷键配置<ul>
<li>preferences -&gt; keys -&gt; hotKey -&gt; Show/Hide all… -&gt; 设置为 <code>⌘.</code></li>
</ul>
</li>
<li>常用的配置：<ol>
<li>配置新开重开使用之前的目录：preferences -&gt; profiles -&gt; general -&gt; working directory -&gt; reuse previous session’s directory</li>
<li>配置快速切换 iterm 的快捷键：preferences -&gt; keys -&gt; hot key -&gt; show/hide all windows with a system-wide hotkey</li>
<li>配置窗口透明度和默认启动大小：preferences -&gt; profiles -&gt; window</li>
<li>配置 command line move-by-word, delete-by-word: preferences -&gt; profiles -&gt; keys (<a href="https://apple.stackexchange.com/questions/154292/iterm-going-one-word-backwards-and-forwards" target="_blank" rel="noopener">stackoverflow</a>)<ol>
<li>向左移动 by word (⌥b)，向右移动 by word (⌥f)，删除右边 by word (⌥d)<ol>
<li>Under Profile Shortcut Keys, click the + sign.</li>
<li>Type your key shortcut (option-b, option-f, option-d, option-left, etc.)</li>
<li>For Action, choose Send Escape Sequence.</li>
<li>Write b, d or f in the input field.</li>
</ol>
</li>
<li>删除左边 by word (⌥⌫)<ol>
<li>preferences -&gt; profiles -&gt; keys -&gt; left option (⌥) key : Esc+</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="iterm2-常用快捷键"><a href="#iterm2-常用快捷键" class="headerlink" title="iterm2 常用快捷键"></a>iterm2 常用快捷键</h3><ul>
<li><code>⌘d</code>：横分屏</li>
<li><code>⌘⇧d</code>：竖分屏</li>
<li><code>⌘⌥ + direction</code>：navigate between panes</li>
<li><code>⌘.</code>：show/hide iterm2</li>
<li><code>⌘⇧↩︎</code>: 最大化当前 pane / 回复当前 pane</li>
</ul>
<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><ul>
<li><a href="https://github.com/newren/git-filter-repo" target="_blank" rel="noopener">git-fitler-repo</a></li>
<li>常用的 git config 配置</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在 ~/.zshrc 中添加 alias</span></span><br><span class="line">$ vi ~/.zshrc</span><br><span class="line"><span class="built_in">alias</span> g=<span class="string">"/usr/bin/git"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 .gitconfig 中添加 alias 配置</span></span><br><span class="line">$ vi ~/.gitconfig</span><br><span class="line">[user]</span><br><span class="line">        name = xxx</span><br><span class="line">        email = xxx@some.com</span><br><span class="line"></span><br><span class="line">[<span class="built_in">alias</span>]</span><br><span class="line">        a = add</span><br><span class="line">        aa = add .</span><br><span class="line">        b = branch</span><br><span class="line">        c = commit</span><br><span class="line">        cm = commit -m</span><br><span class="line">        ca = commit --amend</span><br><span class="line">        d = diff</span><br><span class="line">        l = <span class="built_in">log</span> --graph --pretty=format:<span class="string">'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr)%Creset | %C(bold)%an'</span> --abbrev-commit --date=relative</span><br><span class="line">        o = checkout</span><br><span class="line">        pl = pull</span><br><span class="line">        pb = pull --rebase</span><br><span class="line">        ps = push</span><br><span class="line">        r = reset</span><br><span class="line">        st = status</span><br></pre></td></tr></table></figure>
<h1 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h1><ul>
<li><a href="http://www.shuwu.mobi/" target="_blank" rel="noopener">我的小书屋</a></li>
<li><a href="http://readfree.me/" target="_blank" rel="noopener">readfree</a>：可以直接推送到 kindle，很方便</li>
<li><a href="http://bulaoge.cn/" target="_blank" rel="noopener">不老歌</a>：写日记</li>
<li><a href="https://www.baoshuu.com/" target="_blank" rel="noopener">宝书网</a></li>
</ul>
<h1 id="music"><a href="#music" class="headerlink" title="music"></a>music</h1><ul>
<li><a href="https://www.sq688.com" target="_blank" rel="noopener">超高无损音乐下载</a></li>
</ul>
<h1 id="json"><a href="#json" class="headerlink" title="json"></a>json</h1><ul>
<li><a href="https://jsonformatter.org/xml-viewer" target="_blank" rel="noopener">json formatter</a>: json、xml 等都支持，可以格式化，有 json tree，统计了节点数</li>
<li><a href="https://jsoneditoronline.org/" target="_blank" rel="noopener">json editor online</a>: 界面简单，有 json tree，统计了 tree 节点数</li>
<li><a href="http://jsonviewer.stack.hu/" target="_blank" rel="noopener">json viewer</a>: 功能精简，格式化 json，没有 tree</li>
<li><a href="https://codebeautify.org/yaml-to-json-xml-csv" target="_blank" rel="noopener">yaml to json</a>: yaml 和 json 之间的转化</li>
</ul>
<h1 id="Editing"><a href="#Editing" class="headerlink" title="Editing"></a>Editing</h1><ul>
<li>typora: markdown tool</li>
<li><a href="https://github.com/Clipy/Clipy" target="_blank" rel="noopener">clipy</a>: free tool for pasting</li>
</ul>
<h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><ul>
<li><a href="https://karabiner-elements.pqrs.org/" target="_blank" rel="noopener">karabiner</a>: 定义快捷键。可以将 caps 键重新映射，在自定义快捷键时避免冲突</li>
<li><a href="https://github.com/fikovnik/ShiftIt" target="_blank" rel="noopener">shiftit</a>: 窗口 size &amp; location 定义。用 brew 安装 <code>brew cask install shiftit</code></li>
<li>Eudic: 好用的词典，可以有一个简单悬浮窗口</li>
<li>Alfred: easy used spotlight substitute to search in Mac</li>
<li><a href="https://mediaatelier.com/CheatSheet/?lang=en" target="_blank" rel="noopener">cheatsheet</a>: 显示当前应用的快捷键。Just hold the ⌘-Key a bit longer to get a list of all active short cuts of the current application.</li>
</ul>
<h1 id="给图片加水印"><a href="#给图片加水印" class="headerlink" title="给图片加水印"></a>给图片加水印</h1><ul>
<li>美图秀秀：批量添加水印，编辑水印位置、大小、透明度</li>
<li><a href="https://jingyan.baidu.com/article/0eb457e555170643f1a90592.html" target="_blank" rel="noopener">Photoshop 批量添加水印</a></li>
<li>免费软件：<ul>
<li><a href="https://www.xnview.com/en/xnconvert/#downloads" target="_blank" rel="noopener">XnConvert</a>：<a href="https://uiiiuiii.com/software/202192.html" target="_blank" rel="noopener">使用 XnConvert 批量添加水印</a></li>
<li><a href="https://imagemagick.org/index.php" target="_blank" rel="noopener">imagemagick</a>: 基于命令行的图形处理库（现有的图像处理软件大多都用到此库）</li>
</ul>
</li>
<li><a href="https://sspai.com/post/53079" target="_blank" rel="noopener">6 个小工具，打造图片批处理工作流</a></li>
</ul>
<h2 id="ImageMagick"><a href="#ImageMagick" class="headerlink" title="ImageMagick"></a>ImageMagick</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ brew install imagemagick</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /images</span><br><span class="line"><span class="comment"># 获取图片基本信息</span></span><br><span class="line">$ identify a.jpg</span><br><span class="line"><span class="comment"># 转换图片格式</span></span><br><span class="line">$ magick a.jpg a.png</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量为图片添加水印</span></span><br><span class="line">  1 <span class="comment">#!/bin/bash</span></span><br><span class="line">  2</span><br><span class="line">  3 dir=<span class="variable">$1</span></span><br><span class="line">  4 mark=<span class="variable">$2</span></span><br><span class="line">  5</span><br><span class="line">  6 <span class="built_in">echo</span> <span class="string">"the images dir to process: <span class="variable">$dir</span>"</span></span><br><span class="line">  7 <span class="built_in">echo</span> <span class="string">"the mark location: <span class="variable">$mark</span>"</span></span><br><span class="line">  8</span><br><span class="line">  9 <span class="built_in">shopt</span> -s nullglob <span class="comment"># 如果不添加这个，当目录中没有 .png 类型的文件时，他会产生 "$dir"/*.png，那么后边就会报错</span></span><br><span class="line"> 10 <span class="keyword">for</span> each <span class="keyword">in</span> <span class="string">"<span class="variable">$dir</span>"</span>/&#123;*.jpg,*.jpeg,*.png&#125;</span><br><span class="line"> 11 <span class="keyword">do</span></span><br><span class="line"> 12         <span class="built_in">echo</span> <span class="string">"each is: <span class="variable">$each</span>"</span></span><br><span class="line"> 13         convert <span class="variable">$each</span> <span class="variable">$mark</span> -gravity southeast -geometry +5+20 -composite <span class="variable">$each</span></span><br><span class="line"> 14         convert <span class="variable">$each</span> <span class="variable">$mark</span> -gravity center -composite <span class="variable">$each</span></span><br><span class="line"> 15         convert <span class="variable">$each</span> <span class="variable">$mark</span> -gravity northwest -geometry +5+20 -composite <span class="variable">$each</span></span><br><span class="line"> 16         <span class="built_in">echo</span> <span class="string">"<span class="variable">$each</span>: done"</span></span><br><span class="line"> 17 <span class="keyword">done</span></span><br><span class="line"> 18 <span class="built_in">shopt</span> -u nullglob</span><br><span class="line"> 19 <span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure>
<h1 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h1><h2 id="terminal-1"><a href="#terminal-1" class="headerlink" title="terminal"></a>terminal</h2><p><a href="https://p3terx.com/archives/the-strongest-terminal-solution-under-windows-10.html" target="_blank" rel="noopener">打造 Windows 10 下最强终端方案：WSL + Terminus + Oh My Zsh + The Fuck</a></p>
<blockquote>
<p>注意这里装的是 wsl1，可以改成 wsl2，参见<a href="https://docs.microsoft.com/en-us/windows/wsl/install" target="_blank" rel="noopener">install wsl</a></p>
</blockquote>
<h2 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h2><p>mac 下有免费的 clipy 来实现 copy history，windows 下有 <a href="https://copyq.readthedocs.io/en/latest/basic-usage.html" target="_blank" rel="noopener">copyq</a>（也可以用于 mac）</p>
<p>copyq 在它的窗口里可以配置各种命令的快捷键（全局或程序内），包括显示和隐藏 copyq 窗口的</p>
<h2 id="search"><a href="#search" class="headerlink" title="search"></a>search</h2><p>mac 下有 alfred 来搜索文件，windows 有一些替代的：</p>
<ol>
<li>listary：有免费版</li>
<li>wox：开源</li>
</ol>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>duplicate rows in postgresql</title>
    <url>/2018/11/06/duplicate-rows-in-postgresql/</url>
    <content><![CDATA[<p>参见 <a href="https://blog.theodo.fr/2018/01/search-destroy-duplicate-rows-postgresql/" target="_blank" rel="noopener">Search and destroy duplicate rows in PostgreSQL</a></p>
<h1 id="Find-duplicates"><a href="#Find-duplicates" class="headerlink" title="Find duplicates"></a>Find duplicates</h1><h2 id="using-group"><a href="#using-group" class="headerlink" title="using group"></a>using group</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  firstname,</span><br><span class="line">  lastname,</span><br><span class="line">  <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> people</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  firstname,</span><br><span class="line">  lastname</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">count</span>(*) &gt; <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h2 id="using-partition"><a href="#using-partition" class="headerlink" title="using partition"></a>using partition</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> *, <span class="keyword">count</span>(*)</span><br><span class="line">  <span class="keyword">OVER</span></span><br><span class="line">    (<span class="keyword">PARTITION</span> <span class="keyword">BY</span></span><br><span class="line">      firstname,</span><br><span class="line">      lastname</span><br><span class="line">    ) <span class="keyword">AS</span> <span class="keyword">count</span></span><br><span class="line">  <span class="keyword">FROM</span> people) tableWithCount</span><br><span class="line">  <span class="keyword">WHERE</span> tableWithCount.count &gt; <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h2 id="Using-not-strict-distinct"><a href="#Using-not-strict-distinct" class="headerlink" title="Using not strict distinct"></a>Using not strict distinct</h2><p>利用 not strict distinct <code>DISTINCT ON</code> 找到唯一的那些条，剩余的就是重复的，可以修改或删除</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> people <span class="keyword">WHERE</span> people.id <span class="keyword">NOT</span> <span class="keyword">IN</span> </span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> <span class="keyword">ON</span> (firstname, lastname) *</span><br><span class="line">  <span class="keyword">FROM</span> people));</span><br><span class="line">  </span><br><span class="line">// more readable code</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">unique</span> <span class="keyword">AS</span></span><br><span class="line">    (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> <span class="keyword">ON</span> (firstname, lastname) * <span class="keyword">FROM</span> people)</span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> people <span class="keyword">WHERE</span> people.id <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> <span class="keyword">id</span> <span class="keyword">FROM</span> <span class="keyword">unique</span>);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>sql</tag>
        <tag>postgres</tag>
      </tags>
  </entry>
  <entry>
    <title>构建</title>
    <url>/2018/08/16/gradle-build/</url>
    <content><![CDATA[<h1 id="Problems"><a href="#Problems" class="headerlink" title="Problems?"></a>Problems?</h1><ol>
<li>manifest 是干什么用的？</li>
<li>代码运行时，如何找到 dependency 的包</li>
<li>java -jar 时，classpath 指定？</li>
</ol>
<h1 id="classpath"><a href="#classpath" class="headerlink" title="classpath"></a>classpath</h1><p>classpath 指定的是 java 类所在的目录（包括当前项目的类、依赖的类等）。应该是当打 jar 包的时候，默认会加上当前目录(.)到 classpath，这样就包含了 jar 内部的类？</p>
<h1 id="Thin-jar"><a href="#Thin-jar" class="headerlink" title="Thin jar"></a>Thin jar</h1><p><a href="https://github.com/cuzfrog/gradle-lean" target="_blank" rel="noopener">gradle lean</a></p>
<p>This plugin depends on <code>JavaPlugin</code> and <code>ApplicationPlugin</code>.</p>
<ul>
<li>for <code>installDist</code>, jars under <code>install/$PROJECT_NAME$/lib/</code></li>
<li>for <code>distZip</code>, jars under <code>/lib/</code> inside package</li>
</ul>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">plugins &#123;</span><br><span class="line">    id <span class="string">'java'</span></span><br><span class="line">    <span class="comment">// Apply the application plugin to add support for building a CLI application.</span></span><br><span class="line">    id <span class="string">'application'</span></span><br><span class="line"></span><br><span class="line">    id <span class="string">'scala'</span></span><br><span class="line"></span><br><span class="line">    id <span class="string">'com.github.maiflai.scalatest'</span> version <span class="string">'0.26'</span></span><br><span class="line"></span><br><span class="line">    id <span class="string">"com.github.gradle-lean"</span> version <span class="string">"0.1.2"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="implementation-vs-compile-vs-api"><a href="#implementation-vs-compile-vs-api" class="headerlink" title="implementation vs compile vs api"></a>implementation vs compile vs api</h1><p><a href="https://stackoverflow.com/questions/44493378/whats-the-difference-between-implementation-and-compile-in-gradle" target="_blank" rel="noopener">stackoverflow</a></p>
<h1 id="Dependency-Conflict"><a href="#Dependency-Conflict" class="headerlink" title="Dependency Conflict"></a>Dependency Conflict</h1><h2 id="force-some-edition"><a href="#force-some-edition" class="headerlink" title="force some edition"></a>force some edition</h2><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">configurations.all &#123;</span><br><span class="line">    resolutionStrategy &#123;</span><br><span class="line">        force <span class="string">'com.fasterxml.jackson.module:jackson-module-scala_2.11:2.10.3'</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>gradle</tag>
        <tag>java</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title>gradle test performance</title>
    <url>/2018/10/12/gradle-test-performance/</url>
    <content><![CDATA[<p><a href="https://docs.gradle.org/current/dsl/org.gradle.api.tasks.testing.Test.html#org.gradle.api.tasks.testing.Test:forkEvery" target="_blank" rel="noopener">gradle test configurations</a></p>
<p><a href="https://discuss.gradle.org/t/parallel-test-execution-with-gradle-maxparallelforks-property/15136" target="_blank" rel="noopener">one sample config</a></p>
<p><a href="https://guides.gradle.org/performance/" target="_blank" rel="noopener">ways to improve performance of gradle build</a></p>
<p>common used properties:</p>
<ul>
<li><code>jvmArgs</code>: jvm 参数。通常会配置堆栈大小，保证测试对内存的要求。<ul>
<li><code>&#39;-Xms128m&#39;, &#39;-Xmx1024m&#39;, &#39;-XX:MaxMetaspaceSize=128m&#39;</code>。<code>-Xms</code> 是初始堆大小，<code>-Xmx</code> 是最大堆大小，<code>-XX:MaxMetaspaceSize</code> 是 class metadata 可占用的最大本地内存（默认是 unlimited）。具体 jvm 参数参考 <a href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html" target="_blank" rel="noopener">java doc</a>. </li>
</ul>
</li>
<li><code>forkEvery</code>: 每个 test process 里跑的 test classes 的最大个数。当次数达到限制后，会自动重启。这定义了一个测试线程什么时候回重启，与并发无关。默认是 0，即无最大限制，就是可以一直跑</li>
<li><code>maxParalleForks</code>: 能并发跑的最大 test processes 数目</li>
<li><code>systemProperty</code>: 系统属性</li>
<li><code>environment</code>：系统环境变量</li>
<li><code>include</code>: 具体执行的测试。可以通过这个配置不同的测试级别（单元测试、集成测试、functional 测试……）</li>
</ul>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>guice</title>
    <url>/2018/08/17/guice/</url>
    <content><![CDATA[<p><a href="https://gist.github.com/virasak/3798194" target="_blank" rel="noopener">TEST with GUICE</a></p>
]]></content>
  </entry>
  <entry>
    <title>hadoop</title>
    <url>/2019/01/07/hadoop/</url>
    <content><![CDATA[<p>Hadoop is a framework of distributed storage &amp; computing.</p>
<ul>
<li><strong>distributed storage</strong>: hadoop use <strong>HDFS</strong> to save large amount of data in cluster.</li>
<li><strong>distributed computing</strong>: hadoop use <strong>map-reduce</strong> framework to conduct fast data analysis (query &amp; writing) over data in HDFS.</li>
<li><strong>resource manager &amp; job schedular</strong>: hadoop use <strong>yarn</strong> to manage/allocate cluster resources (memory, cpu, etc.) and to schedule  and moniter job executing.</li>
</ul>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="cluster-architecture"><a href="#cluster-architecture" class="headerlink" title="cluster architecture"></a>cluster architecture</h2><p><img src="/images/hadoop-20201026164010368.png" alt="image-20201026164010368"></p>
<p><img src="/images/hadoop-20201026164402103.png" alt="image-20201026164402103"></p>
<h2 id="request-processing"><a href="#request-processing" class="headerlink" title="request processing"></a>request processing</h2><p><img src="/images/hadoop-20201026164147039.png" alt="image-20201026164147039"></p>
<h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><p>Use <strong>rack aware</strong> so that your replicas will be saved into different racks, which can solve the rack failure issue.</p>
<p>Each data node will send heartbeat and block report to the namenode. Thus when data node fails, the name node knows it and will re-replicated to 3.</p>
<p><img src="/images/hadoop-20201026164613209.png" alt="image-20201026164613209"></p>
<h2 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h2><p><strong>High Availability: Percentage of Uptime of the system</strong>. Fault Tolerance, on the other hand, mainly focus on the data loss / system un-recovered damage tolerance. For example, a name-node failure can be processed by reboot from the aspect of fault-tolerance, while there must be a <strong>quick</strong> working solution from the aspect of high availability.</p>
<h3 id="Name-Node-Failure"><a href="#Name-Node-Failure" class="headerlink" title="Name Node Failure"></a>Name Node Failure</h3><p>For a name node failure, we want to switch to a standby name node with all the informations quickly. How?</p>
<p>A name node saves the file namespaces in memory, besides, it also saved editlog for each change into the disk. A name node failure will lose the in-memory fsImage, but we can reproduce the fsImage from the editlogs</p>
<p><img src="/images/hadoop-20201026165814244.png" alt="y"></p>
<p>A common solution is to use QJM to save the editlogs. And the standby name node will read from the editlogs to rebuild the fsImage. Besides, there’s  two failover controllers on each name node and a zookeeper. ZooKeeper keeps a lock, and both name nodes are requesting the lock. When the active name node fails, it lost the lock, and the standby nn will acquire the lock.</p>
<p><img src="/images/hadoop-20201026170521244.png" alt="image-20201026170521244"></p>
<h3 id="Name-Node-Reboot"><a href="#Name-Node-Reboot" class="headerlink" title="Name Node Reboot"></a>Name Node Reboot</h3><p>What if you just want to reboot the name node, and since the fsImage is in memory, it will be gone at once and it takes a long time to rebuild from the editlogs?</p>
<p>The main issue here is that the fsImage is in memory. Thus to reboot quickly, we need to save the fsImage into disk. The secondary name node is for this. It periodically merge the old fsImage with the editlogs, and replace the old fsImage in the disk, and then truncate the logs.</p>
<p>Secondary Name Node is not necessary. If needed, you can build it on the standby nn.</p>
<p><img src="/images/hadoop-20201026171217319.png" alt="image-20201026171217319"></p>
<h3 id="install-hadoop-on-mac"><a href="#install-hadoop-on-mac" class="headerlink" title="install hadoop on mac"></a><a href="http://www.cnblogs.com/micrari/p/5716851.html" target="_blank" rel="noopener">install hadoop on mac</a></h3><p>see <a href="http://kontext.tech/docs/DataAndBusinessIntelligence/p/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn" target="_blank" rel="noopener">default ports used by hadoop services 3.1.0</a></p>
<blockquote>
<p>when config password-free login by ssh, it may only work when generate key into id_rsa/id_dsa. The other user defind key file name won’t work.</p>
</blockquote>
<ul>
<li><strong>access hdfs</strong></li>
</ul>
<p><a href="https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2_1.html" target="_blank" rel="noopener">hdfs default ports</a> are changed. see <a href="https://issues.apache.org/jira/browse/HDFS-9427" target="_blank" rel="noopener">hdfs issue</a>, or check the <code>dfs.namenode.http-address</code> property in <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">hdfs-default.xml</a> for the newest setting.</p>
<blockquote>
<p>Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820<br>Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868<br>Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 – &gt;9864</p>
</blockquote>
<p>When running the example, it seems that jar can only search files. Thus you need to ensure there’s no sub-dirs in search dir.</p>
<ul>
<li><strong>access yarn</strong></li>
</ul>
<p>Access resource manager through <code>localhost:8088</code>. Or check the property <code>yarn.resourcemanager.webapp.address</code> in  <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a> for the newest configuration</p>
<h4 id="start-hadoop-locally"><a href="#start-hadoop-locally" class="headerlink" title="start hadoop locally"></a>start hadoop locally</h4><p><a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener">map reduce</a> is a framework to write applications which process vast amounts of data in-parallel on large clusters. </p>
<p>A map-reduce job usually splits the input data into independent chunks, and <strong>map</strong> in a parallel manner. Then the frameworks sorts the output and then <strong>reduce</strong> to the integrate output.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># initialize the namenode</span></span><br><span class="line">$ hdfs namenode -format</span><br><span class="line"><span class="comment"># start namenode and datanode daemon (access namenode at localhost:9870)</span></span><br><span class="line">$ start-dfs.sh</span><br><span class="line"><span class="comment"># start ResourceManager &amp; NodeManager daemon (access yarn at localhost:8088)</span></span><br><span class="line">$ start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># stop namenode and datanode daemon</span></span><br><span class="line">$ stop-dfs.sh</span><br><span class="line"><span class="comment"># stop ResourceManager &amp; NodeManager daemon</span></span><br><span class="line">$ stop-yarn.sh</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note:</p>
<p>The <code>hdfs namenode -format</code> command must be executed everytime you restarted your computer. And it’s initialized again. Need to figure out other ways to avoid this.</p>
</blockquote>
<p>There are other commands used to start these daemon:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># deprecated to use the above</span></span><br><span class="line">$ start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># used on specific node (eg. when a new node is added into the cluster, execute on that node)</span></span><br><span class="line">$ hadoop-daemon.sh start datanode/namenode</span><br><span class="line">$ yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed storage</tag>
        <tag>distributed computing</tag>
      </tags>
  </entry>
  <entry>
    <title>hdfs</title>
    <url>/2019/01/07/hdfs/</url>
    <content><![CDATA[<h1 id="hdfs-architecture"><a href="#hdfs-architecture" class="headerlink" title="hdfs architecture"></a><a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#The+File+System+Namespace" target="_blank" rel="noopener">hdfs architecture</a></h1><p>HDFS 集群以 master-slave 模型运行。其中有两种节点：</p>
<ul>
<li>namenode: master node. know where the files are to find in hdfs</li>
<li>datanode: slave node: have the data of the files</li>
</ul>
<p><img src="https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif" alt="architecture"></p>
<h1 id="namenode"><a href="#namenode" class="headerlink" title="namenode"></a>namenode</h1><p>参见 <a href="https://www.cnblogs.com/shitouer/archive/2013/01/07/2837683.html" target="_blank" rel="noopener">namenode and datanode</a></p>
<p>Namenode 管理着文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块 (block) 所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5d86c88472cd002a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="namenode.png"></p>
<p>每个 file 有多个 block 构成，这些 block 分散的存储在各个 datanode 上（并且根据 replication factor，有冗余副本），而 namenode 知道如何一个 file 有哪些 block (file 的元数据信息)，根据 datanode 发送给它的 block 列表，namenode 就可以构建每个文件中各个 block 的位置信息。即根据文件元数据 + datanode block 列表，可以重建文件 block 位置信息，因此不需要持久化。</p>
<h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><p>由于 datanode 只是分布式地存储 block，不知道这些 block 是怎么组织成文件，以及文件是怎么组织成文件树的。因此 namenode 一旦当掉，整个文件系统就挂了（没办法写和查文件）。因此 namenode 的容错机制很重要。常见的方式：</p>
<ol>
<li>同步备份。即 namenode 中需要持久化存储的镜像文件和log，同步地持久化存储到其他文件系统中。</li>
<li>secondary namenode (异步)。secondary namenode 一般定期地去同步本地 namenode 的镜像和 log。但除此之外，secondary namenode 还有其他用途，比如合并镜像和log（避免文件过大），这个合并过程很占用 cpu 和内存，所以正好在 secondary namenode 上做。合并完后，在 secondary namenode 上也保存一份。不过这种备份恢复会丢掉一部分数据。</li>
</ol>
<h1 id="datanode"><a href="#datanode" class="headerlink" title="datanode"></a>datanode</h1><p>datanode 根据客户端或者 namenode 调度存储/检索数据，并定期向 namenode 发送它们所存储的 block 列表。</p>
<h1 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h1><h2 id="hdfs-namenode-format"><a href="#hdfs-namenode-format" class="headerlink" title="hdfs namenode -format"></a>hdfs namenode -format</h2><p><a href="https://stackoverflow.com/questions/27143409/what-the-command-hadoop-namenode-format-will-do" target="_blank" rel="noopener">stackoverflow</a></p>
<p>Remove all metadata in namenode. Initialize the namenode. However, the data in datanode is not removed.</p>
<h2 id="hdfs-dfs-mkdir-xxx"><a href="#hdfs-dfs-mkdir-xxx" class="headerlink" title="hdfs dfs -mkdir xxx"></a>hdfs dfs -mkdir xxx</h2><p>Create a directory. To see the data location, see <code>local storage</code></p>
<h2 id="hdfs-dfs-put-source-dest"><a href="#hdfs-dfs-put-source-dest" class="headerlink" title="hdfs dfs -put source dest"></a>hdfs dfs -put source dest</h2><p>copy content in source to dest</p>
<h2 id="hdfs-dfs-get"><a href="#hdfs-dfs-get" class="headerlink" title="hdfs dfs -get"></a>hdfs dfs -get</h2><h2 id="hdfs-dfs-du-h-v"><a href="#hdfs-dfs-du-h-v" class="headerlink" title="hdfs dfs -du -h -v"></a>hdfs dfs -du -h -v</h2><p>It displays sizes of files and directories contained in the given directory or the length of a file in case it’s just a file.</p>
<ul>
<li>The <strong>-s</strong> option will result in an <strong>aggregate summary of file lengths</strong> being displayed, rather than the individual files. Without the -s option, the calculation is done by going 1-level deep from the given path.</li>
<li>The <strong>-h</strong> option will format file sizes in a <strong>human-readable</strong> fashion (e.g 64.0m instead of 67108864)</li>
<li>The <strong>-v</strong> option will display <strong>the names of columns</strong> as a header line.</li>
<li>The <strong>-x</strong> option will <strong>exclude snapshots</strong> from the result calculation. Without the -x option (default), the result is always calculated from all INodes, including all snapshots under the given path.</li>
</ul>
<h2 id="hadoop-fs-count-h-dir"><a href="#hadoop-fs-count-h-dir" class="headerlink" title="hadoop fs -count -h /dir/*"></a>hadoop fs -count -h /dir/*</h2><p>显示文件夹下的所有文件数、大小</p>
<h1 id="web-ui"><a href="#web-ui" class="headerlink" title="web ui"></a>web ui</h1><p><a href="https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2_1.html" target="_blank" rel="noopener">hdfs default ports</a> are changed. see <a href="https://issues.apache.org/jira/browse/HDFS-9427" target="_blank" rel="noopener">here</a></p>
<blockquote>
<p>Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820<br>Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868<br>Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 –&gt; 9864</p>
</blockquote>
<h2 id="local-storage"><a href="#local-storage" class="headerlink" title="local storage"></a>local storage</h2><p>From localhost:9870, you can get the namenode information. To see the data you created locally:</p>
<ol>
<li>Login localhost:9870, <strong>get the ‘<em>configuration</em>‘ from the ‘<em>utilities</em>‘</strong></li>
<li>Find <code>dfs.datanode.data.dir</code>  to get the data location</li>
</ol>
<h2 id="Issue-Permission-denied-user-dr-who"><a href="#Issue-Permission-denied-user-dr-who" class="headerlink" title="Issue: Permission denied: user=dr.who"></a>Issue: Permission denied: user=dr.who</h2><p>When ‘<em>browse the file system</em>‘ from ‘<em>utilities</em>‘, there are some dirs (e.g. <code>/tmp</code>) you have no permission to access. It may show:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Permission denied: user&#x3D;dr.who, access&#x3D;READ_EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:cherish:supergroup:drwx------</span><br></pre></td></tr></table></figure>
<p>The ‘<em>dr.who</em>‘ is just a configured static user in <code>core-default.xml</code>:</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">hadoop.http.staticuser.user</span>=<span class="string">dr.who</span></span><br></pre></td></tr></table></figure>
<p>And there is permission check because it’s set to check by default in <code>hdfs-default.xml</code>:</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">dfs.permissions.enabled</span>=<span class="string">true</span></span><br></pre></td></tr></table></figure>
<p>There are three ways to solve it:</p>
<h3 id="solutions"><a href="#solutions" class="headerlink" title="solutions"></a>solutions</h3><h4 id="disable-the-permission-check"><a href="#disable-the-permission-check" class="headerlink" title="disable the permission check"></a>disable the permission check</h4><blockquote>
<p>This is not recommended in the prod mode.</p>
</blockquote>
<p>Add the following property in <code>hdfs-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="change-the-staticuser"><a href="#change-the-staticuser" class="headerlink" title="change the staticuser"></a>change the staticuser</h4><p>Add the following property in <code>core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cherish<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="modify-the-file-permission"><a href="#modify-the-file-permission" class="headerlink" title="modify the file permission"></a>modify the file permission</h4><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ hdfs dfs -chmod -R 755 /tmp</span><br></pre></td></tr></table></figure>
<h1 id="Replica"><a href="#Replica" class="headerlink" title="Replica"></a>Replica</h1>]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>distributed storage</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo+next 设置</title>
    <url>/2018/06/14/hexo-next/</url>
    <content><![CDATA[<p><a href="https://hfcherish.github.io/2018/05/23/build-blog-using-hexo/" target="_blank" rel="noopener">使用 hexo + github 部署博客</a> 介绍了怎么部署自己的博客，然后就开始无休止的调整主题。</p>
<p>我选定的主题是 <a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">next</a>：有目录，也有集成搜索的文档，这是一个 <a href="http://www.itfanr.cc/about/" target="_blank" rel="noopener">example</a>，参照 <a href="https://theme-next.iissnan.com/third-party-services.html" target="_blank" rel="noopener">第三方集成</a> 集成搜索等功能. next 优化配置可参考 <a href="http://www.vitah.net/posts/20f300cc/" target="_blank" rel="noopener">这篇文章</a></p>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><p>我是采用两个配置文件的写法，即在 <code>source/_data/next.yml</code> 中写 next 相关的配置。</p>
<h1 id="code-highlight-配置"><a href="#code-highlight-配置" class="headerlink" title="code highlight 配置"></a>code highlight 配置</h1><p>按照<a href="https://theme-next.iissnan.com/getting-started.html#theme-settings" target="_blank" rel="noopener">主题设定教程</a>，我设置的是 <code>scheme: Mist</code>。默认的代码 highlight 是用 <a href="https://github.com/chriskempson/tomorrow-theme" target="_blank" rel="noopener">tomorrow theme</a>，按照 <a href="https://theme-next.iissnan.com/theme-settings.html#syntax-highlight-scheme" target="_blank" rel="noopener">代码高亮设置教程</a>，可以有五种选项。但是很多 code grammar 高亮显示无效，比如 jsx。所以想换一个 highlight 主题。</p>
<p>找了个 <a href="https://vxhly.github.io/2017/10/hexo-next-advanced-settings/" target="_blank" rel="noopener">code highlight theme 配置教程</a> ，开始动手。</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>hikariCP configuration</title>
    <url>/2018/07/19/hikariCP-configuration/</url>
    <content><![CDATA[<p><a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener">hikariCP</a> 是一个轻量级的数据库连接池。引用 <a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener">数据库连接池性能对比</a> 的说法（我并没有测试过）：</p>
<blockquote>
<ol>
<li>性能方面 hikariCP&gt;druid&gt;tomcat-jdbc&gt;dbcp&gt;c3p0 。hikariCP的高性能得益于最大限度的避免锁竞争。</li>
<li>druid功能最为全面，sql拦截等功能，统计数据较为全面，具有良好的扩展性。</li>
<li>综合性能，扩展性等方面，可考虑使用druid或者hikariCP连接池。</li>
<li>可开启prepareStatement缓存，对性能会有大概20%的提升。</li>
</ol>
</blockquote>
<p>在使用 spring jpa 时，默认使用的连接池是 hikariCP，所以最终采用了这个连接池。</p>
<p>使用过程中出现了一些坑，总结一下。</p>
<h1 id="java-sql-SQLTransientConnectionException"><a href="#java-sql-SQLTransientConnectionException" class="headerlink" title="java.sql.SQLTransientConnectionException"></a>java.sql.SQLTransientConnectionException</h1><p>仅使用默认配置，在运行所有测试时，会出现如下异常信息：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-3c58b4722a0f8ab5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sql exception.png"></p>
<p>这是因为默认的连接池数量是 10，而并行运行测试时，连接池数量不够了。通过设置 <a href="https://github.com/brettwooldridge/HikariCP#frequently-used" target="_blank" rel="noopener"><code>maximumPoolSize</code></a> 解决。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">spring.datasource.hikari.maximum-pool-size:1000</span></span><br></pre></td></tr></table></figure>
<h1 id="too-many-connections"><a href="#too-many-connections" class="headerlink" title="too many connections"></a>too many connections</h1><p>在进行上述设置后，启动应用，连接数据库，发现数据库无法连接，报 <code>too many connections</code>。</p>
<h2 id="fixed-size-pool"><a href="#fixed-size-pool" class="headerlink" title="fixed-size pool"></a>fixed-size pool</h2><p>按 <a href="https://github.com/brettwooldridge/HikariCP/issues/657" target="_blank" rel="noopener">hikariCP owner 的解释</a>，这可能是因为当设置 <code>maximumPoolSize</code> 后，这就变成了 fix-size 的连接池了，即总是会占有 1000（上边的设置）个连接池。idle 连接不会被释放，因为释放了也要创建新的 idle 连接池来保证 fix-size。从而新的连接就无法建立了。</p>
<blockquote>
<p>When running as a fixed-size pool (default) the <code>idleTimeout</code> has no effect. Your example is a fixed-size pool – when <code>minimumIdle</code> is not defined it defaults to <code>maximumPoolSize</code>.</p>
<p><code>idleTimeout</code> is meant to shrink the pool from <code>maximumPoolSize</code> down toward <code>minimumIdle</code> when connections are unused in the pool. However, when <code>minimumIdle</code> == <code>maximumPoolSize</code> then closing an “idle” connection makes no sense as it will be replaced immediately in the pool.</p>
</blockquote>
<p>所以配置 <code>minimumIdle</code> 可以解决上述问题。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">spring.datasource.hikari.maximum-pool-size:1000</span></span><br><span class="line"><span class="string">spring.datasource.hikari.miniumIdle:10</span></span><br></pre></td></tr></table></figure>
<h2 id="wait-timeout-too-long"><a href="#wait-timeout-too-long" class="headerlink" title="wait-timeout too long"></a>wait-timeout too long</h2><p>有时就是因为 connection 一直没被释放，这可能是这是的 connection wait_timeout 太长了。参照 <a href="https://support.rackspace.com/how-to/how-to-change-the-mysql-timeout-on-a-server/" target="_blank" rel="noopener">change the mysql timeout on a server</a> 来了解 ‘wait_timeout’ 的设置。</p>
<p>引用原文：</p>
<blockquote>
<p>Choose a reasonable <code>wait_timeout</code> value. Stateless PHP environments do well with a 60 second timeout or less. Stateful applications that use a connection pool (Java, .NET, etc.) will need to adjust <code>wait_timeout</code> to match their connection pool settings. The default 8 hours (<code>wait_timeout = 28800</code>) works well with properly configured connection pools.</p>
<p>Configure the <code>wait_timeout</code> to be slightly longer than the application connection pool’s expected connection lifetime. This is a good safety check.</p>
</blockquote>
<p>在设置了合理的 mysql <code>wait_timeout</code> 后，同样也设置 hikariCP 的连接池空闲时间，参考<a href="https://github.com/brettwooldridge/HikariCP/wiki/FAQ#q-i-am-getting-a-commysqljdbcexceptionsjdbc4communicationsexception-communications-link-failure-exception-logged-in-the-isconnectionalive-method-of-hikaripool-in-my-logs-what-is-happening" target="_blank" rel="noopener">FAQ</a></p>
<blockquote>
<p>If you set the MySQL <code>wait_timeout = 28800</code> (seconds = 8 hours), you should set HikariCP <code>idleTimeout</code> and <code>maxLifetime</code> to the slightly shorter 28000000 (milliseconds = 7 hours 46 minutes).</p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">spring.datasource.hikari.maximum-pool-size:1000</span></span><br><span class="line"><span class="string">spring.datasource.hikari.miniumIdle:10</span></span><br><span class="line"><span class="string">spring.datasource.hikari.idleTimeout:28000000</span></span><br><span class="line"><span class="string">spring.datasource.hikari.maxLifetime:28000000</span></span><br></pre></td></tr></table></figure>
<h1 id="性能优化配置"><a href="#性能优化配置" class="headerlink" title="性能优化配置"></a>性能优化配置</h1><p>按照前边的说法，合理启用 prepareStatement 缓存，可以大幅提升性能。官方推荐的配置可参考 <a href="https://github.com/brettwooldridge/HikariCP/wiki/MySQL-Configuration" target="_blank" rel="noopener">mysql configuration</a></p>
<p>一个典型配置如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">jdbcUrl=jdbc:mysql://localhost:3306/simpsons</span></span><br><span class="line"><span class="string">user=test</span></span><br><span class="line"><span class="string">password=test</span></span><br><span class="line"><span class="string">dataSource.cachePrepStmts=true</span></span><br><span class="line"><span class="string">dataSource.prepStmtCacheSize=250</span></span><br><span class="line"><span class="string">dataSource.prepStmtCacheSqlLimit=2048</span></span><br><span class="line"><span class="string">dataSource.useServerPrepStmts=true</span></span><br><span class="line"><span class="string">dataSource.useLocalSessionState=true</span></span><br><span class="line"><span class="string">dataSource.rewriteBatchedStatements=true</span></span><br><span class="line"><span class="string">dataSource.cacheResultSetMetadata=true</span></span><br><span class="line"><span class="string">dataSource.cacheServerConfiguration=true</span></span><br><span class="line"><span class="string">dataSource.elideSetAutoCommits=true</span></span><br><span class="line"><span class="string">dataSource.maintainTimeStats=false</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title>hive introduction</title>
    <url>/2019/01/03/hive-introduction/</url>
    <content><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/Hive/Home#Home-HiveDocumentation" target="_blank" rel="noopener">apache hive</a> 是一个 data warehouse 应用。支持分布式存储的大数据读、写和管理，并且支持使用标准的 SQL 语法查询。Hive is not a database.  This is to make use of SQL capabilities by defining a metadata to the files in HDFS.  Long story short, it brings the possibility to query the hdfs file.</p>
<p>hive 并没有固定的数据存储方式。自带的是 csv（comma-separated value）和 tsv (tab-separated values) connectors，也可以使用 connector for other formats。</p>
<h2 id="database-v-s-warehouse"><a href="#database-v-s-warehouse" class="headerlink" title="database v.s. warehouse"></a>database v.s. warehouse</h2><p>参见 <a href="https://panoply.io/data-warehouse-guide/the-difference-between-a-database-and-a-data-warehouse/" target="_blank" rel="noopener">the difference between database and data warehouse</a></p>
<h3 id="database："><a href="#database：" class="headerlink" title="database："></a>database：</h3><p>存储具体的业务数据，完善支持 concurrent transaction 操作（CRUD）。</p>
<p>database contains highly detailed data as well as a detailed relational views. Tables are normalized to achieve efficient storage, concurrent transaction processing, as well as return quick query results.</p>
<ul>
<li><strong>主要用于 OLTP (online trancaction processing)</strong>。</li>
<li><strong>use a normalized structure</strong>. 即通常会组织成 table、row、column，冗余信息很少（比如三张表 product、color、product-color），所以节省空间。在查询时就需要通过复杂的 join 来实现，所以分析性的查询会比较耗时</li>
<li><strong>no historical data</strong>. 主要处理 transaction 数据，只保存现在的数据，进行的查询和分析也是基于现有数据。即它的分析是 static one-time reports</li>
<li><strong>optimization 主要是优化写速度、读速度</strong>。复杂分析因为涉及很多 join，其性能提升也是一个主要的问题。</li>
<li><strong>经常需要满足关系型数据库的 ACID 原则</strong>（atomicity, consistency, isolation, and durability）。所以它需要支持并发操作下的数据完整性。对 concurrent transaction 的支持要求比较高。</li>
</ul>
<h3 id="data-warehouse"><a href="#data-warehouse" class="headerlink" title="data warehouse"></a>data warehouse</h3><p>将企业中的各种数据收集起来，重新组织，对这些数据做高效 <strong><em>分析</em></strong></p>
<blockquote>
<p>A <a href="https://panoply.io/data-warehouse-guide" target="_blank" rel="noopener">data warehouse</a> is a system that pulls together data from many different sources within an organization for reporting and analysis. The reports created from complex queries within a data warehouse are used to make business decisions.</p>
<p>The primary focus of a data warehouse is to provide a correlation between data from existing systems, i.e., product inventory stored in one system, purchase orders for a specific customer, stored in another system. Data warehouses are used for online analytical processing (OLAP), which uses complex queries to analyze rather than process transactions.</p>
</blockquote>
<ul>
<li><strong>主要用于 OLAP (online analysis processing)</strong>. 它收集企业内各个数据源的数据，建立数据关联，对这些数据做复杂的查询分析，以辅佐业务决策。</li>
<li><strong>use a denormalized structure</strong>. 它收集多个相关数据源的数据，将这些 table <a href="https://searchdatamanagement.techtarget.com/definition/denormalization" target="_blank" rel="noopener">denormailize</a>、transform，获得 summarized data、multidimentional views，并基于这些数据实现快速分析和查询。它不在乎冗余，相反，很多时候正是通过冗余重新组织数据，使得查询更方便。</li>
<li><strong>store historical data</strong>. data warehouse 主要是用于分析的，所以通常会存储历史数据，以实现对历史数据和现有数据的对比分析。</li>
<li><strong>optimization 主要是查询响应速度</strong>。它对大数据做分析，响应速度是主要的衡量标准。</li>
<li><strong>一般不支持高并发操作</strong>。支持一定并发，但支持程度远不如 database</li>
</ul>
<h1 id="installation"><a href="#installation" class="headerlink" title="installation"></a>installation</h1><p>See <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">hadoop: setting up a single-node cluster</a>, <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">GettingStarted</a></p>
<p>Hive relies on hadoop. And we need a db (eg. mysql) to store hive metadata. So the prerequisites are:</p>
<ul>
<li><strong>hadoop installed</strong></li>
<li><strong>mysql installed</strong>: to store hive metadata</li>
<li><strong>java installed</strong>: ??</li>
<li><strong>ssh installed and sshd running</strong>: when running hadoop scripts and managing remote hadoop daemons, it use ssh to authenticate.</li>
</ul>
<h3 id="install-hadoop-on-mac"><a href="#install-hadoop-on-mac" class="headerlink" title="install hadoop on mac"></a><a href="https://hfcherish.github.io/2019/01/07/hadoop/" target="_blank" rel="noopener">install hadoop on mac</a></h3><h3 id="install-hive-on-mac"><a href="#install-hive-on-mac" class="headerlink" title="install hive on mac"></a><a href="https://www.cnblogs.com/micrari/p/7067968.html" target="_blank" rel="noopener">install hive on mac</a></h3><blockquote>
<p>After init mysql, you may find that you can’t connect mysql using ‘-uhive -pxxx’. Then try to grant privileges to <code>&#39;hive&#39;@&#39;%&#39;</code> instead of <code>&#39;hive&#39;@&#39;localhost&#39;</code>. Use wildcard <code>%</code> to match all hosts.</p>
</blockquote>
<p>After installation, can try the <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SimpleExampleUseCases" target="_blank" rel="noopener">simple example</a> to see how to conduct analysis on hive.</p>
<h3 id="set-env"><a href="#set-env" class="headerlink" title="set env"></a>set env</h3><p>To use hadoop and hive conveniently, set the bin in Path. Just add the follow config into <code>~/.zshrc</code>, and then source it <code>source ~/.zshrc</code>.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/Cellar/hadoop/3.1.1/libexec</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/Cellar/hive/3.1.1/libexec</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<h3 id="running-using-beeline"><a href="#running-using-beeline" class="headerlink" title="running using beeline"></a><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHiveServer2andBeeline.1" target="_blank" rel="noopener">running using beeline</a></h3><p>beeline is a new hive client to replace the deprecated HiveCli. With beeline, you can execute write, load, query, etc. on hive.</p>
<p>To connect simply, type the following:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hiveserver2</span><br><span class="line">$ beeline -u jdbc:hive2:&#x2F;&#x2F;</span><br></pre></td></tr></table></figure>
<p>To create, alter database/table/column/etc. on hive, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">Hive Data Definition Language</a>.</p>
<p>To get the query commands, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">LanguageManual Select</a></p>
<p>To load data from file, insert, delete, merge, update data, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">DML (data manipulation language)</a></p>
<p>Other non-sql commands to use in HiveQL or beeline, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Commands" target="_blank" rel="noopener">LanguageManual Commands</a>. The <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> related commands are non-sql commands.</p>
<h1 id="Configure-Hive"><a href="#Configure-Hive" class="headerlink" title="Configure Hive"></a>Configure Hive</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-ConfiguringHive" target="_blank" rel="noopener">how to configure hive properties</a></p>
<p>To show hive config in hive cli: (<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ShowConf" target="_blank" rel="noopener">show conf</a>)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># to show current: `set confName`</span></span><br><span class="line">0: jdbc:hive2://slave1:2181,slave2:2181,maste&gt; <span class="built_in">set</span> hive.fetch.task.conversion;</span><br></pre></td></tr></table></figure>
<p>There are two hive-site.xml files. See <a href="https://community.cloudera.com/t5/Support-Questions/Why-do-I-have-two-hive-site-xml-config-files-on-my-HDP-host/td-p/209500" target="_blank" rel="noopener">two hive-site.xml config files on HDP</a></p>
<ul>
<li><p>/etc/hive/conf/hive-site.xml is the config for Hive service itself and is managed via Ambari through the Hive service config page.</p>
</li>
<li><p>/usr/hdp/current/spark-client/conf/hive-site.xml actually points to /etc/spark/conf/hive-site.xml . This is the minimal hive config that Spark needs to access Hive. This is managed via Ambari through the Spark service config page. Ambari correctly configures this hive site for Kerberos. Depending upon your version of HDP you may not have the correct support in Ambari for configuring Livy.  The hive-site.xml in Spark doesn’t have the same template as Hive’s. Ambari will notice the hive-site.xml and overwrite it in the Spark directory whenever Spark is restarted.</p>
</li>
</ul>
<h1 id="analysis-on-hive"><a href="#analysis-on-hive" class="headerlink" title="analysis on hive"></a>analysis on hive</h1><p>When you start a sql function (eg. <code>select count(*) from xxx</code>), it in fact  starts an map-reduce job based on hadoop to search among all datanodes. Such functions are simple analysis implemented by hive.</p>
<blockquote>
<p>Hive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable:</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mapred.job.tracker</span><br></pre></td></tr></table></figure>
<p>For complex analysis, you may need to write custom mappers (map data) &amp; reducers (collect data) scripts. Use<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" target="_blank" rel="noopener"><code>TRANSFORM</code></a> keyword in hive to achieve this.</p>
<p>For example, the <code>weekday_mapper.py</code> to convert <code>unixtime</code> to <code>weekday</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  userid, movieid, rating, unixtime = line.split(<span class="string">'\t'</span>)</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'\t'</span>.join([userid, movieid, rating, str(weekday)])</span><br></pre></td></tr></table></figure>
<p>And then use the script:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> u_data_new (</span><br><span class="line">  userid <span class="built_in">INT</span>,</span><br><span class="line">  movieid <span class="built_in">INT</span>,</span><br><span class="line">  rating <span class="built_in">INT</span>,</span><br><span class="line">  <span class="keyword">weekday</span> <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line">add FILE weekday_mapper.py;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> u_data_new</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  TRANSFORM (userid, movieid, rating, unixtime)</span><br><span class="line">  <span class="keyword">USING</span> <span class="string">'python weekday_mapper.py'</span></span><br><span class="line">  <span class="keyword">AS</span> (userid, movieid, rating, <span class="keyword">weekday</span>)</span><br><span class="line"><span class="keyword">FROM</span> u_data;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">weekday</span>, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> u_data_new</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">weekday</span>;</span><br></pre></td></tr></table></figure>
<h1 id="storage-on-hive"><a href="#storage-on-hive" class="headerlink" title="storage on hive"></a>storage on hive</h1><p>Hive relies on Hadoop. The data in hive is saved in hdfs in fact. And the metadata is saved in mysql (the db can be configured). When check <code>localhost:9870</code>, you can see a new folder <code>/user/hive/warehouse</code>. All tables in hive are dirs in <code>/user/hive/warehouse</code>.</p>
<h2 id="Hive-Partition"><a href="#Hive-Partition" class="headerlink" title="Hive Partition"></a>Hive Partition</h2><p><a href="https://blog.csdn.net/helloxiaozhe/article/details/78445276" target="_blank" rel="noopener">hive中简单介绍分区表(partition table)，含动态分区(dynamic partition)与静态分区(static partition)</a></p>
<blockquote>
<p>Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.<br>Tables or partitions are sub-divided into <strong>buckets,</strong> to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table.<br>For example, a table named <strong>Tab1</strong> contains employee data such as id, name, dept, and yoj (i.e., year of joining). Suppose you need to retrieve the details of all employees who joined in 2012. A query searches the whole table for the required information. However, if you partition the employee data with the year and store it in a separate file, it reduces the query processing time. The following example shows how to partition a file and its data:</p>
</blockquote>
<h2 id="Hive-bucket"><a href="#Hive-bucket" class="headerlink" title="Hive bucket"></a>Hive bucket</h2><p><a href="https://sparkbyexamples.com/apache-hive/hive-partitioning-vs-bucketing-with-examples/" target="_blank" rel="noopener">hive partitioning vs bucket with examples</a></p>
<p><a href="https://sparkbyexamples.com/apache-hive/hive-partitioning-vs-bucketing-with-examples/" target="_blank" rel="noopener">stack-overflow: hive partition vs bucket</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> zipcodes(</span><br><span class="line">RecordNumber <span class="built_in">int</span>,</span><br><span class="line">Country <span class="keyword">string</span>,</span><br><span class="line">City <span class="keyword">string</span>,</span><br><span class="line">Zipcode <span class="built_in">int</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(state <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> Zipcode <span class="keyword">INTO</span> <span class="number">10</span> BUCKETS</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">PARTITIONING</th>
<th style="text-align:left">BUCKETING</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Directory is created on HDFS for each partition.</td>
<td style="text-align:left">File is created on HDFS for each bucket.</td>
</tr>
<tr>
<td style="text-align:left">You can have one or more Partition columns</td>
<td style="text-align:left">You can have only one Bucketing column</td>
</tr>
<tr>
<td style="text-align:left">You can’t manage the number of partitions to create</td>
<td style="text-align:left">You can manage the number of buckets to create by specifying the count</td>
</tr>
<tr>
<td style="text-align:left">NA</td>
<td style="text-align:left">Bucketing can be created on a partitioned table</td>
</tr>
<tr>
<td style="text-align:left">Uses PARTITIONED BY</td>
<td style="text-align:left">Uses CLUSTERED BY</td>
</tr>
</tbody>
</table>
<p>partition  和 bucket 都是将大数据集拆成更小的数据集，加速查询处理的方式。比如按日期拆分区，很多分析只拿当天的分区，处理的数据量、读取的 hdfs 文件很少，就快。</p>
<p>最大的区别是 partition 拆数据就是按 column 值拆，bucket 拆数据是按 column hash 值拆，所以 bucket 最终的桶的数目是固定的，同时一个桶里可能有多个 column 值（parition 每个分区只会存一种 column 的值）</p>
<p>相对来讲，bucket 粒度可能更细。比如一个场景，我们将 order 按 date 分区，分区后每天的数据量还是特别大，如果我们很多查询/join是基于 employee，此时可以基于 employe_id 再分成更多的小集合，即按 employe_id 字段 hash 到 n 个桶里，这种拆桶方式特别有利于宏宇今天说的 map-side join，而且相比 partition，可以控制文件数量（有时想用的 partition 字段可能会分成特别特别多小分区，这个时候 bucket 就更合适些）</p>
<p>上边那个例子，假如 order 按 date+employee_id partition，分区就会特别多（对 hdfs namenode 造成大压力，hive metadata 也有压力），所以按 date partition, 按 employee_id bucket 就比较合适</p>
<h2 id="ORC-vs-Parquet"><a href="#ORC-vs-Parquet" class="headerlink" title="ORC vs Parquet"></a>ORC vs Parquet</h2><p><a href="https://community.cloudera.com/t5/Support-Questions/ORC-vs-Parquet-When-to-use-one-over-the-other/td-p/95942" target="_blank" rel="noopener">orc vs Parquet</a></p>
<p><a href="https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/" target="_blank" rel="noopener">ORCFile in HDP 2: Better Compression, Better Performance</a></p>
<h1 id="Hive-Transactional"><a href="#Hive-Transactional" class="headerlink" title="Hive Transactional"></a>Hive Transactional</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-NewConfigurationParametersforTransactions" target="_blank" rel="noopener">hive transaction</a></p>
<p>Close:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.support.concurrency &#x3D; false;</span><br><span class="line">set hive.optimize.index.filter &#x3D; false;</span><br><span class="line">set hive.txn.manager &#x3D; org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;</span><br><span class="line">set hive.compactor.initiator.on &#x3D; false;</span><br><span class="line">set hive.compactor.worker.threads &#x3D; 0;</span><br><span class="line">set hive.strict.managed.tables &#x3D; false;</span><br><span class="line"></span><br><span class="line">TBLPROPERTIES (&#39;transactional&#39;&#x3D;&#39;false&#39;)</span><br></pre></td></tr></table></figure>
<h1 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">architecture</a></h1><p><img src="https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&amp;modificationDate=1414560669000&amp;api=v2" alt="hive architecture"></p>
<h1 id="Hive-Data-Types"><a href="#Hive-Data-Types" class="headerlink" title="Hive Data Types"></a>Hive Data Types</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-decimal" target="_blank" rel="noopener">hive data types</a></p>
<h1 id="Common-used-commands"><a href="#Common-used-commands" class="headerlink" title="Common used commands"></a>Common used commands</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://slave1:2181&gt; use dbname;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; show tables;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; describe formatted tablename;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; describe extended tableName</span><br></pre></td></tr></table></figure>
<h2 id="auto-increment-id"><a href="#auto-increment-id" class="headerlink" title="auto increment id"></a>auto increment id</h2><p><a href="https://cloud.tencent.com/developer/article/1433240" target="_blank" rel="noopener">two ways hive auto increment id</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">## use row_number</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl_dim  </span><br><span class="line"><span class="keyword">select</span> row_number() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> tbl_stg.id) + t2.sk_max, tbl_stg.*  </span><br><span class="line"><span class="keyword">from</span> tbl_stg </span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="keyword">max</span>(sk),<span class="number">0</span>) sk_max <span class="keyword">from</span> tbl_dim) t2; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## use UDFRowSequence</span></span><br><span class="line">add jar hdfs:///user/hive-contrib-2.0.0.jar;  </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> row_sequence <span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.contrib.udf.udfrowsequence'</span>; </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl_dim  </span><br><span class="line"><span class="keyword">select</span> row_sequence() + t2.sk_max, tbl_stg.*  </span><br><span class="line"><span class="keyword">from</span> tbl_stg </span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="keyword">max</span>(sk),<span class="number">0</span>) sk_max <span class="keyword">from</span> tbl_dim) t2;</span><br></pre></td></tr></table></figure>
<h2 id="get-latest-partition"><a href="#get-latest-partition" class="headerlink" title="get latest partition"></a>get latest partition</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">## will only scan 2-3 partitions</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(ingest_date) <span class="keyword">from</span> db.table_name</span><br><span class="line"><span class="keyword">where</span> ingest_date&gt;<span class="keyword">date_add</span>(<span class="keyword">current_date</span>,<span class="number">-3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="create-table-from-another-table"><a href="#create-table-from-another-table" class="headerlink" title="create table from another table"></a>create table from another table</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> new_test </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'|'</span> </span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFile </span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">source</span> <span class="keyword">where</span> <span class="keyword">col</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="select-all-without-some-columns"><a href="#select-all-without-some-columns" class="headerlink" title="select all without some columns"></a>select all without some columns</h2><p><a href="https://blog.csdn.net/Kikitious_Du/article/details/84754240" target="_blank" rel="noopener">blog</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.support.quoted.identifiers=<span class="keyword">none</span>;</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="string">`(num|uid)?+.+`</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> </span><br><span class="line">    row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uid <span class="keyword">order</span> <span class="keyword">by</span> pay_time <span class="keyword">asc</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line">    ,*</span><br><span class="line">    <span class="keyword">from</span> <span class="keyword">order</span>) first_order</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">num</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="select-latest-in-group"><a href="#select-latest-in-group" class="headerlink" title="select latest in group"></a>select latest in group</h2><p><a href="https://stackoverflow.com/questions/35520193/how-to-find-most-recent-records-for-every-group-in-hive" target="_blank" rel="noopener">link</a></p>
<p>use rank</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">  <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span>, starttime, <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">unix_timestamp</span>(starttime, <span class="string">'EEE, dd MMM yyyy hh:mm:ss z'</span>) <span class="keyword">desc</span>) <span class="keyword">as</span> rnk <span class="keyword">from</span> hive_table) a </span><br><span class="line"> <span class="keyword">where</span> a.rnk=<span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h2 id="hive-cli-pretty"><a href="#hive-cli-pretty" class="headerlink" title="hive cli pretty"></a>hive cli pretty</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cli.print.header=<span class="literal">true</span>; // 打印列名</span><br><span class="line"><span class="keyword">set</span> hive.cli.print.row.to.vertical=<span class="literal">true</span>; // 开启行转列功能, 前提必须开启打印列名功能</span><br><span class="line"><span class="keyword">set</span> hive.cli.print.row.to.vertical.num=<span class="number">1</span>; // 设置每行显示的列数</span><br></pre></td></tr></table></figure>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><h2 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h2><p>和 spark 的小文件问题一样，hive 的运算引擎（mapreduce 或 Tez），为了提高性能，最后都会采用多个 reducer 来写数据，这个时候就会有小文件。不同于 Spark，Hive 本身提供了多种措施来优化小文件存储，我们只需要设置就行</p>
<h3 id="1-使用-concatenate"><a href="#1-使用-concatenate" class="headerlink" title="1. 使用 concatenate"></a>1. 使用 concatenate</h3><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTable/PartitionConcatenate" target="_blank" rel="noopener">hive concatenate</a> 主要针对 orc 和 rcfile 文件格式存储的文件，特别是 orc ，可以直接执行 stripe level 的 merge，省掉 deserialize 和 decode 的开销，很高效。（concatenate 可以执行多次，最终文件数量不会变化）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> (partition_key = <span class="string">'partition_value'</span> [, ...])] CONCATENATE;</span><br></pre></td></tr></table></figure>
<h3 id="2-使用一些配置，在写文件时，自动-merge"><a href="#2-使用一些配置，在写文件时，自动-merge" class="headerlink" title="2. 使用一些配置，在写文件时，自动 merge"></a>2. 使用一些配置，在写文件时，自动 merge</h3><p>输入时合并：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--  每个Map最大输入大小，决定合并后的文件数</span></span><br><span class="line"><span class="keyword">set</span>  mapred. <span class="keyword">max</span> .split.size=<span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并</span></span><br><span class="line"><span class="keyword">set</span>  mapred. <span class="keyword">min</span> .split.size.per.node=<span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并</span></span><br><span class="line"><span class="keyword">set</span>  mapred. <span class="keyword">min</span> .split.size.per.rack=<span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 执行Map前进行小文件合并</span></span><br><span class="line"><span class="keyword">set</span>  hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
<p>输出时合并：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- hive 输出时合并的配置参数</span></span><br><span class="line"><span class="comment">-- 在Map-only的任务结束时合并小文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 在Map-Reduce的任务结束时合并小文件, 默认 false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.tezfiles=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 合并文件的大小, 默认 256000000</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task=<span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 当输出文件的平均大小小于该值时, 启动一个独立的map-reduce任务进行文件merge， 默认 16000000</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize=<span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 当这个参数设置为true,orc文件进行stripe Level级别的合并,当设置为false,orc文件进行文件级别的合并。默认 true</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.orcfile.stripe.level=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>
<p>根据查询类型不同，相应的mapfiles/mapredfiles参数需要打开；</p>
<p>结果文件的平均大小需要大于avgsize参数的值。</p>
<h1 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h1><h2 id="count-return-0"><a href="#count-return-0" class="headerlink" title="count(*) return 0"></a>count(*) return 0</h2><p><a href="https://community.cloudera.com/t5/Support-Questions/hive-count-not-working/td-p/216889" target="_blank" rel="noopener">hive count(*) not working</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-ExistingTables%E2%80%93ANALYZE" target="_blank" rel="noopener">hive analyze</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以设，但不好</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; <span class="built_in">set</span> hive.fetch.task.conversion=none;</span><br><span class="line"><span class="comment"># 或者设</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; <span class="built_in">set</span> hive.compute.query.using.stats=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 推荐</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; analyze table t [partition p] compute statistics <span class="keyword">for</span> [columns c,...];</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Its better not to disturb the properties on the statistics usage like hive.compute.query.using.stats. It impacts the way the statistics are used in your query for performance optimization and execution plans. It has tremendous influence on execution plans, the statistics stored depends on the file format as well. Therefore definitely not a solution to change any property with regards to statistics.<br>The real reason for count not working correctly is the statistics not updated in the hive due to which it returns 0. When a table is created first, the statistics is written with no data rows. Thereafter any data append/change happens hive requires to update this statistics in the metadata. Depending on the circumstances hive might not be updating this real time.<br>Therefore running the ANALYZE command recomputes this statistics to make this work correctly.</p>
</blockquote>
<h2 id="hive-not-recognizing-alias-names-in-select-part"><a href="#hive-not-recognizing-alias-names-in-select-part" class="headerlink" title="hive not recognizing alias names in select part"></a>hive not recognizing alias names in select part</h2><p>The where clause is evaluated before the select clause, which is why you can’t refer to select aliases in your where clause.</p>
<p>You can however refer to aliases from a derived table.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">select * from (</span><br><span class="line">  select user as u1, url as u2 from rank_test</span><br><span class="line">) t1 <span class="built_in">where</span> u1 &lt;&gt; <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">select * from (</span><br><span class="line">  select user, count(*) as cnt from rank_test group by user</span><br><span class="line">) t1 <span class="built_in">where</span> cnt &gt;= 2;</span><br></pre></td></tr></table></figure>
<p>Side note: a more efficient way to write the last query would be</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">select user, count(*) as cnt from rank_test group by user</span><br><span class="line">having count(*) &gt;= 2</span><br></pre></td></tr></table></figure>
<h2 id="In-not-in-substitution"><a href="#In-not-in-substitution" class="headerlink" title="In, not in substitution"></a>In, not in substitution</h2><p>Hive supports sub-query in <code>in</code> , <code>not in</code> only after 0.13. And <code>in</code> may be slow, so we can replace it with join.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- in</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b)</span><br><span class="line"><span class="comment">-- in substitutionn</span></span><br><span class="line"><span class="keyword">select</span> a.* <span class="keyword">from</span> a <span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b) b1 <span class="keyword">on</span> a.id = b1.id</span><br></pre></td></tr></table></figure>
<h2 id="VERTEX-FAILURE"><a href="#VERTEX-FAILURE" class="headerlink" title="VERTEX_FAILURE"></a>VERTEX_FAILURE</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions=8000;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions.pernode=8000;</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> hive.tez.log.level=DEBUG;</span><br></pre></td></tr></table></figure>
<h2 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">select</span> <span class="keyword">sum</span>(<span class="keyword">id</span>) <span class="keyword">from</span> my;</span><br></pre></td></tr></table></figure>
<h2 id="xa0"><a href="#xa0" class="headerlink" title="\xa0"></a>\xa0</h2><p>(<code>SPACE_SEPARATOR</code>, <code>LINE_SEPARATOR</code>, or <code>PARAGRAPH_SEPARATOR</code>) but is not also a non-breaking space (<code>&#39;\u00A0&#39;</code>, <code>&#39;\u2007&#39;</code>, <code>&#39;\u202F&#39;</code>).</p>
<p><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Character.html#isWhitespace-char-" target="_blank" rel="noopener">java isWhiteSpace()</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(res.selectExpr(&quot;trim(translate(mobile1, &#39;\u00A0&#39;, &#39; &#39;))&quot;).collect())</span><br><span class="line">print(res.selectExpr(&quot;trim(regexp_replace(mobile1, &#39;\u00A0|\u2007|\u202F&#39;, &#39; &#39;))&quot;).collect())</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>hive</tag>
        <tag>data warehouse</tag>
      </tags>
  </entry>
  <entry>
    <title>数据导入 hive</title>
    <url>/2020/11/11/import-data-to-hive/</url>
    <content><![CDATA[<h1 id="ftp-csv-文件导入"><a href="#ftp-csv-文件导入" class="headerlink" title="ftp .csv 文件导入"></a>ftp .csv 文件导入</h1><p>可以先将文件弄到 HDFS，然后创建/更新 hive 表来关联到 HDFS 文件。</p>
<p>将文件弄到 HDFS有以下一些方法：</p>
<ol>
<li><strong>ftp -&gt; local -&gt; hdfs:</strong> 将文件先下载到本地，再通过 hdfs 命令拷贝到 hdfs 中</li>
<li><strong>ftp -&gt; hdfs</strong>: 直接连接 FTP，将文件拷到 hdfs 中，省却本地拷贝</li>
<li><strong>已有的数据采集工具</strong>：使用实时数据流处理系统，来实现不同系统之间的流通</li>
</ol>
<h2 id="一、ftp-gt-local-gt-hdfs"><a href="#一、ftp-gt-local-gt-hdfs" class="headerlink" title="一、ftp -&gt; local -&gt;hdfs"></a>一、ftp -&gt; local -&gt;hdfs</h2><p>几种方案：</p>
<ol>
<li><p><code>hadoop fs -get ftp://uid:password@server_url/file_path temp_file | hadoop fs -moveFromLocal tmp_file hadoop_path/dest_file</code> </p>
</li>
<li><p>参照<a href="https://community.cloudera.com/t5/Support-Questions/How-read-ftp-server-files-and-load-into-hdfs-in-incremental/m-p/223519/highlight/true#M185384" target="_blank" rel="noopener">这个实现</a>用 python 包从 ftp 中读，然后用 hdfs 命令写到 hdfs</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> hdfs <span class="keyword">import</span> InsecureClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also use KerberosClient or custom client</span></span><br><span class="line">namenode_address = <span class="string">'your namenode address'</span></span><br><span class="line">webhdfs_port = <span class="string">'your webhdfs port'</span> <span class="comment"># default for Hadoop 2: 50070, Hadoop 3: 9870</span></span><br><span class="line">user = <span class="string">'your user name'</span></span><br><span class="line">client = InsecureClient(<span class="string">'http://'</span> + namenode_address + <span class="string">':'</span> + webhdfs_port, user=user)</span><br><span class="line"></span><br><span class="line">ftp_address = <span class="string">'your ftp address'</span></span><br><span class="line">hdfs_path = <span class="string">'where you want to write'</span></span><br><span class="line"><span class="keyword">with</span> urlopen(ftp_address) <span class="keyword">as</span> response:</span><br><span class="line">    content = response.read()</span><br><span class="line">    <span class="comment"># You can also use append=True</span></span><br><span class="line">    <span class="comment"># Further reference: https://hdfscli.readthedocs.io/en/latest/api.html#hdfs.client.Client.write</span></span><br><span class="line">    <span class="keyword">with</span> client.write(hdfs_path) <span class="keyword">as</span> writer:</span><br><span class="line">        writer.write(content</span><br></pre></td></tr></table></figure>
</li>
<li><p>参考 <a href="https://blog.csdn.net/yiluohan0307/article/details/79364525" target="_blank" rel="noopener">ftp 提取文件到 hdfs</a></p>
</li>
</ol>
<h2 id="二、ftp-gt-hdfs"><a href="#二、ftp-gt-hdfs" class="headerlink" title="二、ftp -&gt; hdfs"></a>二、ftp -&gt; hdfs</h2><p>几种方案：(参考 <a href="https://blog.csdn.net/yiluohan0307/article/details/79364525" target="_blank" rel="noopener">ftp 提取文件到 hdfs</a>)</p>
<ol>
<li><p>用 <a href="http://hadoop101.blogspot.com/?view=classic" target="_blank" rel="noopener">FTP To HDFS</a> 连接 ftp，把文件直接放到 hdfs</p>
</li>
<li><p>HDFS dfs -cp: 简单快速，但不显示进度，适用于小文件</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ hdfs dfs –cp [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hadoop distcp: 分布式提取，快，能显示拷贝进度，不支持流式写入（即拷贝的文件不能有其他程序在写入），适合大量文件或大文件的拷贝</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ hadoop distcp [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="三、已有的数据采集工具"><a href="#三、已有的数据采集工具" class="headerlink" title="三、已有的数据采集工具"></a>三、已有的数据采集工具</h2><h3 id="文件导入"><a href="#文件导入" class="headerlink" title="文件导入"></a>文件导入</h3><ol>
<li><p><a href="https://nifichina.github.io/general/GettingStarted.html#%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%88%AB%E7%9A%84%E5%A4%84%E7%90%86%E5%99%A8" target="_blank" rel="noopener">apache NiFi</a> 来实现不同系统之间的流通，似乎拷贝完，会直接删除 ftp 上的文件</p>
</li>
<li><p>Apache Flume是一个分布式、可靠、高可用的日志收集系统，支持各种各样的数据来源。基于流式数据，适用于日志和事件类型的数据收集，重构后的Flume-NG版本中一个agent（数据传输流程）中的source（源）和sink（目标）之间通过channel进行链接，同一个源可以配置多个channel。多个agent还可以进行链接组合共同完成数据收集任务，使用起来非常灵活。</p>
<p><a href="https://blog.csdn.net/qq_39160721/article/details/80255588" target="_blank" rel="noopener">flume 采集 ftp 文件 上传到 hadoop</a> 使用 <a href="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#spooling-directory-source" target="_blank" rel="noopener">spooldir source</a>（不确定是不是能用）, 也可以使用第三方 source 组件 <a href="https://github.com/keedio/flume-ftp-source" target="_blank" rel="noopener">flume-ftp-source</a></p>
<blockquote>
<p>Flume 也支持 sql source 的流式导入（使用 <a href="https://github.com/keedio/flume-ng-sql-source" target="_blank" rel="noopener">flume-ng-sql-source</a> 插件），并提供对数据进行简单处理，并写到各数据接收方的能力。因此它的实时性更好。</p>
</blockquote>
</li>
<li><p>DataX：阿里的开源框架，本身社区不太活跃，但有很多 fork 再改的，似乎架构不错</p>
</li>
<li><p>Gobllin: Gobblin是用来整合各种数据源的通用型ETL框架，Gobblin的接口封装和概念抽象做的很好，作为一个ETL框架使用者，我们只需要实现我们自己的Source，Extractor，Conventer类，再加上一些数据源和目的地址之类的配置文件提交给Gobblin就行了。Gobblin相对于其他解决方案具有普遍性、高度可扩展性、可操作性。</p>
</li>
<li><p>kettle：一款开源的ETL工具</p>
</li>
</ol>
<h3 id="其他数据源（非-FTP-文件）"><a href="#其他数据源（非-FTP-文件）" class="headerlink" title="其他数据源（非 FTP 文件）"></a>其他数据源（非 FTP 文件）</h3><ol>
<li>Apache Sqoop：RDBMS <--> HDFS</li>
<li>Aegisthus：针对 Cassandra 数据源</li>
<li>mongo-hadoop：针对 mongodb 数据源</li>
</ol>
<h1 id="数据导入需要关注的问题"><a href="#数据导入需要关注的问题" class="headerlink" title="数据导入需要关注的问题"></a>数据导入需要关注的问题</h1><ol>
<li><strong>数据源都有哪些？</strong><ol>
<li>结构化（sql）、半结构化（json, xml…)、非结构化（video、image、file…)</li>
<li>日志数据（csv)、业务数据</li>
</ol>
</li>
<li><strong>是否可以直接连接数据库？</strong><ol>
<li>针对关系型数据，如果可连接数据库，可以通过 sqoop 导入数据到 hive<ol>
<li>增量式导入？？</li>
</ol>
</li>
<li>针对关系型数据，如果不能连接数据库：<ol>
<li><strong>是否可以默认周期性导出符合特定标准的 .csv 文件？</strong><ol>
<li>如果数据库导出 dump 文件，再将 dump 文件导入到 hadoop，则比较麻烦，以 oracle 为例，可能需要使用 COPYToBDA 来创建 hive table <a href="https://weidongzhou.wordpress.com/2016/11/12/data-query-between-bda-and-exadata-part-4-query-oracle-dump-file-on-bda-using-copy2bda/" target="_blank" rel="noopener">Query Oracle Dump File on BDA Using Copy2BDA</a> ，或者将 dump 文件先导入到一个 temp oracle 数据库中，再用 sqoop 导入到 hive</li>
<li>如果数据库周期性导出 .csv 文件，将这些 .csv 文件使用上述工具（flume 等）导入到 hive，需要关注增量式导出和导入<ol>
<li>增量式导出：文件的组织结构、命名规范 ，.csv 内 record 要求包含 modified date, delete date（在增量式导入时，需要基于这些时间来合并表）</li>
<li>增量式导入：将新增的 .csv 文件作为 hive external table，然后通过中间 view 来合并基表和incremental 表，并更新基表、清空 incremental 表。<a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html" target="_blank" rel="noopener">Incrementally Updating a Table</a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>导入周期和实时性需求<ol>
<li><strong>哪些需要每天批量导入、哪些需要流式实时导入</strong></li>
<li><strong>哪些需要全量导入、哪些需要增量式导入？</strong></li>
</ol>
</li>
<li><strong>如何实现增量式导入？删除的数据是否有删除标识（软删除）？</strong><ol>
<li>如果用 sqoop，参考 <a href="https://hfcherish.github.io/2020/11/10/sqoop/" target="_blank" rel="noopener">sqoop 增量导入</a>，不支持对删除数据的处理</li>
<li>如果用 flume<ol>
<li>如果是 sql source，使用 <a href="https://github.com/keedio/flume-ng-sql-source" target="_blank" rel="noopener">flume-ng-sql-source</a>, 对于 mysql 可以通过 query <code>agent.sources.sqlSource.custom.query</code> 来获取增量 source</li>
<li>如果是文件导入，则需要通过 <a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html" target="_blank" rel="noopener">Incrementally Updating a Table</a> 来合并表</li>
</ol>
</li>
<li>Spark SQL</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>java api lib for excel</title>
    <url>/2018/07/25/java-api-for-excel/</url>
    <content><![CDATA[<h1 id="可选-lib"><a href="#可选-lib" class="headerlink" title="可选 lib"></a>可选 lib</h1><ul>
<li><a href="http://poi.apache.org/components/document/index.html" target="_blank" rel="noopener">apache POI</a>：java 中最大众的 ，支持 <code>xls</code>、<code>xlsx</code>，提供接口来创建、读写 excel文件。</li>
<li><a href="http://www.openoffice.org/download/sdk/" target="_blank" rel="noopener">apache openOffice uno</a>：</li>
<li><del><a href="http://www.openoffice.org/download/sdk/" target="_blank" rel="noopener">JExcel app</a></del>：这个功能强大，什么都可以做，但是可能收费，而且仅基于 windows + installed excel</li>
<li><del><a href="http://jexcelapi.sourceforge.net/" target="_blank" rel="noopener">JExcelAPI</a></del>：轻量更易用，但似乎仅支持 <code>xls</code>，而且不支持复杂的 range、formular 计算操作。</li>
<li><a href="https://www.teamdev.com/jexcel" target="_blank" rel="noopener">javascript 版本的 JExcel</a></li>
<li><del><a href="https://www.docx4java.org/trac/docx4j" target="_blank" rel="noopener">docx4j</a></del>：其中 <a href="https://github.com/plutext/docx4j/tree/master/src/samples/xlsx4j/org/xlsx4j/samples" target="_blank" rel="noopener">xlsx4j</a> 是处理 excel 的，不过看起来功能比较简单。</li>
<li><a href="https://msdn.microsoft.com/zh-cn/vba/vba-excel" target="_blank" rel="noopener">microsoft excel VBA</a>：vba 是微软写的操作 office 软件的语言。所以可以利用这个 vba 写代码……</li>
</ul>
<p>ref doc:</p>
<ul>
<li><a href="http://www.baeldung.com/java-microsoft-excel" target="_blank" rel="noopener">baeldung example for apache POI &amp; JExcelAPI</a></li>
<li><a href="https://www.mkyong.com/java/jexcel-api-reading-and-writing-excel-file-in-java/" target="_blank" rel="noopener">Mkyong example for JExcelAPI</a></li>
</ul>
<h1 id="核心操作支持"><a href="#核心操作支持" class="headerlink" title="核心操作支持"></a>核心操作支持</h1><table>
<thead>
<tr>
<th>lib</th>
<th>apache poi</th>
<th>apache openoffice</th>
<th>JExcelAPI</th>
</tr>
</thead>
<tbody>
<tr>
<td>read/create excel file</td>
<td>✔️</td>
<td>可以</td>
<td>✔️</td>
</tr>
<tr>
<td>read/write excel cells</td>
<td>1. 获取 sheet，遍历 row，遍历 row cells；</td>
<td>可以</td>
<td>1. 获取 sheet，可以根据 cell 行列数获取（<code>getCell(rowIndex, columnIndex)</code>）</td>
</tr>
<tr>
<td>compute formula for cells</td>
<td>可以（用 formular evaluator，参见<a href="https://stackoverflow.com/questions/5937373/using-apache-poi-hssf-how-can-i-refresh-all-formula-cells-at-once" target="_blank" rel="noopener">stackoverflow</a>）</td>
<td>可以(calculateAll())</td>
<td>可能可以</td>
</tr>
<tr>
<td>write/read named cells of file</td>
<td>可以（<code>workBook.getNamedAt(&quot;name&quot;)</code>，参见 <a href="https://stackoverflow.com/questions/33183144/apache-poi-update-cells-in-a-named-range" target="_blank" rel="noopener">stackoverflow</a>, <a href="https://poi.apache.org/components/spreadsheet/quick-guide.html#NamedRanges" target="_blank" rel="noopener">官方文档</a>）</td>
<td>可以（<code>namedRange</code>)</td>
<td>NO</td>
</tr>
<tr>
<td>pros</td>
<td>1. 使用广泛；2. 文档清晰；3. 支持很多复杂需求</td>
<td>1. 似乎功能比 poi 更简易，也更丰富些；2. 可以同时操作 openoffice 和 microoffice；3. 语言支持多，只要使用相应语言的 binding 就可以</td>
<td>1. 简单轻量；2. 接口友好</td>
</tr>
<tr>
<td>cons</td>
<td>接口不是很友好，访问起来比较麻烦</td>
<td>1. 文档乱七八糟；2. 采用 uno，领域对象不太一致；3. 必须要安装 openoffice</td>
<td>1. 功能太简单，复杂需求实现不了</td>
</tr>
</tbody>
</table>
<h1 id="核心操作-example（POI）"><a href="#核心操作-example（POI）" class="headerlink" title="核心操作 example（POI）"></a>核心操作 example（POI）</h1><p><a href="https://poi.apache.org/components/spreadsheet/quick-guide.html#NamedRanges" target="_blank" rel="noopener">quick guide</a></p>
<p><a href="https://github.com/HFCherish/learning-common/blob/master/common-excel/src/test/java/learning/common/excel/utils/XLSXTest.java" target="_blank" rel="noopener">源代码</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//        load excel</span></span><br><span class="line">XSSFWorkbook workbook = <span class="keyword">new</span> XSSFWorkbook(getFile(<span class="string">"test.xlsx"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        get sheet by name</span></span><br><span class="line">String sheetName = <span class="string">"Product Mix"</span>;</span><br><span class="line">XSSFSheet sheet = workbook.getSheet(sheetName);</span><br><span class="line">assertThat(sheet.getSheetName(), is(sheetName));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        get cell by coordinate</span></span><br><span class="line">XSSFCell tvsetNumber = getCell(sheet, <span class="string">"D4"</span>);</span><br><span class="line">assertThat(tvsetNumber.getNumericCellValue(), closeTo(<span class="number">100</span>, <span class="number">0.1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        get formula cell by coordinate</span></span><br><span class="line">XSSFCell total = getCell(sheet, <span class="string">"D13"</span>);</span><br><span class="line">assertThat(total.getNumericCellValue(), closeTo(<span class="number">16000</span>, <span class="number">0.1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        change cell value</span></span><br><span class="line">tvsetNumber.setCellValue(<span class="number">200</span>);</span><br><span class="line">assertThat(tvsetNumber.getNumericCellValue(), closeTo(<span class="number">200</span>, <span class="number">0.1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        refresh formulas</span></span><br><span class="line">XSSFFormulaEvaluator.evaluateAllFormulaCells(workbook);</span><br><span class="line">assertThat(total.getNumericCellValue(), closeTo(<span class="number">23500</span>, <span class="number">0.1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        write back to file</span></span><br><span class="line">workbook.write(<span class="keyword">new</span> FileOutputStream(getFile(<span class="string">"update.xlsx"</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        check data updated</span></span><br><span class="line">XSSFWorkbook updateWorkBook = <span class="keyword">new</span> XSSFWorkbook(getFile(<span class="string">"update.xlsx"</span>));</span><br><span class="line">assertThat(getCell(updateWorkBook.getSheet(sheetName), <span class="string">"D4"</span>).getNumericCellValue(), closeTo(<span class="number">200</span>, <span class="number">0.1</span>));</span><br><span class="line">assertThat(getCell(updateWorkBook.getSheet(sheetName), <span class="string">"D13"</span>).getNumericCellValue(), closeTo(<span class="number">23500</span>, <span class="number">0.1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//		  get cell by name</span></span><br><span class="line">XSSFName test_name = workbook.getName(<span class="string">"test_cell_name"</span>);</span><br><span class="line">AreaReference areaReference = <span class="keyword">new</span> AreaReference(test_name.getRefersToFormula(), SpreadsheetVersion.EXCEL2007);</span><br><span class="line"></span><br><span class="line">CellReference firstCell = areaReference.getFirstCell();</span><br><span class="line">XSSFSheet sheet = workbook.getSheet(firstCell.getSheetName());</span><br><span class="line">XSSFRow row = sheet.getRow(firstCell.getRow());</span><br><span class="line">XSSFCell cell = row.getCell(firstCell.getCol());</span><br><span class="line"></span><br><span class="line"><span class="comment">// get cell function</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> XSSFCell <span class="title">getCell</span><span class="params">(XSSFSheet sheet, String cellCoordinate)</span> </span>&#123;</span><br><span class="line">    CellReference d4 = <span class="keyword">new</span> CellReference(cellCoordinate);</span><br><span class="line">    XSSFRow row = sheet.getRow(d4.getRow());</span><br><span class="line">    <span class="keyword">return</span> row.getCell(d4.getCol());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="其他操作（POI）"><a href="#其他操作（POI）" class="headerlink" title="其他操作（POI）"></a>其他操作（POI）</h1><h2 id="copy-sheet"><a href="#copy-sheet" class="headerlink" title="copy sheet"></a>copy sheet</h2><h2 id="xlsm-to-xlsx"><a href="#xlsm-to-xlsx" class="headerlink" title="xlsm to xlsx"></a>xlsm to xlsx</h2>]]></content>
      <tags>
        <tag>java</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title>java reflection</title>
    <url>/2018/07/14/java-reflection/</url>
    <content><![CDATA[<h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><p>java 泛型存在类型擦除（参见 <a href="https://blog.csdn.net/briblue/article/details/76736356" target="_blank" rel="noopener">java 泛型</a>）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; l1 = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">List&lt;Integer&gt; l2 = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line"></span><br><span class="line">System.out.println(l1.getClass() == l2.getClass());	<span class="comment">// return true, 两个都是 List.class</span></span><br></pre></td></tr></table></figure>
<h2 id="获取运行时泛型类型"><a href="#获取运行时泛型类型" class="headerlink" title="获取运行时泛型类型"></a>获取运行时泛型类型</h2><p>类型擦除使得根据类定义获取 runtime 泛型类型是不可能的，一般有几种方法(参见 <a href="https://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime" target="_blank" rel="noopener">stackoverflow</a>)：</p>
<ol>
<li>根据类对象实例获取，可参见 <a href="http://qussay.com/2013/09/28/handling-java-generic-types-with-reflection/#has_default_constructor" target="_blank" rel="noopener">handle java generic types with reflection</a><ul>
<li>eg. <code>Class&lt;T&gt; tClass = (Class&lt;T&gt;) ReflectionUtil.getClass(ReflectionUtil.getParameterizedTypes(this)[0]);</code></li>
</ul>
</li>
<li>从父类中获取（要求父类有相同的泛型参数）<ul>
<li>eg. <code>Class&lt;T&gt; tClass = (Class&lt;T&gt;) ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]</code></li>
</ul>
</li>
<li>通过方法存储泛型类型为 field。但这意味着所有的 client 都必须要通过相应方法设置该 field</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 通过方法（constructor）存储泛型类型</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GenericClass</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">private</span> <span class="keyword">final</span> Class&lt;T&gt; type;</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="title">GenericClass</span><span class="params">(Class&lt;T&gt; type)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">this</span>.type = type;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Class&lt;T&gt; <span class="title">getMyType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">this</span>.type;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="获取一个带泛型信息的-class-变量"><a href="#获取一个带泛型信息的-class-变量" class="headerlink" title="获取一个带泛型信息的 class 变量"></a>获取一个带泛型信息的 class 变量</h2><p>例如当使用 <code>new ObjectMapper().readValue(string, someClass)</code>，而 someClass 是包含泛型参数的类型（eg. List<Integer>），如何获取这样的 class 变量？</p>
<p>参见 <a href="https://stackoverflow.com/a/6349488/10003123" target="_blank" rel="noopener">stackOverflow</a>，总结如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line">ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line"></span><br><span class="line"><span class="comment">// as Array</span></span><br><span class="line">MyClass[] myObjects = mapper.readValue(json, MyClass[]<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// as List</span></span><br><span class="line">List&lt;MyClass&gt; myObjects = mapper.readValue(jsonInput, <span class="keyword">new</span> TypeReference&lt;List&lt;MyClass&gt;&gt;()&#123;&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// as List (another more generic way)</span></span><br><span class="line">List&lt;MyClass&gt; myObjects = mapper.readValue(jsonInput, mapper.getTypeFactory().constructCollectionType(List<span class="class">.<span class="keyword">class</span>, <span class="title">MyClass</span>.<span class="title">class</span>))</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// as List (using TypeToken in Gson)</span></span><br><span class="line">Class&lt;List&lt;MyClass&gt;&gt; tClass = <span class="keyword">new</span> TypeToken&lt;List&lt;MyClass&gt;() &#123;</span><br><span class="line">        &#125;.getRawType();</span><br><span class="line">        </span><br><span class="line"><span class="comment">// use c</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java.lang.UnsatisfiedLinkError: no xxx in java.library.path</title>
    <url>/2018/08/14/java-application-using-third-party-lib/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><ol>
<li>项目需要引入 local 第三方包</li>
<li>该第三方包只有 window/linux license，而开发在 macos</li>
<li>开发时，通过 gradle dependency <code>compile files(&#39;path/to/thejar.jar&#39;)</code> 来引入包</li>
</ol>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>运行时，报错误 <code>java.lang.UnsatisfiedLinkError: no thejar in java.library.path</code></p>
<h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>引入 <code>.dll</code> 或 <code>.so</code> 失败造成。</p>
<h1 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h1><ol>
<li>把 thejar 加入到 path 中 ————— not work</li>
<li>加入 path，并 loadLibrary ——————— not work</li>
<li>should work（配置 .dll 或 .so 路径）：<ol>
<li>配置 PATH</li>
<li>或 jar 包启动时，设置 ‘-Djava.library.path’</li>
</ol>
</li>
</ol>
<p>有关 <code>PATH</code>, <code>-classpath</code>, <code>java.library.path</code> 的区别，再 google。java 在使用这三个 path 时：</p>
<ol>
<li><code>PATH</code>：用来寻找 <code>java</code>, <code>javac</code> 等 command 并执行</li>
<li><code>classpath</code>：jvm 在执行时用来寻找 java class。classpath 一般指向 jar 包的位置，即 jdk 的 lib 目录</li>
<li><code>java.library.path</code>: 指向非 java 类包的位置，如 .dll, .so 等。在 java <code>System.loadLibrary()</code> 时从这找</li>
</ol>
<p><code>java.library.path</code> 的设定可参照 <a href="https://stackoverflow.com/questions/1403788/java-lang-unsatisfiedlinkerror-no-dll-in-java-library-path" target="_blank" rel="noopener">stackoverflow</a></p>
<ul>
<li><code>java -jar xxx.jar -Djava.library.path=xxx</code></li>
<li>Set PATH</li>
<li>Set programmatically (NOT RECOMMENDED, may cause unpredictable result):</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"java.library.path"</span>, path);</span><br><span class="line"><span class="comment">//set sys_paths to null</span></span><br><span class="line">final Field sysPathsField = ClassLoader.class.getDeclaredField("sys_paths");</span><br><span class="line">sysPathsField.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">sysPathsField.set(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br></pre></td></tr></table></figure>
<p>linux 下可通过 <code>LD_LIBRARY_PATH</code> 设置 <code>java.library.path</code>。windows 下似乎是通过 <code>PATH</code> 设定（待确定）</p>
]]></content>
      <tags>
        <tag>jvm</tag>
        <tag>problems</tag>
        <tag>ini</tag>
      </tags>
  </entry>
  <entry>
    <title>javascript 解构语法</title>
    <url>/2018/05/31/javascript-destructuring-assignment/</url>
    <content><![CDATA[<p>具体可参见 <a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment" target="_blank" rel="noopener">mdn javascript 解构语法</a>. 这里简单总结一下。</p>
<h1 id="解构是做什么的"><a href="#解构是做什么的" class="headerlink" title="解构是做什么的"></a>解构是做什么的</h1><p>解构就是一种方便变量赋值的语法，由编译器完成真正的变量赋值</p>
<h1 id="数组解构"><a href="#数组解构" class="headerlink" title="数组解构"></a>数组解构</h1><ul>
<li><strong>将数组元素赋值给变量</strong></li>
<li><strong>赋值依据是元素顺序</strong></li>
<li>指定变量名时，可以提供默认值，以避免 <code>undefined</code> 赋值</li>
<li>支持忽略一些元素（添加 <code>,</code> ，但不提供变量名）</li>
<li>支持     <code>rest</code> 数组赋值</li>
</ul>
<p>eg.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基本赋值</span></span><br><span class="line"><span class="keyword">var</span> a, b, rest;</span><br><span class="line">[a, b] = [<span class="number">10</span>, <span class="number">20</span>];</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 10</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 20</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认值</span></span><br><span class="line"><span class="keyword">var</span> a, b;</span><br><span class="line"></span><br><span class="line">[a=<span class="number">5</span>, b=<span class="number">7</span>] = [<span class="number">1</span>];</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 1</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 忽略某些元素</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> [a, , b] = f();</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 1</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// rest 赋值</span></span><br><span class="line">[a, b, ...rest] = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>];</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 10</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 20</span></span><br><span class="line"><span class="built_in">console</span>.log(rest); <span class="comment">// [30, 40, 50]</span></span><br></pre></td></tr></table></figure>
<h1 id="对象解构"><a href="#对象解构" class="headerlink" title="对象解构"></a>对象解构</h1><p>和数组解构差不多：</p>
<ul>
<li><strong>将对象属性赋值给变量</strong></li>
<li><strong>赋值依据是属性名称</strong></li>
<li>指定属性名时，可以给默认值，避免 <code>undefined</code> 赋值</li>
<li>变量名称默认是属性名，也可以自定义，通过 <code>propertyName: customName</code> 方式定义</li>
<li>支持 <code>rest</code> 对象赋值（在 proposal 中）</li>
</ul>
<p>eg.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基本赋值</span></span><br><span class="line">(&#123; a, b &#125; = &#123; <span class="attr">a</span>: <span class="number">10</span>, <span class="attr">b</span>: <span class="number">20</span> &#125;);</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 10</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 20</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认值</span></span><br><span class="line"><span class="keyword">var</span> &#123;a = <span class="number">10</span>, b = <span class="number">5</span>&#125; = &#123;<span class="attr">a</span>: <span class="number">3</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 3</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义变量名 + 默认值</span></span><br><span class="line"><span class="keyword">var</span> o = &#123;<span class="attr">p</span>: <span class="number">42</span>, <span class="attr">q</span>: <span class="literal">true</span>&#125;;</span><br><span class="line"><span class="keyword">var</span> &#123;<span class="attr">p</span>: foo, <span class="attr">q</span>: bar, <span class="attr">m</span>: other=<span class="string">'haha'</span>&#125; = o;</span><br><span class="line"> </span><br><span class="line"><span class="built_in">console</span>.log(foo); <span class="comment">// 42 </span></span><br><span class="line"><span class="built_in">console</span>.log(bar); <span class="comment">// true</span></span><br><span class="line"><span class="built_in">console</span>.log(other); <span class="comment">// haha</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// rest 赋值</span></span><br><span class="line"><span class="comment">// Stage 3 proposal</span></span><br><span class="line">(&#123;a, b, ...rest&#125; = &#123;<span class="attr">a</span>: <span class="number">10</span>, <span class="attr">b</span>: <span class="number">20</span>, <span class="attr">c</span>: <span class="number">30</span>, <span class="attr">d</span>: <span class="number">40</span>&#125;);</span><br><span class="line"><span class="built_in">console</span>.log(a); <span class="comment">// 10</span></span><br><span class="line"><span class="built_in">console</span>.log(b); <span class="comment">// 20</span></span><br><span class="line"><span class="built_in">console</span>.log(rest); <span class="comment">//&#123;c: 30, d: 40&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 无声明赋值</span></span><br><span class="line"><span class="comment">// 相当于 var &#123;a,b&#125; = &#123;a:1, b:2&#125;;</span></span><br><span class="line"><span class="comment">// 括号去掉， &#123;a,b&#125; 是块代码，不是对象，所以不能去掉</span></span><br><span class="line"><span class="keyword">var</span> a, b;</span><br><span class="line"></span><br><span class="line">(&#123;a, b&#125; = &#123;<span class="attr">a</span>: <span class="number">1</span>, <span class="attr">b</span>: <span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>java stream parallel 有时比 sequential 还慢？</title>
    <url>/2022/10/22/java-stream-parallel/</url>
    <content><![CDATA[<h1 id="为什么-java-stream-parallel-有时比-sequential-执行还慢？"><a href="#为什么-java-stream-parallel-有时比-sequential-执行还慢？" class="headerlink" title="为什么 java stream parallel 有时比 sequential 执行还慢？"></a>为什么 java stream parallel 有时比 sequential 执行还慢？</h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>考虑下边的代码，并行执行不一定比顺序执行快，甚至很多时候都是更慢的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">should_not_sure_if_without_warm_up</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String[] array = <span class="keyword">new</span> String[<span class="number">1000000</span>];</span><br><span class="line">        Arrays.fill(array, <span class="string">"AbabagalamagA"</span>);</span><br><span class="line">        System.out.println(<span class="string">"Benchmark..."</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; ++i) &#123;</span><br><span class="line">            System.out.printf(<span class="string">"Run %d:  sequential %s  -  parallel %s\n"</span>,</span><br><span class="line">                    i,</span><br><span class="line">                    test(() -&gt; sequential(array)),</span><br><span class="line">                    test(() -&gt; parallel(array)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sequential</span><span class="params">(String[] array)</span> </span>&#123;</span><br><span class="line">        Arrays.stream(array).map(String::toLowerCase).collect(Collectors.toList());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">parallel</span><span class="params">(String[] array)</span> </span>&#123;</span><br><span class="line">        Arrays.stream(array).parallel().map(String::toLowerCase).collect(Collectors.toList());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">test</span><span class="params">(Runnable runnable)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        runnable.run();</span><br><span class="line">        <span class="keyword">long</span> elapsed = System.currentTimeMillis() - start;</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"%4.2fs"</span>, elapsed / <span class="number">1000.0</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h2><p>有几个原因（<a href="http://www.theserverside.com/definition/just-in-time-compiler-JIT" target="_blank" rel="noopener">stackoverflow</a>）：</p>
<ol>
<li><strong>stream 的并行执行比串行执行要做更多的事</strong>。并行执行需要拆分程序，使得程序可以并行执行，最后要合并结果。例如，上述并行执行涉及到 new 线程池、分配线程执行特定的 string 操作并加到一个 list、最终合并 list。这个程序本身已经执行很快，此时，这些额外开销比本身执行的时间可能还要长，就影响了它最终带来的性能。</li>
<li><strong>编译器、jvm、GC 等会影响代码执行效率，因此对 java 做这些基准测试很微妙</strong>。例如 <a href="http://www.theserverside.com/definition/just-in-time-compiler-JIT" target="_blank" rel="noopener">JIT compiler</a>、GC 等就会很大程度的影响测试结果。<ol start="3">
<li>测试很大程度受 JIT compiler 执行的影响<ol start="4">
<li>在 JIT compiler 完成之前，可能测试已经跑完了。此时顺序执行和并行执行哪个 JIT compiler 先跑完，可能测试就会跑的更快一些</li>
<li>而且 JIT compiler 什么时候开始跑也不确定。</li>
<li>并且 JIT compiler 会做一些运行时优化，比如有些代码，其输出没有在任何地方被使用，JIT compiler 会直接消除这些代码的执行。这种情况还是非常容易发生的。此时，你这些测试衡量就更微妙了，因为可能最终执行的测试并不是你所写的测试，而是优化之后的。</li>
<li>如果在测试执行之前，加上一些预热，就可以保证程序都已经再编译完成，此时评估的就是同等条件下的程序执行效率了（参见下边的 code）。</li>
</ol>
</li>
<li>GC 会影响执行效率，不同的代码会产生不同的 eliminated objects<ol start="5">
<li>stream、并行运行等会涉及到很多中间变量的构建、copy 等，比如中间 string、list 等，这时 GC 执行工作量就比较大，会影响最终的测试执行时间，使得测试结果也不可信。</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>对 java 做这些基准测试，有时结果会比较 confusing，所以建议采用专门的 benchmark 框架来做基准测试，比如 <a href="http://openjdk.java.net/projects/code-tools/jmh/" target="_blank" rel="noopener">JMH</a>，这框架执行过程中，可以看到很多 java 额外执行的一些操作时间等，就可以更好的观察测试结果了。        </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 更改测试，加上预热，保证 JIT 编译已完成，此时基本是在同等条件下测试，测试结果相对更可信一些</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">should_parallel_faster_if_has_warm_up</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String[] array = <span class="keyword">new</span> String[<span class="number">1000000</span>];</span><br><span class="line">    Arrays.fill(array, <span class="string">"AbabagalamagA"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Warmup..."</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">        sequential(array);</span><br><span class="line">        parallel(array);</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"Benchmark..."</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; ++i) &#123;</span><br><span class="line">        System.out.printf(<span class="string">"Run %d:  sequential %s  -  parallel %s\n"</span>,</span><br><span class="line">                i,</span><br><span class="line">                test(() -&gt; sequential(array)),</span><br><span class="line">                test(() -&gt; parallel(array)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="什么是-JIT-compiler"><a href="#什么是-JIT-compiler" class="headerlink" title="什么是 JIT compiler"></a>什么是 <a href="http://www.theserverside.com/definition/just-in-time-compiler-JIT" target="_blank" rel="noopener">JIT compiler</a></h3><p>JIT (just-in-time) compiler 指在运行时执行的编译器。</p>
<p>(1) java 是编译成字节码，然后在运行时解释执行的</p>
<p>c、C++ 等编程语言都是直接编译成机器码，可以在机器上直接执行的。但是不同平台处理器有差异，导致用户可能需要为不同平台写多套程序。</p>
<p>java 就提出了 JVM，将代码一次编译成字节码，然后提供不同的 JVM，JVM 会将字节码解释执行为可运行的机器码。</p>
<p>但是解释执行是一行一行做的，就影响了执行效率。这也是为啥 c++ 等会诟病 java 很慢的原因。</p>
<p>(2) 为了提高解释执行的效率，使用了 JIT compiler</p>
<p>正如上文所说，因为解释执行慢，所以在程序运行起来后，同时会执行 JIT compiler，将字节码编译成可执行代码（相当于二次编译）。这就可以一定程度的加快解释执行的效率。而且 JIT compiler 因为可以获取运行时环境、参数等，所以可以做更多的优化</p>
<h1 id="parallel-慎用？？？"><a href="#parallel-慎用？？？" class="headerlink" title="parallel 慎用？？？"></a>parallel 慎用？？？</h1><p><a href="https://dzone.com/articles/think-twice-using-java-8" target="_blank" rel="noopener">DZone: parallel 慎用</a> 说因为 stream 公用线程池，一个 broken thread 会影响所有 healthy 线程的执行，所以要慎用。</p>
<p>简单看了一些，比如这个 <a href="https://stackoverflow.com/questions/20375176/should-i-always-use-a-parallel-stream-when-possible" target="_blank" rel="noopener">stackoverflow</a>，应该是说 stream 提供了方便的形式去写 function、可读性高、promote 大家写出 side-effects-free 的代码，但是 stream 本身还是有很多缺陷的。</p>
<p>公用线程池的测试代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">should_be_influenced_by_long_tasks</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">/** Simulating multiple threads in the system</span></span><br><span class="line"><span class="comment">    * if one of them is executing a long-running task.</span></span><br><span class="line"><span class="comment">    * Some of the other threads/tasks are waiting</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">int</span> MAX = <span class="number">12</span>;</span><br><span class="line">    ExecutorService es = Executors.newCachedThreadPool();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这个线程执行很慢，但是因为共享线程池，因此会影响其他线程的执行。极端情况，这里是一个 broken tread，其他 healthy thread 都会受影响</span></span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行结果不确定，因为有上边的长线程。如果注释掉上边线程，下边这个可以很快执行</span></span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">0</span>));</span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">0</span>));</span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">0</span>));</span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">0</span>));</span><br><span class="line">    es.execute(() -&gt; countPrimes(MAX, <span class="number">0</span>));</span><br><span class="line">    es.shutdown();</span><br><span class="line">    es.awaitTermination(<span class="number">60</span>, TimeUnit.SECONDS);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">countPrimes</span><span class="params">(<span class="keyword">int</span> max, <span class="keyword">int</span> delay)</span> </span>&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getId() + <span class="string">": "</span> + range(<span class="number">1</span>, max).parallel().filter(<span class="keyword">this</span>::isPrime).peek(i -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            sleep(delay);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).count());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n &gt; <span class="number">1</span> &amp;&amp; rangeClosed(<span class="number">2</span>, (<span class="keyword">long</span>) sqrt(n)).noneMatch(divisor -&gt; n % divisor == <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="ForkJoinPool"><a href="#ForkJoinPool" class="headerlink" title="ForkJoinPool"></a><a href="http://tutorials.jenkov.com/java-util-concurrent/java-fork-and-join-forkjoinpool.html" target="_blank" rel="noopener">ForkJoinPool</a></h2><p><a href="https://www.jianshu.com/p/bd825cb89e00" target="_blank" rel="noopener">这个文章</a> 介绍了 ForkJoinPool，说是 parallel stream 实现的主要原理和背后手段</p>
<p>stream 的并发执行现在基本上都是采用分治法，先拆分用多线程逐个处理，然后再合并结果。最后的合并操作必须在前边某几个线程执行完之后才做。</p>
<p>而普通的线程池 <a href="">ThreadPoolExecutor</a> 就是构建一个线程池，并发执行，但是它没办法决定线程执行的父子关系。</p>
<p>ForkJoinPool 就是为了解决上述问题而存在，它可以让子任务并发执行完成之后，才开始执行父任务。除此以外，和 ThreadPoolExecutor 一样，都是用一个无限队列来保存待执行的任务。</p>
<p>ForkJoinPool 采用了一个通用线程池，实现了 <strong><a href="http://ifeve.com/talk-concurrency-forkjoin/" target="_blank" rel="noopener">工作窃取</a></strong>。工作窃取指某个线程从其他队列里窃取任务来执行。ForkJoinPool 就可以？？？？？</p>
<h1 id="什么时候用-parallel"><a href="#什么时候用-parallel" class="headerlink" title="什么时候用 parallel"></a>什么时候用 parallel</h1><p>目前来说，在 java 中：</p>
<ol>
<li>如果是数据量很大的操作，可以考虑用 parallel</li>
<li>如果有性能问题，再考虑用 parallel</li>
<li>如果确实有多核，再考虑用</li>
<li>如果确实是无 side effect 的函数，才可以考虑用</li>
<li>如果已经有其他并行措施，可以不用 parallel</li>
<li>如果数据操作很慢，慎用（可能 block 其他 thread）</li>
<li>如果数据操作很快，也慎用（可能这个时候用并行的额外开销会超过它所能带来的优势）</li>
</ol>
<p><a href="https://blog.oio.de/2016/01/22/parallel-stream-processing-in-java-8-performance-of-sequential-vs-parallel-stream-processing/" target="_blank" rel="noopener">这篇文章</a>也对比了并行和串行 stream，然后画了个决策象限图，如下图所示：</p>
<p><img src="https://blog.oio.de/wp-content/uploads/2016/01/stream_performance_image3.png" alt="parallel 决策象限图"></p>
<p>跟上边类似，关注下边四个方面：</p>
<ol>
<li><code>number_of_elements * cost_per_element</code> 比较大。这可以比较好的解决这种状况：每个元素运行很快时，如果数据量大就可以用；如果每个元素运行稍费时些，即使数据量不那么大，也 ok。但是应该要避免过于费时的那些场景，见上边的分析。</li>
<li>source collection 可以很高效的被拆分（这样才方便拆线程处理）</li>
<li>每个元素的函数执行是独立的（这才可以并行处理，即并行首先要求 side effect free）</li>
<li>多核</li>
</ol>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>javascript 表达式和操作符</title>
    <url>/2018/05/31/javascript-spread-operator/</url>
    <content><![CDATA[<h1 id="扩展运算符"><a href="#扩展运算符" class="headerlink" title="... 扩展运算符"></a><code>...</code> 扩展运算符</h1><p><code>...obj</code> 是 js 的扩展运算符，可以将一个可迭代的对象在函数调用的位置展开成为多个参数,或者在数组字面量中展开成多个数组元素。(其他可参见<a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Guide/Expressions_and_Operators#Relational_operators" target="_blank" rel="noopener">运算符介绍</a>，<a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators" target="_blank" rel="noopener">运算符和表达式清单 reference</a>)</p>
<p>eg.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在数组字面量中展开</span></span><br><span class="line"><span class="comment">// 利用扩展运算符，实现了数组合并</span></span><br><span class="line"><span class="keyword">var</span> parts = [<span class="string">'shoulder'</span>, <span class="string">'knees'</span>];</span><br><span class="line"><span class="keyword">var</span> lyrics = [<span class="string">'head'</span>, ...parts, <span class="string">'and'</span>, <span class="string">'toes'</span>];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 在函数调用处展开</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params">x, y, z</span>) </span>&#123; &#125;</span><br><span class="line"><span class="keyword">var</span> args = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>];</span><br><span class="line">f(...args);</span><br></pre></td></tr></table></figure>
<h1 id="template-literals"><a href="#template-literals" class="headerlink" title=" template literals"></a><code></code> template literals</h1><p><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals" target="_blank" rel="noopener">template literals</a></p>
<p><a href="https://www.npmjs.com/package/sql-template-strings" target="_blank" rel="noopener">sql template strings</a></p>
]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>js ecosystem</title>
    <url>/2018/06/21/js-ecosystem-md/</url>
    <content><![CDATA[<p>我在学习 react，一直在使用 <a href="https://github.com/facebook/create-react-app" target="_blank" rel="noopener">create-react-app</a> 创建项目。create-react-app 其实包括两个核心：</p>
<ul>
<li><code>create-react-app</code>：主要提供了 command-line 工具，方便用户创建 react 项目</li>
<li><code>react-scripts</code>：这才是核心。它封装了所有开发 react 项目的配置，使得用户可以零配置直接开始开发 react。用户基本不需要更新 <code>create-react-app</code>，因为它总是拉最新的 <code>react-scripts</code>，而 <code>react-scripts</code> 才是简化用户配置的核心。</li>
</ul>
<p>问题来了，我在写测试的时候发现，<code>react-scripts</code> 默认配置使用的是 jest，而且版本较低（可运行 <code>npm ls jest</code> 查看依赖树，结果如下图所示）。而我需要使用的 data-driven-test 依赖包 <code>jest-each</code> 是 jest 23.0.0 以后的版本才有的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-cc0aae2294d01d0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="react-scripts-dependency.png"></p>
<p>所以我需要在测试的时候不使用默认安装的 jest，</p>
<p>读了一个 <a href="https://medium.com/@francesco.agnoletto/i-didnt-like-create-react-app-so-i-created-my-own-boilerplate-190a7dd5d74" target="_blank" rel="noopener">post: 自己写样板，不使用 create-react-app</a>，受它的启发要自己配置 react 项目，那么就有必要了解 js ecosystem 中的一些工程。</p>
<h1 id="webpack"><a href="#webpack" class="headerlink" title="webpack"></a><a href="https://www.webpackjs.com/concepts/" target="_blank" rel="noopener">webpack</a></h1><p>一个开源的前端打包工具，支持用户进行模块化开发（即用户开发很多 module，然后不同的 mudule 之间可通过 import, export 进行相互引用）。原本 js 是不支持的，可参见 <a href="https://hfcherish.github.io/2018/06/21/js-global-namespace/" target="_blank" rel="noopener">js global namespace</a>。</p>
<h1 id="ESLint"><a href="#ESLint" class="headerlink" title="ESLint"></a><a href="http://eslint.cn/docs/user-guide/configuring" target="_blank" rel="noopener">ESLint</a></h1><p>是一个 javascript 的语法检查器。类似于在 IDE 中写 java 时的即时编译检查。它是可以配置的，以完成你所需要的检查。</p>
<h1 id="ECMAScript"><a href="#ECMAScript" class="headerlink" title="ECMAScript"></a><a href="https://zh.wikipedia.org/wiki/ECMAScript" target="_blank" rel="noopener">ECMAScript</a></h1><p><a href="https://huangxuan.me/2015/09/22/js-version/" target="_blank" rel="noopener">ES5, ES6, ES2016, ES.Next: What’s going on with JavaScript versioning?</a> 翻译的这个文章讲得很好，我直接引用原文：</p>
<blockquote>
<ul>
<li><strong>ECMAScript</strong>：一个由 ECMA International 进行标准化，TC39 委员会进行监督的语言。通常用于指代标准本身。</li>
<li><strong>JavaScript</strong>：ECMAScript 标准的各种实现的最常用称呼。这个术语并不局限于某个特定版本的 ECMAScript 规范，并且可能被用于任何不同程度的任意版本的 ECMAScript 的实现。</li>
<li><strong>ECMAScript 5 (ES5)</strong>：ECMAScript 的第五版修订，于 2009 年完成标准化。这个规范在所有现代浏览器中都相当完全的实现了。</li>
<li><strong>ECMAScript 6 (ES6) / ECMAScript 2015 (ES2015)</strong>：ECMAScript 的第六版修订，于 2015 年完成标准化。这个标准被部分实现于大部分现代浏览器。可以查阅这张兼容性表来查看不同浏览器和工具的实现情况。</li>
<li><strong>ECMAScript 2016</strong>：预计的第七版 ECMAScript 修订，计划于明年夏季发布。这份规范具体将包含哪些特性还没有最终确定</li>
<li><strong>ECMAScript Proposals</strong>：被考虑加入未来版本 ECMAScript 标准的特性与语法提案，他们需要经历五个阶段：Strawman（稻草人），Proposal（提议），Draft（草案），Candidate（候选）以及 Finished （完成）。</li>
</ul>
</blockquote>
<p>blog 里也讲了 ecmascript 的历史。简答总结一下：</p>
<ol>
<li>很久以前（1996），网景浏览器把他们写的 javascript 交给 ECMA International（欧洲计算机制造协会）进行标准化</li>
<li>ECMA 1，2，3 版本很快发布，而且被各大浏览器厂商支持</li>
<li>ECMA 一直没有变化，各大浏览器厂商自行扩展（所以 javascript 其实是 ECMAScript 的实现和扩展）</li>
<li>某一年，ECMA 4 出来，太激进，被废弃了（只有 Adobe 实现了）</li>
<li>2009 年，ECMA 5（es5） 出来。但直到 2012 年才逐渐被公众接受</li>
<li>2015 年，es6 出来。与此同时，相关委员会决定每年定义一次新标准，避免等待整个草案完成。因此 ES6 也被命名为 ECMAScript 2015</li>
</ol>
<p>一些资源：</p>
<ul>
<li>如果你还不熟悉 ES6，Babel 有一个<a href="https://babeljs.io/docs/learn-es2015/" target="_blank" rel="noopener">很不错的特性概览</a></li>
<li>如果你希望深入 ES6，这里有两本很不错的书： Axel Rauschmayer 的 <a href="http://exploringjs.com/" target="_blank" rel="noopener">Exploring ES6</a>和 Nicholas Zakas 的 <a href="https://leanpub.com/understandinges6" target="_blank" rel="noopener">Understanding ECMAScript 6</a>。Axel 的博客 <a href="http://www.2ality.com/" target="_blank" rel="noopener">2ality</a> 也是很不错的 ES6 资源</li>
</ul>
<h1 id="babel"><a href="#babel" class="headerlink" title="babel"></a><a href="https://www.babeljs.cn/" target="_blank" rel="noopener">babel</a></h1><p>babel 是一个 javascript 编译器。其核心是一个语法转换器，能将使用 es6、flow、jsx 的代码转换为浏览器兼容的 javascript。所以这些项目 build 后其实是生成了浏览器兼容的 javascript。</p>
<p>为啥会有这个，原因就是上边说的，新版本 js 出来了，但是大家还不支持。</p>
]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>js export</title>
    <url>/2018/06/15/js-export/</url>
    <content><![CDATA[<p><a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/export" target="_blank" rel="noopener">export</a> 用于从 module 中导出函数、对象、原始值，在其他地方通过 <code>import</code> 使用这些函数、对象、原始值。</p>
<p>有两种导出方式：命名导出（<code>export</code>），默认导出（<code>export default</code>）</p>
<h1 id="1-命名导出"><a href="#1-命名导出" class="headerlink" title="1. 命名导出"></a>1. 命名导出</h1><p>就是导出 module 中有命名的函数、对象、原始值。相应的，import 时，必须使用相同的命名引入（当然可以使用 <code>import a as b from &#39;./module&#39;</code> 来修改名称）</p>
<p>举例：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义时导出</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> a = <span class="string">'a'</span>, b = <span class="string">'b'</span>; <span class="comment">// 适用于 var, let</span></span><br><span class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">a</span>(<span class="params"></span>)</span>&#123;&#125;;</span><br><span class="line"><span class="keyword">export</span> <span class="class"><span class="keyword">class</span> <span class="title">a</span></span>&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义后导出</span></span><br><span class="line"><span class="keyword">const</span> a = <span class="string">'a'</span>, b = <span class="string">'b'</span>;</span><br><span class="line"><span class="keyword">export</span> &#123;a, b <span class="keyword">as</span> newB&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导出其他模块的导出。</span></span><br><span class="line"><span class="comment">// 此时仅会导出 otherModule 的命名导出，所以这里最终导出的还是有命名的</span></span><br><span class="line"><span class="keyword">export</span> * <span class="keyword">from</span> <span class="string">'otherModule'</span>;</span><br><span class="line"><span class="keyword">export</span> &#123;a, b <span class="keyword">as</span> newB&#125; <span class="keyword">from</span> <span class="string">'otherModule'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果需要导出 otherModule 的默认导出，只能这样写：</span></span><br><span class="line"><span class="keyword">import</span> defaultExport <span class="keyword">from</span> <span class="string">'otherModule'</span>;</span><br><span class="line"><span class="keyword">export</span> [<span class="keyword">default</span>] defaultExport;</span><br></pre></td></tr></table></figure>
<h1 id="2-默认导出"><a href="#2-默认导出" class="headerlink" title="2. 默认导出"></a>2. 默认导出</h1><p>默认导出（<code>export default</code>）的函数、类可以认为没有固定名称，即 import 时，可以用任何名称导入</p>
<p>一个 module 中只能有一个默认导出。</p>
<p><code>export default</code> 后不能跟 <code>const</code>, <code>let</code>, <code>var</code></p>
<p>举例：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认导出函数（定义时）</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;&#125;</span><br><span class="line"><span class="keyword">import</span> myname <span class="keyword">from</span> <span class="string">'module'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="function"><span class="keyword">function</span> <span class="title">funcName</span>(<span class="params"></span>)</span>&#123;&#125;</span><br><span class="line"><span class="keyword">import</span> funcNameMy <span class="keyword">from</span> <span class="string">'module2'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认导出类（定义时）</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">className</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认导出 expression</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> a = <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认导出（定义后）</span></span><br><span class="line"><span class="keyword">export</span> &#123;a <span class="keyword">as</span> <span class="keyword">default</span>, b, c <span class="keyword">as</span> newC&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>js global namespace</title>
    <url>/2018/06/21/js-global-namespace/</url>
    <content><![CDATA[<ol>
<li>最基本的js写法是：不管是要调用项目内或项目外的其他文件的方法，都是直接在当前文件中调用，然后在web文件（html）中利用 <code>&lt;script&gt;</code> 按依赖顺序引入所有的内部和外部文件。<ul>
<li>即每个js中声明的变量都是全局变量。js采用{}来定义变量生命周期（或者说namespace），除了被｛｝所包围的其余变量都是全局变量。有个god object即window－－－全局对象，所有的变量、函数都是这个god object的member－－－全局变量。</li>
<li>利用 <code>&lt;script&gt;</code> 引入所有js，效果类似于将所有文件的内容组装到一个大文件运行。因此声明顺序受依赖关系约束。</li>
<li>即browser运行每个html时，它始终是将这个html中的所有script作为一个文件来运行。即browser是个解释执行器，它总是执行一个文件。而不同于后端（例如java 的jre）的编译运行，</li>
</ul>
</li>
<li>前后端的区别：<ul>
<li>browser是解释执行器，而js不提供import机制，所以只能人工解决依赖确定关系－－－需要提供一种机制来解决这中依赖确定关系</li>
<li>后端执行是通过server来执行的，服务器会下载所需lib存放到服务器容器中，运行是直接从容器拿。而前端执行是通过browser来执行，每次执行js browser都需要下载依赖的各种js文件，然后将这些文件组合成一个来运行。当然有jquery之类的会利用缓存使得不需要每次执行都下载，然而这依然不是一个最佳解决方案－－－需要提供一种机制来解决依赖下载和组装。</li>
</ul>
</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>前端</th>
<th>后端</th>
</tr>
</thead>
<tbody>
<tr>
<td>开发生命周期</td>
<td>1. code<br> 2. 简单组合所有文件<br> 3. 解释一行为可执行码－－即时性<br> 4. 执行一行</td>
<td>1. code（提供import）<br> 2. 编译：确定依赖关系，据此依次编译各个文件为汇编码／机器码（其中有预编译、预处理等步骤）<br> 3. 连接：将外部函数代码添加到上述文件中，组合成一个完整可执行文件。<br> 4. 运行整个文件</td>
</tr>
</tbody>
</table>
]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s general</title>
    <url>/2019/03/01/k8s/</url>
    <content><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/" target="_blank" rel="noopener">k8s</a> is a platform to manage containerized workloads and services.</p>
<h1 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h1><h2 id="kubernetes-Objects"><a href="#kubernetes-Objects" class="headerlink" title="kubernetes Objects"></a>kubernetes Objects</h2><ol>
<li>Kubernetes abstract <strong>a desired state of cluster</strong> as objects.</li>
<li>an object configuration includes:<ol>
<li>spec: describe the desired state<ol>
<li>apiVersion: the api version of kubernetes</li>
<li>metadata: the name &amp; namespace</li>
<li>spec: the desired state definition.</li>
</ol>
</li>
<li>status: describe the actual state of the object</li>
</ol>
</li>
<li>cluster state (<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" rel="noopener">understanding kubernetes objects</a>):<ol>
<li>what containerized applications are running and where they’re running</li>
<li>how many resources (disk, network, etc.) are attached to the the container</li>
<li>the policies around how the application behaves, such as restart policies, fault-tolerance, etc.</li>
</ol>
</li>
</ol>
<h3 id="Workloads"><a href="#Workloads" class="headerlink" title="Workloads"></a>Workloads</h3><p>Objects that set deployment rules of pods.</p>
<p>All controllers are workloads.</p>
<h3 id="POD"><a href="#POD" class="headerlink" title="POD"></a>POD</h3><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener">pod</a> is <strong>a minimize runnable object</strong> in k8s object model.</p>
<p>A Pod encapsulates an application <em>container</em> (or, in some cases, multiple containers), <em>storage resources</em>, <em>a unique network IP</em>, and <em>options that govern how the container(s) should run.</em></p>
<p>It represents <strong>a single instances of application</strong>.</p>
<p>There’re two common use cases:</p>
<ul>
<li><strong>Pods that run a single container</strong>, which is the most case. So often pod is a synomynous with container.</li>
<li><strong>Pods that run multiple containers that need to work together.</strong> All the containers in a pod share the storage &amp; network, which means they use the same ip and same storage volume.</li>
</ul>
<h4 id="Pod-lifecycle"><a href="#Pod-lifecycle" class="headerlink" title="Pod lifecycle"></a>Pod lifecycle</h4><p><strong>A pod doesn’t self-heal &amp; self-scale</strong>. It’s just a running instance. It stopped &amp; deleted when the process is finished, or the node is failed, or the resource is exhaused… Controllers (eg. deployment, statefulSet…) instead can create and manage multiple pods.</p>
<p>Pod restarting is different from container restarting. <strong>Pod provides the env for containers: os, storage, network…</strong></p>
<h3 id="Controllers"><a href="#Controllers" class="headerlink" title="Controllers"></a>Controllers</h3><p>Controllers (eg. deployment, statefulSet…) instead can create and manage multiple pods.</p>
<h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>Service is an <strong>abstraction</strong> which defines <strong>a logical set of pods</strong>, and <strong>a policy by which to access the pods</strong>.</p>
<p>It enables <strong>the decoupling of access &amp; the real pods</strong>, which means you can access by service name/ip rather than the pod ip, while <strong>pod ip is not stable</strong>.</p>
<blockquote>
<p>Q: How does k8s get all pod endpoints by service?</p>
<p>A: By <code>LabelSelector</code>. You can define labels for each pod. And we define <code>LabelSelector</code> in service, so that k8s can search pod node by label first, and then using the <code>targetPort</code> to locate the pod on node.</p>
</blockquote>
<h2 id="kubernetes-control-plane"><a href="#kubernetes-control-plane" class="headerlink" title="kubernetes control plane"></a>kubernetes control plane</h2><ol>
<li>the control plane <strong>manage the cluster state to match the desired state of objects</strong></li>
<li>the control plane consists of a collection of processes for the above intention.<ol>
<li>kubernetes master:<ol>
<li>is responsible for the maintaining the desired state.</li>
<li>“master” in fact refers to three processes: <a href="https://kubernetes.io/docs/admin/kube-apiserver/" target="_blank" rel="noopener">kube-apiserver</a>, <a href="https://kubernetes.io/docs/admin/kube-controller-manager/" target="_blank" rel="noopener">kube-controller-manager</a> and <a href="https://kubernetes.io/docs/admin/kube-scheduler/" target="_blank" rel="noopener">kube-scheduler</a>.</li>
<li>these three processes are typically run on single node in the cluster. This node is called “master”, too. The master can also be replicated for avaibility and redundancy.</li>
</ol>
</li>
<li>kubernetes nodes:<ol>
<li>the nodes to run applications.</li>
<li>there’re two process in each node:<ol>
<li>kubelet: communicate with the master</li>
<li>kube-proxy: a network proxy.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="kubernetes-api"><a href="#kubernetes-api" class="headerlink" title="kubernetes api"></a>kubernetes api</h1><p>The <strong>api</strong> communicates with master to operate on kubernetes objects.</p>
<p><strong>kubectl</strong> is a cli to implement the above intention. It in fact calls the api internally.</p>
<p>All kinds of <strong>sdk</strong> (java, python…) encapsulate the api, too.</p>
<p>There are two kinds of api groups:</p>
<ol>
<li>core groups: the original k8s api</li>
<li>other groups: other api to extend the core group, like you may want to abstract more objects?</li>
</ol>
<h1 id="Access-services-on-cluster"><a href="#Access-services-on-cluster" class="headerlink" title="Access services on cluster"></a>Access services on cluster</h1><p>There are several levels.</p>
<p>ip:</p>
<ul>
<li>pod ip: </li>
<li>cluster ip: the virtual ip for a service</li>
<li>node ip</li>
</ul>
<h2 id="In-pod-access"><a href="#In-pod-access" class="headerlink" title="In-pod access"></a>In-pod access</h2><p><strong>Each pod has a unique IP</strong>.  And all <strong>containers in a pod share the ip</strong>, which means that they can access each other by <code>localhost</code></p>
<h2 id="In-cluster-access"><a href="#In-cluster-access" class="headerlink" title="In-cluster access"></a>In-cluster access</h2><p>By default, a service can be accessed by other pods in the same cluster, through:</p>
<h4 id="cluster-ip-port"><a href="#cluster-ip-port" class="headerlink" title="cluster-ip:port"></a>cluster-ip:port</h4><ul>
<li>cluster ip is the virtual ip of a service</li>
<li>port is is the node port of the service.</li>
</ul>
<p>Every node on k8s has a <code>kube-proxy</code>. It installs iptable rules which keep a simple record of (servicename:clusterip:serviceport).</p>
<p>In some node:</p>
<ol>
<li>call cluster-ip:port————— <strong>In a pod</strong></li>
<li>the kube-proxy search the iptable, and get some info.———— <strong>pod =&gt; kube-proxy of node where the pod resides.</strong></li>
<li>the kube-proxy using the info to ask master for the real endpoints.—— <strong>kube-proxy of node =&gt; master kube-api</strong></li>
<li>the kube-proxy chooses a endpoint by <code>SessionAffinity</code> defined in service (round-robin by default) —— <strong>in kube-proxy</strong></li>
<li>the kube-proxy redirect request to <code>pod-ip:podPort</code> —— <strong>kube-proxy =&gt; pod endpoint</strong></li>
</ol>
<p>kube-proxy: enables the Kubernetes service abstraction by maintaining network rules on the host and performing connection forwarding</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/e351b830334b8622a700a8da6568cb081c464a9b/13020/images/docs/services-userspace-overview.svg" alt="proxy-mode:user space"></p>
<h4 id="servicename-namespacename"><a href="#servicename-namespacename" class="headerlink" title="servicename.namespacename"></a>servicename.namespacename</h4><p>When accessing by service name, there’re two ways:</p>
<h5 id="environement-viaribles"><a href="#environement-viaribles" class="headerlink" title="environement viaribles"></a>environement viaribles</h5><p>when create a service, k8s will create some env for each serivce, e.g. <code>{SVCNAME_CAPTIPAL}_SERVICE_HOST</code>. So the kube-proxy in fact gets the service cluster ip from these envs first. When a pod calls a service, t<strong>he service must be created before the pod</strong> so that the envs are created.</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># the cluster ip</span></span><br><span class="line"><span class="attr">REDIS_MASTER_SERVICE_HOST</span>=<span class="string">10.0.0.11</span></span><br><span class="line"><span class="attr">REDIS_MASTER_SERVICE_PORT</span>=<span class="string">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT</span>=<span class="string">tcp://10.0.0.11:6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP</span>=<span class="string">tcp://10.0.0.11:6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_PROTO</span>=<span class="string">tcp</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_PORT</span>=<span class="string">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_ADDR</span>=<span class="string">10.0.0.11</span></span><br></pre></td></tr></table></figure>
<h5 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a><a href="https://kubernetes.io/docs/concepts/services-networking/service/#dns" target="_blank" rel="noopener">DNS</a></h5><p>It’s an add-on k8s object, which can be chosed to add to the cluster.</p>
<p>It’s an in-cluster dns, which serves DNS records for Kubernetes services.</p>
<p>It watches the k8s api for new services and create <strong>DNS SVC records</strong> for each.</p>
<p>Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p>
<h2 id="between-cluster-access"><a href="#between-cluster-access" class="headerlink" title="between-cluster access"></a>between-cluster access</h2><ol>
<li>NodePort: expose nodeIP</li>
<li>LoadBalancer: expose loadbalancer ip</li>
<li>Ingress: expose loadbalancer &amp; path</li>
</ol>
]]></content>
      <tags>
        <tag>container</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>mime type</title>
    <url>/2018/07/26/mime-type/</url>
    <content><![CDATA[<p><a href="MIME 类型">MIME 类型</a> 是用一种标准化的方式来表示文档的性质和格式。浏览器一般通过 MIME 类型（而不是文档扩展名）来确定如何处理文档。因此服务器传输数据时，必须设置正确的 MIME 类型。</p>
<h1 id="通用结构"><a href="#通用结构" class="headerlink" title="通用结构"></a>通用结构</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type&#x2F;subtype</span><br></pre></td></tr></table></figure>
<ol>
<li>不允许空格</li>
<li>大小写不敏感，一般都是小写</li>
</ol>
<h1 id="独立类型"><a href="#独立类型" class="headerlink" title="独立类型"></a>独立类型</h1><p>type 可以是独立类型，表示文件的分类，可以是如下值：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>典型示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>text</code></td>
<td>表明文件是普通文本，理论上是可读的语言</td>
<td><code>text/plain</code>, <code>text/html</code>, <code>text/css, text/javascript</code></td>
</tr>
<tr>
<td><code>image</code></td>
<td>表明是某种图像。不包括视频，但是动态图（比如动态gif）也使用image类型</td>
<td><code>image/gif</code>, <code>image/png</code>, <code>image/jpeg</code>, <code>image/bmp</code>, <code>image/webp</code></td>
</tr>
<tr>
<td><code>audio</code></td>
<td>表明是某种音频文件</td>
<td><code>audio/midi</code>, <code>audio/mpeg, audio/webm, audio/ogg, audio/wav</code></td>
</tr>
<tr>
<td><code>video</code></td>
<td>表明是某种视频文件</td>
<td><code>video/webm</code>, <code>video/ogg</code></td>
</tr>
<tr>
<td><code>application</code></td>
<td>表明是某种二进制数据</td>
<td><code>application/octet-stream</code>, <code>application/pkcs12</code>, <code>application/vnd.mspowerpoint</code>, <code>application/xhtml+xml</code>, <code>application/xml</code>,  <code>application/pdf,`</code>application/json`</td>
</tr>
</tbody>
</table>
<p>一般是文本，但是具体类型不确定时，就用 <code>test/plain</code>；是二进制数据，而类型不确定时，用 <code>application/octet-stream</code></p>
<h2 id="application-octet-stream"><a href="#application-octet-stream" class="headerlink" title="application/octet-stream"></a>application/octet-stream</h2><p>这是应用程序文件的默认值。意思是 <em>未知的应用程序文件 ，</em>浏览器一般不会自动执行或询问执行。浏览器会将它作为附件来处理，附件类型等信息通过HTTP头<a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Content-Disposition" target="_blank" rel="noopener"><code>Content-Disposition</code></a> 设置。</p>
<h1 id="Multipart-类型"><a href="#Multipart-类型" class="headerlink" title="Multipart 类型"></a>Multipart 类型</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">multipart&#x2F;form-data</span><br><span class="line">multipart&#x2F;byteranges</span><br></pre></td></tr></table></figure>
<p> 顾名思义，这是复合文件的一种表现形式，即传递过来的数据有多重类型。典型的如果表单数据可能有 string、文件、视频、音频等。</p>
<p>它由边界线（一个由<code>&#39;--&#39;</code>开始的字符串）划分出的不同部分组成。每一部分有自己的实体，以及自己的 HTTP 请求头，<a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Content-Disposition" target="_blank" rel="noopener"><code>Content-Disposition</code></a>和 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Content-Type" target="_blank" rel="noopener"><code>Content-Type</code></a> 用于文件上传领域，最常用的 (<a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Content-Length" target="_blank" rel="noopener"><code>Content-Length</code></a> 因为边界线作为分隔符而被忽略）。</p>
<p>例如，如下表单:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"http://localhost:8000/"</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"myTextField"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">name</span>=<span class="string">"myCheckBox"</span>&gt;</span>Check<span class="tag">&lt;/<span class="name">input</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"myFile"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">button</span>&gt;</span>Send the file<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>请求是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: localhost:8000</span><br><span class="line">User-Agent: Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko&#x2F;20100101 Firefox&#x2F;50.0</span><br><span class="line">Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,*&#x2F;*;q&#x3D;0.8</span><br><span class="line">Accept-Language: en-US,en;q&#x3D;0.5</span><br><span class="line">Accept-Encoding: gzip, deflate</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Upgrade-Insecure-Requests: 1</span><br><span class="line">Content-Type: multipart&#x2F;form-data; boundary&#x3D;---------------------------8721656041911415653955004498</span><br><span class="line">Content-Length: 465</span><br><span class="line"></span><br><span class="line">-----------------------------8721656041911415653955004498</span><br><span class="line">Content-Disposition: form-data; name&#x3D;&quot;myTextField&quot;</span><br><span class="line"></span><br><span class="line">Test</span><br><span class="line">-----------------------------8721656041911415653955004498</span><br><span class="line">Content-Disposition: form-data; name&#x3D;&quot;myCheckBox&quot;</span><br><span class="line"></span><br><span class="line">on</span><br><span class="line">-----------------------------8721656041911415653955004498</span><br><span class="line">Content-Disposition: form-data; name&#x3D;&quot;myFile&quot;; filename&#x3D;&quot;test.txt&quot;</span><br><span class="line">Content-Type: text&#x2F;plain</span><br><span class="line"></span><br><span class="line">Simple file.</span><br><span class="line">-----------------------------8721656041911415653955004498--</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>rest</tag>
      </tags>
  </entry>
  <entry>
    <title>lombok</title>
    <url>/2019/01/17/lombok/</url>
    <content><![CDATA[<p><a href="https://projectlombok.org/" target="_blank" rel="noopener">lombok</a> is a library to help your <strong>write java cleaner and more efficiently</strong>. It’s plugged into the editor and build tool, which works <strong>at compile time</strong>. </p>
<p>Essentially, it modifies the byte-codes by operating AST (abstract semantic tree) at compile time, which is allowed by javac. This is, in fact, a way to modify java grammar.</p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>To use it,</p>
<ol>
<li>install lombok plugin in intellij</li>
<li>add package dependency in project (to use its annotations)</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.16.18<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>See <a href="https://projectlombok.org/features/all" target="_blank" rel="noopener">all the annotations</a>. Give some example here:</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="@Data"></a>@Data</h3><p><a href="https://projectlombok.org/features/Data" target="_blank" rel="noopener"><code>@Data</code></a> bundles the features of   <a href="https://projectlombok.org/features/ToString" target="_blank" rel="noopener"><code>@ToString</code></a>, <a href="https://projectlombok.org/features/EqualsAndHashCode" target="_blank" rel="noopener"><code>@EqualsAndHashCode</code></a>, <a href="https://projectlombok.org/features/GetterSetter" target="_blank" rel="noopener"><code>@Getter</code> / <code>@Setter</code></a> and <a href="https://projectlombok.org/features/constructor" target="_blank" rel="noopener"><code>@RequiredArgsConstructor</code></a> together.</p>
<p><strong>With lombok:</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lombok.AccessLevel;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"><span class="keyword">import</span> lombok.ToString;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataExample</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">  <span class="meta">@Setter</span>(AccessLevel.PACKAGE) <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> score;</span><br><span class="line">  <span class="keyword">private</span> String[] tags;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@ToString</span>(includeFieldNames=<span class="keyword">true</span>)</span><br><span class="line">  <span class="meta">@Data</span>(staticConstructor=<span class="string">"of"</span>)</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Exercise</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> T value;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Vanila java:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataExample</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> score;</span><br><span class="line">  <span class="keyword">private</span> String[] tags;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">DataExample</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.age;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setScore</span><span class="params">(<span class="keyword">double</span> score)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.score = score;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getScore</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.score;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> String[] getTags() &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.tags;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTags</span><span class="params">(String[] tags)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.tags = tags;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"DataExample("</span> + <span class="keyword">this</span>.getName() + <span class="string">", "</span> + <span class="keyword">this</span>.getAge() + <span class="string">", "</span> + <span class="keyword">this</span>.getScore() + <span class="string">", "</span> + Arrays.deepToString(<span class="keyword">this</span>.getTags()) + <span class="string">")"</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">canEqual</span><span class="params">(Object other)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> other <span class="keyword">instanceof</span> DataExample;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">this</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (!(o <span class="keyword">instanceof</span> DataExample)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    DataExample other = (DataExample) o;</span><br><span class="line">    <span class="keyword">if</span> (!other.canEqual((Object)<span class="keyword">this</span>)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.getName() == <span class="keyword">null</span> ? other.getName() != <span class="keyword">null</span> : !<span class="keyword">this</span>.getName().equals(other.getName())) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.getAge() != other.getAge()) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (Double.compare(<span class="keyword">this</span>.getScore(), other.getScore()) != <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (!Arrays.deepEquals(<span class="keyword">this</span>.getTags(), other.getTags())) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> PRIME = <span class="number">59</span>;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> temp1 = Double.doubleToLongBits(<span class="keyword">this</span>.getScore());</span><br><span class="line">    result = (result*PRIME) + (<span class="keyword">this</span>.getName() == <span class="keyword">null</span> ? <span class="number">43</span> : <span class="keyword">this</span>.getName().hashCode());</span><br><span class="line">    result = (result*PRIME) + <span class="keyword">this</span>.getAge();</span><br><span class="line">    result = (result*PRIME) + (<span class="keyword">int</span>)(temp1 ^ (temp1 &gt;&gt;&gt; <span class="number">32</span>));</span><br><span class="line">    result = (result*PRIME) + Arrays.deepHashCode(<span class="keyword">this</span>.getTags());</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Exercise</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> T value;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Exercise</span><span class="params">(String name, T value)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.name = name;</span><br><span class="line">      <span class="keyword">this</span>.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">Exercise&lt;T&gt; <span class="title">of</span><span class="params">(String name, T value)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Exercise&lt;T&gt;(name, value);</span><br><span class="line">      &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">this</span>.value;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="string">"Exercise(name="</span> + <span class="keyword">this</span>.getName() + <span class="string">", value="</span> + <span class="keyword">this</span>.getValue() + <span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">canEqual</span><span class="params">(Object other)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> other <span class="keyword">instanceof</span> Exercise;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (o == <span class="keyword">this</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      <span class="keyword">if</span> (!(o <span class="keyword">instanceof</span> Exercise)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      Exercise&lt;?&gt; other = (Exercise&lt;?&gt;) o;</span><br><span class="line">      <span class="keyword">if</span> (!other.canEqual((Object)<span class="keyword">this</span>)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.getName() == <span class="keyword">null</span> ? other.getValue() != <span class="keyword">null</span> : !<span class="keyword">this</span>.getName().equals(other.getName())) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.getValue() == <span class="keyword">null</span> ? other.getValue() != <span class="keyword">null</span> : !<span class="keyword">this</span>.getValue().equals(other.getValue())) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="keyword">int</span> PRIME = <span class="number">59</span>;</span><br><span class="line">      <span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line">      result = (result*PRIME) + (<span class="keyword">this</span>.getName() == <span class="keyword">null</span> ? <span class="number">43</span> : <span class="keyword">this</span>.getName().hashCode());</span><br><span class="line">      result = (result*PRIME) + (<span class="keyword">this</span>.getValue() == <span class="keyword">null</span> ? <span class="number">43</span> : <span class="keyword">this</span>.getValue().hashCode());</span><br><span class="line">      <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>mybatis 工作原理</title>
    <url>/2018/08/10/mybatis-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h1 id="几个核心类"><a href="#几个核心类" class="headerlink" title="几个核心类"></a>几个核心类</h1><p>参见:</p>
<ul>
<li><a href="http://www.mybatis.org/mybatis-3/zh/java-api.html" target="_blank" rel="noopener">java api</a></li>
<li><a href="http://www.mybatis.org/mybatis-3/zh/getting-started.html" target="_blank" rel="noopener">入门 - 介绍核心使用组件和最佳实践</a></li>
</ul>
<h2 id="SqlSessionFactory"><a href="#SqlSessionFactory" class="headerlink" title="SqlSessionFactory"></a>SqlSessionFactory</h2><p>mybatis 应用以一个 sqlSessionFactory 实例为核心，即一个应用中有一个<strong>单例</strong> <code>SqlSessionFactory</code>，所以数据库 session 都从这里获得。</p>
<p><code>SqlSessionFactory</code> 可以通过 <code>SqlSessionFactoryBuilder</code> 获得，builder 负责从 xml 配置或 java configuration 类获得。xml (或相应的 java configuration 类) 配置了 datasource（数据库连接信息）、mappers 等信息</p>
<h2 id="SqlSessionFactoryBuilder"><a href="#SqlSessionFactoryBuilder" class="headerlink" title="SqlSessionFactoryBuilder"></a>SqlSessionFactoryBuilder</h2><p>它主要就是用来获取 <code>SqlSessionFactory</code>，可以从 xml 或 Java Configuration 类加载配置并构建。提供如下几种方式来获取（参见<a href="http://www.mybatis.org/mybatis-3/zh/java-api.html" target="_blank" rel="noopener">java api</a>）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 从 xml 获取，其中配置了 environment，datasource，mappers  </span></span><br><span class="line"><span class="function">SqlSessionFactory <span class="title">build</span><span class="params">(InputStream inputStream)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 xml 获取，但当 xml 配置了多个 env 中的 datasource 等时，通过 env 指定加载的环境</span></span><br><span class="line"><span class="function">SqlSessionFactory <span class="title">build</span><span class="params">(InputStream inputStream, String environment)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 xml 获取，但可以指定其中使用到的 properties(详见下文的解释)</span></span><br><span class="line"><span class="function">SqlSessionFactory <span class="title">build</span><span class="params">(InputStream inputStream, Properties properties)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 从 xml 获取，并指定 env 和使用的 props</span></span><br><span class="line"><span class="function">SqlSessionFactory <span class="title">build</span><span class="params">(InputStream inputStream, String env, Properties props)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 Java Configuration 获取</span></span><br><span class="line"><span class="function">SqlSessionFactory <span class="title">build</span><span class="params">(Configuration config)</span></span></span><br></pre></td></tr></table></figure>
<p><code>SqlSessionFactoryBuilder</code> 只是为了创建 <code>SqlSessionFactory</code>，创建完成就可以丢弃 builder 了。所以一般它的生命周期是方法级，是其中的一个局部变量</p>
<h3 id="使用-xml"><a href="#使用-xml" class="headerlink" title="使用 xml"></a>使用 xml</h3><p><strong>先配置一个 <code>config.xml</code>：</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">configuration</span></span></span><br><span class="line"><span class="meta">  <span class="meta-keyword">PUBLIC</span> <span class="meta-string">"-//mybatis.org//DTD Config 3.0//EN"</span></span></span><br><span class="line"><span class="meta">  <span class="meta-string">"http://mybatis.org/dtd/mybatis-3-config.dtd"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">environments</span> <span class="attr">default</span>=<span class="string">"development"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">environment</span> <span class="attr">id</span>=<span class="string">"development"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">transactionManager</span> <span class="attr">type</span>=<span class="string">"JDBC"</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dataSource</span> <span class="attr">type</span>=<span class="string">"POOLED"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"driver"</span> <span class="attr">value</span>=<span class="string">"$&#123;driver&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"url"</span> <span class="attr">value</span>=<span class="string">"$&#123;url&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"username"</span> <span class="attr">value</span>=<span class="string">"$&#123;username&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"password"</span> <span class="attr">value</span>=<span class="string">"$&#123;password&#125;"</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dataSource</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">environment</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">environments</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mappers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">"org/mybatis/example/BlogMapper.xml"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">mappers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>上边的配置中，<code>environments</code> 配置的是各个环境下的数据库配置。每个环境下，都可以配置 TransactionManager、datasource 等，连接数据库、包括操作数据库的 driver 等信息都是在这里配置的)。</p>
<p><code>${driver}</code> 这种写法是用的 property。property 可以直接在这个 xml 中配置（使用 <code>&lt;properties&gt;</code> 标签），也是 java 中的 <code>System.getProperties()</code> 中的 property，还可以是在 <code>SqlSessionBuilder</code> 中传递的 <code>props</code> 参数。</p>
<p><strong>构建 <code>SqlSessionFactory</code>：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String resource = <span class="string">"org/mybatis/example/mybatis-config.xml"</span>;</span><br><span class="line">InputStream inputStream = Resources.getResourceAsStream(resource);</span><br><span class="line">SqlSessionFactory sqlSessionFactory = <span class="keyword">new</span> SqlSessionFactoryBuilder().build(inputStream);</span><br></pre></td></tr></table></figure>
<h3 id="使用-java-Configuration-类"><a href="#使用-java-Configuration-类" class="headerlink" title="使用 java Configuration 类"></a>使用 java Configuration 类</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource dataSource = BlogDataSourceFactory.getBlogDataSource();</span><br><span class="line">TransactionFactory transactionFactory = <span class="keyword">new</span> JdbcTransactionFactory();</span><br><span class="line">Environment environment = <span class="keyword">new</span> Environment(<span class="string">"development"</span>, transactionFactory, dataSource);</span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration(environment);</span><br><span class="line">configuration.addMapper(BlogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">SqlSessionFactory sqlSessionFactory = <span class="keyword">new</span> SqlSessionFactoryBuilder().build(configuration);</span><br></pre></td></tr></table></figure>
<h2 id="SqlSession"><a href="#SqlSession" class="headerlink" title="SqlSession"></a>SqlSession</h2><p><code>SqlSession</code> 是执行 sql 命令的接口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SqlSession session = sqlSessionFactory.openSession();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  BlogMapper mapper = session.getMapper(BlogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  Blog blog = mapper.selectBlog(<span class="number">101</span>);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  session.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>SqlSession</code> 可以执行所有的 sql 命令、做事务提交、回滚、获取 Mapper 实例等，非常强大。<code>SqlSession</code> 也是方法作用域级别的，并且必须被正确关闭：</p>
<blockquote>
<p>每个线程都应该有它自己的 SqlSession 实例。SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。<strong>绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。</strong>也绝不能将 SqlSession 实例的引用放在任何类型的管理作用域中，比如 Servlet 架构中的 HttpSession。如果你现在正在使用一种 Web 框架，要考虑 SqlSession 放在一个和 HTTP 请求对象相似的作用域中。换句话说，每次收到的 HTTP 请求，就可以打开一个 SqlSession，返回一个响应，就关闭它。这个关闭操作是很重要的，你应该把这个关闭操作放到 finally 块中以确保每次都能执行关闭。下面的示例就是一个确保 SqlSession 关闭的标准模式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SqlSession session = sqlSessionFactory.openSession();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// do work</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  session.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h2><p><code>Mapper</code> 是资源和数据库实例的映射，提供了相关操作来做转换。可以用两种方式写：xml 或 annotation。</p>
<p>mapper 实例从 <code>SqlSession</code> 获得，所以生命周期和 SqlSession 相同。</p>
<h3 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">  <span class="meta-keyword">PUBLIC</span> <span class="meta-string">"-//mybatis.org//DTD Mapper 3.0//EN"</span></span></span><br><span class="line"><span class="meta">  <span class="meta-string">"http://mybatis.org/dtd/mybatis-3-mapper.dtd"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"org.mybatis.example.BlogMapper"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"selectBlog"</span> <span class="attr">resultType</span>=<span class="string">"Blog"</span>&gt;</span></span><br><span class="line">    select * from Blog where id = #&#123;id&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>访问：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Blog blog = (Blog) session.selectOne(<span class="string">"org.mybatis.example.BlogMapper.selectBlog"</span>, <span class="number">101</span>);</span><br></pre></td></tr></table></figure>
<p>当然，我们希望通过 java 接口来调用这些方法，所以可以写相应的 <code>mapper</code> 接口 ，只要保证 <code>namespace</code> 是一致的即可。<code>namespace</code> 是实现接口绑定的方式。mybatis 基于命名空间的命名解析规则如下：</p>
<blockquote>
<ul>
<li>完全限定名（比如“com.mypackage.MyMapper.selectAllThings”）将被直接查找并且找到即用。</li>
<li>短名称（比如“selectAllThings”）如果全局唯一也可以作为一个单独的引用。如果不唯一，有两个或两个以上的相同名称（比如“com.foo.selectAllThings ”和“com.bar.selectAllThings”），那么使用时就会收到错误报告说短名称是不唯一的，这种情况下就必须使用完全限定名。</li>
</ul>
</blockquote>
<p>一旦绑定了接口，就可以用如下方式访问 mapper 方法了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BlogMapper mapper = session.getMapper(BlogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">Blog blog = mapper.selectBlog(<span class="number">101</span>);</span><br></pre></td></tr></table></figure>
<h3 id="annotation"><a href="#annotation" class="headerlink" title="annotation"></a>annotation</h3><p>也可以不依赖于 xml，直接在 mapper 接口上通过 annotation 定义 sql：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.mybatis.example;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">BlogMapper</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Select</span>(<span class="string">"SELECT * FROM blog WHERE id = #&#123;id&#125;"</span>)</span><br><span class="line">  <span class="function">Blog <span class="title">selectBlog</span><span class="params">(<span class="keyword">int</span> id)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>annotation 可能更简洁一些，但是 mybatis 目前还是 xml 更强大。大家可以依据需求自由的在两种方式之间切换，mybatis 会自己检测。</p>
<blockquote>
<p>由于 Java 注解的一些限制加之某些 MyBatis 映射的复杂性，XML 映射对于大多数高级映射（比如：嵌套 Join 映射）来说仍然是必须的。有鉴于此，如果存在一个对等的 XML 配置文件的话，MyBatis 会自动查找并加载它（这种情况下， BlogMapper.xml 将会基于类路径和 BlogMapper.class 的类名被加载进来）</p>
</blockquote>
]]></content>
      <tags>
        <tag>orm</tag>
      </tags>
  </entry>
  <entry>
    <title>linux commands</title>
    <url>/2019/01/23/linux-command/</url>
    <content><![CDATA[<h1 id="chmod-chown"><a href="#chmod-chown" class="headerlink" title="chmod, chown"></a>chmod, chown</h1><p><a href="https://www.linux.com/learn/understanding-linux-file-permissions" target="_blank" rel="noopener">understanding linux file permissions</a></p>
<p>File permissions are defined by <strong>permission group</strong> and <strong>permission type</strong></p>
<ol>
<li>permission group<ul>
<li>owner(u)</li>
<li>group(g)</li>
<li>all other users(a)</li>
</ul>
</li>
<li>permission type<ul>
<li>read (r - 4)</li>
<li>write(w - 2)</li>
<li>execute(x - 1)</li>
</ul>
</li>
</ol>
<h2 id="permission-presentation"><a href="#permission-presentation" class="headerlink" title="permission presentation"></a>permission presentation</h2><p>The permission in the command line is displayed as <strong><em>_rwxrwxrwx 1 owner:group</em></strong></p>
<ul>
<li>the first character (underscore <strong>_</strong>  here) is the <strong>special permission flag</strong> that can vary.</li>
<li>the following three groups of <strong><em>rwx</em></strong> represent <strong>permission of owner, group and all other users</strong> respectively. If the owner and all users has no read permission, it is <strong><em>__wxrwx_wx</em></strong></li>
<li>follwing that grouping since the integer displays <strong>the number of hardlinks to the file</strong></li>
<li>the last piece is the owner and group assignment.</li>
</ul>
<h3 id="special-permission-flag"><a href="#special-permission-flag" class="headerlink" title="special permission flag"></a>special permission flag</h3><p>The special permission flag can be:</p>
<ul>
<li><strong>_</strong>: no special permissions</li>
<li><strong><em>d</em></strong>: directory</li>
<li><strong><em>l</em></strong>: the file or dir is a symbolic link</li>
<li><strong><em>s</em></strong>: This indicated the setuid/setgid permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a <strong>s</strong> in the read portion of the owner or group permissions.</li>
<li><strong><em>t</em></strong>: This indicates the sticky bit permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a <strong>t</strong> in the executable portion of the all users permissions</li>
</ul>
<h2 id="permission-modification"><a href="#permission-modification" class="headerlink" title="permission modification"></a>permission modification</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grant read and write permissions to the user and group</span></span><br><span class="line">$ chmod ug+rw file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove read and write permissions to the user and group</span></span><br><span class="line">$ chmod ug-rw file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># set permission using binary references (owner: rwx = 4+2+1, group: rx = 4+1, all users: rx = 4+1)</span></span><br><span class="line">$ chmod 755 file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># change the file permission recursively in the file/dir instead of just the files themselves</span></span><br><span class="line">$ chmod -R 755 dir1</span><br></pre></td></tr></table></figure>
<h2 id="change-owner-group-assignments"><a href="#change-owner-group-assignments" class="headerlink" title="change owner:group assignments"></a>change owner:group assignments</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># change the owner of file1 to user1 and group to family</span></span><br><span class="line">$ chown user1:family file1</span><br></pre></td></tr></table></figure>
<h1 id="find"><a href="#find" class="headerlink" title="find"></a>find</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># find all xml files from the current dir</span></span><br><span class="line">$ find ./* -name <span class="string">'*.xml'</span></span><br></pre></td></tr></table></figure>
<p>To find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ find /directory_path -mtime -1 -ls</span><br></pre></td></tr></table></figure>
<p>Should be to your liking</p>
<p>The <code>-</code> before <code>1</code> is important - it means anything changed one day or less ago. A <code>+</code> before <code>1</code> would instead mean anything changed at least one day ago, while having nothing before the <code>1</code> would have meant it was changed exacted one day ago, no more, no less.</p>
<p>Another, more humane way:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">"-24 hours"</span> -ls</span><br></pre></td></tr></table></figure>
<p>or:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">"1 day ago"</span> -ls</span><br></pre></td></tr></table></figure>
<p>or:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">"yesterday"</span> -ls</span><br></pre></td></tr></table></figure>
<h1 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h1><p>找到文件并删除</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">find /home/raven -name abc.txt | xargs rm -rf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 不使用 xargs</span></span><br></pre></td></tr></table></figure>
<h1 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># list file with creation date and sort by it</span></span><br><span class="line">$ ls -lct</span><br></pre></td></tr></table></figure>
<h1 id="du"><a href="#du" class="headerlink" title="du"></a>du</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ du -sh -- * | sort -hr</span><br></pre></td></tr></table></figure>
<h1 id="List-users"><a href="#List-users" class="headerlink" title="List users"></a>List users</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ cat /etc/passwd | cut -d: -f1</span><br></pre></td></tr></table></figure>
<h1 id="pbcopy"><a href="#pbcopy" class="headerlink" title="pbcopy"></a>pbcopy</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copy file content to clipboard</span></span><br><span class="line">$ pbcopy &lt; test.txt</span><br></pre></td></tr></table></figure>
<h1 id="dstat"><a href="#dstat" class="headerlink" title="dstat"></a>dstat</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ dstat -t -a --tcp --output network.log</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>oauth</title>
    <url>/2018/07/27/oauth/</url>
    <content><![CDATA[<h1 id="traditional-authentication"><a href="#traditional-authentication" class="headerlink" title="traditional authentication"></a>traditional authentication</h1><p>传统认证使用  session:</p>
<ol>
<li>client 发送 username、password 给 server</li>
<li>server 查数据库，检查信息，是否正确。正确就把用户登录信息(即用户状态)写到 session 里（即服务器内存中），并将 sessionId 返回给 client。</li>
<li>client 在请求 api 时，在 cookie 中传递 sessionId。server 端根据 sessionId 获取用户登录信息，如果已认证，返回正常响应；反之，401</li>
</ol>
<p><img src="https://camo.githubusercontent.com/f6ea1099ada7ec855919d6e483d0d903f1cc96ca/68747470733a2f2f636d732d6173736574732e74757473706c75732e636f6d2f75706c6f6164732f75736572732f3438372f706f7374732f32323534332f696d6167652f747261646974696f6e616c2d61757468656e7469636174696f6e2d73797374656d2d706e672e706e67" alt="auth image"></p>
<p>这种方式有个缺陷：如果做分布式服务部署，那么需要每个服务器都要同步相同的登录信息，这不是一个好的方式。所以一般 rest 微服务都要求的是 stateless，即 server 端不保存任何用户信息，请求中包含所有需要的信息。</p>
<h1 id="oauth"><a href="#oauth" class="headerlink" title="oauth"></a>oauth</h1><p><a href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%94%BE%E6%8E%88%E6%9D%83" target="_blank" rel="noopener">oauth</a> 是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和<a href="https://zh.wikipedia.org/wiki/%E5%AF%86%E7%A0%81" target="_blank" rel="noopener">密码</a>提供给第三方应用。</p>
<p>OAuth允许用户提供一个 <a href="https://zh.wikipedia.org/w/index.php?title=%E4%BB%A4%E7%89%8C&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">令牌</a>，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容。</p>
<p> 其令牌可以是 JWT 或其他形式。</p>
<p><img src="https://docs.oracle.com/cd/E74890_01/books/RestAPI/images/OAuth2leg_V.gif" alt="oauth"></p>
<p>ref: <a href="https://my.oschina.net/eyes4/blog/639970" target="_blank" rel="noopener">oauth 2.0 的用途</a></p>
<h1 id="Auth0"><a href="#Auth0" class="headerlink" title="Auth0"></a>Auth0</h1><p><a href="https://auth0.com/" target="_blank" rel="noopener">autho0</a> 实现了<a href="https://auth0.com/docs/protocols" target="_blank" rel="noopener">很多开放标准</a>，包括 oauth。(<a href="https://www.youtube.com/watch?v=QsMK3d3LxYQ" target="_blank" rel="noopener">学习视频</a>)</p>
<ol>
<li>要使用 Auth0，首先需要创建一个 App（被称作 client），其中定义了 clientId、domain name、callbackUrl、secret 等。</li>
<li>前端交互：<ol>
<li>当访问某个页面时，查看 localstorage，看用户是否登录；</li>
<li>如果未登录，利用 Auth0 sdk 或 api 登录认证（提供前边 App 中的 clientId、secret 等信息），认证通过将认证信息（token 等）存入 localstorage，并跳转到 callback url</li>
<li>如果登录，直接访问</li>
</ol>
</li>
<li>后端交互：<ol>
<li>前端携带 token 访问 API</li>
<li>server 利用 Auth0 sdk 或 api 验证 token 的有效性；认证通过返回资源，否则 401</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>security</tag>
        <tag>oauth</tag>
      </tags>
  </entry>
  <entry>
    <title>obs</title>
    <url>/2019/03/01/obs/</url>
    <content><![CDATA[<p><a href="https://support.huaweicloud.com/productdesc-obs/zh-cn_topic_0045829060.html" target="_blank" rel="noopener">Huawei Obs</a> is an object storage service on cloud.</p>
<h1 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><ol>
<li>The real complete file or byte stream to save</li>
<li><strong>object name is the unique id</strong> in a bucket<ol>
<li>it’s used as <strong>part of url path</strong>. The naming restrictions are fit to url path naming restrictions.</li>
</ol>
</li>
<li>Access(based on version in fact)<ol>
<li>Object ACL:<ol>
<li>general control to object: <strong>read object, read/write object ACL, only users in the same account</strong></li>
</ol>
</li>
<li>Object policy<ol>
<li>fine-grained control to object: <strong>fine-grained actions(put,delete…) on object, all users</strong></li>
</ol>
</li>
</ol>
</li>
<li><strong>multi-versions</strong><ol>
<li>an object can has multiple versions, each of which has an unique id.</li>
<li>Whether there’s multi-version, it’s a policy set on a bucket.</li>
</ol>
</li>
<li>directory:<ol>
<li><strong>directory is just a view</strong>. Essentially, it’s an empty object end with “/“.</li>
<li><strong>all objects in a bucket are on the same level</strong>. There’s no multi-level directory in fact.</li>
<li>to create the directory view, you need to create an object with name ending with “/“ explicility, eg. “sub1/sub2/ . It will create a two-level dir in console. There’s no need to create “sub1/“ first then “sub1/sub2”.</li>
</ol>
</li>
<li>object actions:<ol>
<li>For writing, there’s <strong>only write/restricted-append/delete, no put</strong></li>
<li><strong>basically-write-once-read-many</strong></li>
</ol>
</li>
<li>upload modes:<ol>
<li>stream</li>
<li>file</li>
<li>multi-part (support breakpoint resume)</li>
<li>append</li>
</ol>
</li>
</ol>
<h2 id="Bucket"><a href="#Bucket" class="headerlink" title="Bucket"></a>Bucket</h2><ol>
<li>The place to save objects</li>
<li><strong>bucket name is the unique id</strong> for one account(a tenant).<ol>
<li>it’s used as <strong>part of domain name</strong> on url. The naming restrictions are fit to domain naming restrictions.</li>
</ol>
</li>
<li>Access<ol>
<li>Bucket ACL: <ol>
<li>general control to bucket and all objects in bucket: <strong>read/put buckets, read/write bucket ACL, only users in the same account</strong></li>
</ol>
</li>
<li>Bucket policy<ol>
<li>fine-grained control to specific objects in bucket: <strong>fine-grained actions on bucket or specific objects in bucket, all users</strong></li>
</ol>
</li>
</ol>
</li>
<li>storage type<ol>
<li>standard: <ol>
<li><strong>quick access &amp; high throughput</strong>. It’s used for high access requests and not so big files.</li>
</ol>
</li>
<li>warm:<ol>
<li><strong>low access.</strong></li>
</ol>
</li>
<li>cold:<ol>
<li><strong>very very low access</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="region"><a href="#region" class="headerlink" title="region"></a>region</h2><p>The region of nodes where the storage really happens.</p>
<h2 id="signature"><a href="#signature" class="headerlink" title="signature"></a>signature</h2><p>The signature to identify a user when accessing buckets/objects.</p>
<ol>
<li>ak(access key): represent a user. one user can have multi aks. It’s kind of like an user role</li>
<li>sk(secret key): one-to-one corresponding with ak. The secret key used for RSA authentication &amp; authorization.</li>
</ol>
]]></content>
      <tags>
        <tag>cloud</tag>
        <tag>object storage</tag>
      </tags>
  </entry>
  <entry>
    <title>performance for io in java</title>
    <url>/2018/10/15/performance-of-io-in-java/</url>
    <content><![CDATA[<h1 id="java-io-ByteArrayOutputStream"><a href="#java-io-ByteArrayOutputStream" class="headerlink" title="java.io.ByteArrayOutputStream"></a>java.io.ByteArrayOutputStream</h1><p>这一般在用到字节流是会用到。</p>
<p><a href="http://java-performance.info/java-io-bytearrayoutputstream/" target="_blank" rel="noopener">java performance tuning guide</a> 这篇文章不建议在 performance-criticted 代码中使用 <code>ByteArrayOutputStream</code>：</p>
<ol>
<li><strong>同步写入，效率低</strong></li>
</ol>
<blockquote>
<p><code>ByteArrayOutputStream</code> allows you to write anything to an internal expandable byte array and use that array as a single piece of output afterwards. Default buffer size is 32 bytes, so if you expect to write something longer, provide an explicit buffer size in the <code>ByteArrayOutputStream(int)</code> constructor</p>
<p> 注：</p>
<ol>
<li><code>ByteArrayOutputStream</code> 内部是一个可变长度的 byte[]（通过扩充实现可变）。它有个初始长度（默认 32），可以在 constructor 中指定.</li>
<li><code>ByteArrayOutputStream</code> 是同步写入，比较影响效率</li>
</ol>
</blockquote>
<ol start="2">
<li><strong>toByteArray() 效率低，使用 toString(charset)</strong></li>
</ol>
<p><code>toByteArray</code> 执行了一遍拷贝，效率低。<code>toString</code> 则使用 <code>String(bytes[])</code> 直接将内部 byte 转成了 String</p>
<p><a href="http://java-performance.info/inefficient-byte-to-string-constructor/" target="_blank" rel="noopener">inefficient byte[] to String constructor</a> 指出 <code>String(bytes[])</code>  也是 copy，但 java8 中的源码看了下，似乎不是 copy，待考证……</p>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>pipeline process: beam</title>
    <url>/2019/01/30/pipeline-process-beam/</url>
    <content><![CDATA[<h1 id="What’s-beam"><a href="#What’s-beam" class="headerlink" title="What’s beam"></a>What’s beam</h1><p><a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">beam</a> is a open-source, unified model for defining both batched &amp; streaming data-parallel processing pipelines.</p>
<ul>
<li>open-source (apache v2 license)</li>
<li>to define data-parallel processing pipelines</li>
<li>an unified model to define pipelines. The real processing is run by the underlying runner (eg. spark, apache apex, etc.). <a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">all available runners</a></li>
<li>can process both batched  (bounded datasets) &amp; streaming (unbounded datasets) datasets</li>
</ul>
<h1 id="Use-it"><a href="#Use-it" class="headerlink" title="Use it"></a>Use it</h1><p>See the <a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">wordcount examples</a>, <a href="https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java" target="_blank" rel="noopener">wordcount src</a></p>
<p>Now we define a simple pipeline and run it.</p>
<p><code>Transform</code>, <code>Count</code> are all built-in atom operations to define the pipeline scripts.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.beam.examples;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.Pipeline;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.io.TextIO;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.options.PipelineOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.options.PipelineOptionsFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.Count;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.Filter;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.FlatMapElements;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.MapElements;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.values.KV;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.values.TypeDescriptors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MinimalWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a PipelineOptions object. This object lets us set various execution</span></span><br><span class="line">    <span class="comment">// options for our pipeline, such as the runner you wish to use.</span></span><br><span class="line">    PipelineOptions options = PipelineOptionsFactory.create();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the Pipeline object with the options we defined above</span></span><br><span class="line">    Pipeline p = Pipeline.create(options);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set</span></span><br><span class="line">    p.apply(TextIO.read().from(<span class="string">"gs://apache-beam-samples/shakespeare/*"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Concept #2: Apply a FlatMapElements transform the PCollection of text lines.</span></span><br><span class="line">        .apply(</span><br><span class="line">            FlatMapElements.into(TypeDescriptors.strings())</span><br><span class="line">                .via((String word) -&gt; Arrays.asList(word.split(<span class="string">"[^\\p&#123;L&#125;]+"</span>))))</span><br><span class="line">        .apply(Filter.by((String word) -&gt; !word.isEmpty()))</span><br><span class="line">        <span class="comment">// Concept #3: Apply the Count transform to our PCollection of individual words. </span></span><br><span class="line">        .apply(Count.perElement())</span><br><span class="line">        .apply(</span><br><span class="line">            MapElements.into(TypeDescriptors.strings())</span><br><span class="line">                .via(</span><br><span class="line">                    (KV&lt;String, Long&gt; wordCount) -&gt;</span><br><span class="line">                        wordCount.getKey() + <span class="string">": "</span> + wordCount.getValue()))</span><br><span class="line">        <span class="comment">// Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline.</span></span><br><span class="line">        .apply(TextIO.write().to(<span class="string">"wordcounts"</span>));</span><br><span class="line"></span><br><span class="line">    p.run().waitUntilFinish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Some-conceptions"><a href="#Some-conceptions" class="headerlink" title="Some conceptions"></a>Some conceptions</h1><h2 id="I-O-data-source-target"><a href="#I-O-data-source-target" class="headerlink" title="I/O (data source/target)"></a>I/O (data source/target)</h2><p>Beam can process both batched  (bounded datasets) &amp; streaming (unbounded datasets) datasets. <a href="https://beam.apache.org/documentation/io/built-in/" target="_blank" rel="noopener">built-in io transforms</a></p>
<p>Take reading as example, you specify the file location (the location must be accessable for the runner), and then the reader pull from datasource. You may also define the trigger to collect input window. When trigger is satisfied, window elements are emitted.</p>
<p>For unbounded datasets, they are split into windows. And each window is again a bounded datasets. In each window, there’re some elements. You can define how the elements are grouped as a window and when to emit the window elements for processing. <a href="https://beam.apache.org/documentation/programming-guide/#windowing" target="_blank" rel="noopener">window concept</a></p>
<h2 id="Runner"><a href="#Runner" class="headerlink" title="Runner"></a>Runner</h2><p>Beam is an unified model. It abstracts the conception to define and run a pipeline. The real execution is conducted by the underlying runners.</p>
<p><a href="https://beam.apache.org/get-started/beam-overview/" target="_blank" rel="noopener">all available runners</a></p>
<p>For unbounded datasets, the underlying runner must support stream processing.</p>
]]></content>
      <tags>
        <tag>bigdata</tag>
        <tag>distributed processing</tag>
      </tags>
  </entry>
  <entry>
    <title>network</title>
    <url>/2020/07/23/network/</url>
    <content><![CDATA[<p><a href="https://www.youtube.com/watch?v=rL8RSFQG8do&amp;list=PLF360ED1082F6F2A5" target="_blank" rel="noopener">Eli the computer guy</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>the whole picture </p>
<p>Speed &amp; storage unit</p>
<p>Physical &amp; logical</p>
<p>modem</p>
<ul>
<li>t1</li>
<li>dsl:<ul>
<li>no faster than 12Mb/s</li>
<li>Asynchronous: download faster than upload</li>
</ul>
</li>
<li>cabel</li>
<li>satellite</li>
</ul>
<p>Router</p>
<p>firewall</p>
<ul>
<li>block the internet to get into your network</li>
</ul>
<p>VPN</p>
<ul>
<li>enable the internet to get into your network</li>
<li>client-server</li>
</ul>
<p>Switch</p>
<ul>
<li>Connect everything together</li>
</ul>
<h1 id="TCP-IP"><a href="#TCP-IP" class="headerlink" title="TCP/IP"></a>TCP/IP</h1><p>ip: internet protocol. How to find a computer.</p>
<p>tcp: transmission control protocol. How to communicate between 2 computers</p>
<p>suite: tcp protocol, ip protocol, etc</p>
<p>tcp windowing</p>
<p>components:</p>
<ul>
<li><p>ip address</p>
</li>
<li><p>subnet</p>
</li>
<li><p>default gateway</p>
</li>
<li><p>dns</p>
</li>
<li><p>dhcp</p>
</li>
<li><p>nat</p>
</li>
</ul>
<p>subnet mask: network identifier, device identifier.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> windows, first release and renew to ensure to get the latest configurations</span></span><br><span class="line">ipconfig /release</span><br><span class="line">ipconfig /renew</span><br><span class="line">ipconfig /all</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> linux and mac</span></span><br><span class="line">ifconfig [-a]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ping 10.1.10.11 10 <span class="built_in">times</span> with 60ms ttl</span></span><br><span class="line">ping www.baidu.com -c 10 -m 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> windows, it<span class="string">'s tracert. For linux and mac, it'</span>s traceroute</span></span><br><span class="line">traceroute www.baidu.com</span><br></pre></td></tr></table></figure>
<h2 id="OpenDNS-for-network-security"><a href="#OpenDNS-for-network-security" class="headerlink" title="OpenDNS for network security"></a>OpenDNS for network security</h2><p>dns can be hacked so that you’re redirected to some unwanted websites. Or a virus on your computer tries to pull more viruses  from some virus website. </p>
<p>OpenDNS is a product to prevent such nasty issues.</p>
<ol>
<li><p>With a router, and dhcp on that router (allocating ip/subset mask,etc to all your network devices), you config the dns as the opendns</p>
</li>
<li><p>On opendns control panel, add your network (the external ip of your network, i.e. the router ip address) and config the filter for your network.</p>
</li>
<li><p>When accessing unallowed websites, you will be redirected to a block opendns page by opendns.</p>
</li>
</ol>
<p>If you don’t have static ip, install the dynamic ip address from opendns, which will tell the opendns your current network address.</p>
<h1 id="Servers"><a href="#Servers" class="headerlink" title="Servers"></a>Servers</h1><p>server operating system: more robust and expensive, compared to desktop operating system.</p>
<p>specific hardware: for data center. xeon processor, redundant power supply, raid (redundant hard drive).</p>
<h1 id="Voice-over-ip-VOIP"><a href="#Voice-over-ip-VOIP" class="headerlink" title="Voice over ip (VOIP)"></a>Voice over ip (VOIP)</h1><p>At the beginning, telephone system communicate using wires. It’s completely separate to comupter system.</p>
<p>VOIP make the audio transimit through the TCP/IP protocol.</p>
<p>Client-Server infrastructure: hard phones / soft phones have to install the VOIP client to communicate to VOIP server, which routes all audio communications.</p>
<p>Iphone in fact installs VOIP client and phone through VOIP service. IPhone is the above soft phones.</p>
<p>Gateways. It can connect different communication system, e.g. the old telephone system and VOIP system, so that you can call through VOIP server -&gt; Gateway -&gt; the wireline telephone.</p>
<p>Codec. It encode the audio transmitted on VOIP service and decide what the packet should be like. So it decides the quality and bandwidth it needs.</p>
<p>QOS. quality of service. On switches and routers, config the VOIP transmision as high priority, so that it won’t be influenced by other bandwidth sharer.</p>
<p>Unifed communication. I think it’s the whole idea that make the telephone and computer stay in the same system, so that we can do a lot by this.</p>
<h1 id="Cloud-Computing"><a href="#Cloud-Computing" class="headerlink" title="Cloud Computing"></a>Cloud Computing</h1><p>Web application. If we want some application, but it’s in fact not on our own hardware, and it’s outside somewhere, then it’s in fact cloud computing. It’s just maybe it’s private cloud.</p>
<p>Cluster. And to be a cloud, there also needs to be a cluster, so that when some bad things happen, it can recover itself.</p>
<p>Cluster make the services more robust. It’s a normal way for cloud to provide robustness.</p>
<p>Terminal Services. There’s terminal service server and thin clients. You can access a remote operating system desktop through thin clients. This seems to be the remote connection. Furthermore, the remote system can just be an specific application instead of a desktop. In such case, there’s application server.</p>
<p>This is an old technique. In old days, it’s called mainframe and dumb terminals. Anyway, the thin clients capture all the strokes you made and send them to the terminal service server, and the terminal service server sends the result image back. The teminal service server is the one with processing abilities, and all thin clients share the cpu time, each allocated with one slice of CPU time.</p>
<p>Terminal service is also a cloud related technique to access cloud services.</p>
<p>Virtualization. A tenichque that separate the operating system from the hardware, so that you can transfer the operating system with the applications in it easily. There’re 2 flavours to implement it:</p>
<ol>
<li><p>client installer. This is the normal virtualbox or vmware fusion. You installed this installer on your current operating system, and then you installed another os box using the installer.</p>
</li>
<li><p>hypervision. Hypervision is in fact an operating system. Install hypervision on hardware, and then using the management client on your computer to install anything you want on that hypervision. The hypervision will then allocate a piece of hard drive, ram, etc to the os you asked. In the vmware world, the hypervision is esxi which is always free, the management software is the vsphere which is in charged.</p>
</li>
</ol>
<p>Current cloud computing should be using some techniques like the hypervision. Anyway, they try to separate the os from the hardware, so that you can copy-paste the whole environment easily to another hardware like a file.</p>
]]></content>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>react native components &amp; apis</title>
    <url>/2022/10/22/react-native-components-apis/</url>
    <content><![CDATA[<p>react 提倡组件化开发，以促进复用。react native 也一样是组件化开发思想。不同的是，react 中是使用原生的 html 组件作为基本组件（div、a…），而 react native 使用的是另一些原生组件。用户的自定义组件也是基于这些原生组件。</p>
<p>所以要用 react native，必须了解这些原生组件（就跟学 html 组件差不多）</p>
<h1 id="general-props"><a href="#general-props" class="headerlink" title="general props"></a>general props</h1><p>一些所有组件或大部分组件都有的属性。</p>
<h2 id="style"><a href="#style" class="headerlink" title="style"></a><a href="https://reactnative.cn/docs/0.51/style.html#content" target="_blank" rel="noopener">style</a></h2><p>所有的核心组件都接受名为 <code>style</code> 的属性（类似 html 标签中的 <code>html</code>）。这些样式名基本上是遵循了 web 上的 CSS 的命名，只是按照 JS 的语法要求使用了驼峰命名法，例如将 <code>background-color</code> 改为<code>backgroundColor</code>。</p>
<p>实际开发中组件的样式会越来越复杂，我们建议使用StyleSheet.create来集中定义组件的样式。常见的做法是按顺序声明和使用 <code>style</code> 属性，以借鉴 CSS 中的“层叠”做法（即后声明的属性会覆盖先声明的同名属性）。比如像下面这样：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> React, &#123; Component &#125; <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"><span class="keyword">import</span> &#123; AppRegistry, StyleSheet, Text, View &#125; <span class="keyword">from</span> <span class="string">'react-native'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">LotsOfStyles</span> <span class="keyword">extends</span> <span class="title">Component</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;View&gt;</span><br><span class="line">        &lt;Text style=&#123;styles.red&#125;&gt;just red&lt;<span class="regexp">/Text&gt;</span></span><br><span class="line"><span class="regexp">        &lt;Text style=&#123;styles.bigblue&#125;&gt;just bigblue&lt;/</span>Text&gt;</span><br><span class="line">        &lt;Text style=&#123;[styles.bigblue, styles.red]&#125;&gt;bigblue, then red&lt;<span class="regexp">/Text&gt;</span></span><br><span class="line"><span class="regexp">        &lt;Text style=&#123;[styles.red, styles.bigblue]&#125;&gt;red, then bigblue&lt;/</span>Text&gt;</span><br><span class="line">      &lt;<span class="regexp">/View&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">const styles = StyleSheet.create(&#123;</span></span><br><span class="line"><span class="regexp">  bigblue: &#123;</span></span><br><span class="line"><span class="regexp">    color: 'blue',</span></span><br><span class="line"><span class="regexp">    fontWeight: 'bold',</span></span><br><span class="line"><span class="regexp">    fontSize: 30,</span></span><br><span class="line"><span class="regexp">  &#125;,</span></span><br><span class="line"><span class="regexp">  red: &#123;</span></span><br><span class="line"><span class="regexp">    color: 'red',</span></span><br><span class="line"><span class="regexp">  &#125;,</span></span><br><span class="line"><span class="regexp">&#125;);</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">/</span><span class="regexp">/ 注册应用(registerComponent)后才能正确渲染</span></span><br><span class="line"><span class="regexp">/</span><span class="regexp">/ 注意：只把应用作为一个整体注册一次，而不是每个组件/</span>模块都注册</span><br><span class="line">AppRegistry.registerComponent(<span class="string">'LotsOfStyles'</span>, () =&gt; LotsOfStyles);</span><br></pre></td></tr></table></figure>
<h3 id="组件宽度高度"><a href="#组件宽度高度" class="headerlink" title="组件宽度高度"></a><a href="https://reactnative.cn/docs/0.51/height-and-width.html#content" target="_blank" rel="noopener">组件宽度高度</a></h3><p>有两种设定方法：</p>
<p><strong>1. 指定宽高</strong></p>
<p>最简单的给组件设定尺寸的方式就是在样式中指定固定的 <code>width</code>和 <code>height</code>。React Native中的尺寸都是无单位的，表示的是与设备像素密度无关的逻辑像素点。</p>
<p><strong>2. 弹性宽高</strong></p>
<p>在组件样式中使用 <code>flex</code> 可以使其在可利用的空间中动态地扩张或收缩。一般而言我们会使用 <code>flex:1</code> 来指定某个组件扩张以撑满所有剩余的空间。如果有多个并列的子组件使用了 <code>flex:1</code>，则这些子组件会平分父容器中剩余的空间。如果这些并列的子组件的 <code>flex</code> 值不一样，则谁的值更大，谁占据剩余空间的比例就更大（即占据剩余空间的比等于并列组件间 <code>flex</code> 值的比）。</p>
<blockquote>
<p>组件能够撑满剩余空间的前提是其父容器的尺寸不为零。如果父容器既没有固定的 <code>width</code> 和 <code>height</code>，也没有设定 <code>flex</code>，则父容器的尺寸为零。其子组件如果使用了 <code>flex</code>，也是无法显示的。</p>
</blockquote>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> React, &#123; Component &#125; <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"><span class="keyword">import</span> &#123; AppRegistry, View &#125; <span class="keyword">from</span> <span class="string">'react-native'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">FlexDimensionsBasics</span> <span class="keyword">extends</span> <span class="title">Component</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      <span class="comment">// 试试去掉父View中的`flex: 1`。</span></span><br><span class="line">      <span class="comment">// 则父View不再具有尺寸，因此子组件也无法再撑开。</span></span><br><span class="line">      &lt;View style=&#123;&#123;<span class="attr">flex</span>: <span class="number">1</span>&#125;&#125;&gt;</span><br><span class="line">      <span class="comment">// 这里设定的是固定宽高</span></span><br><span class="line">        &lt;View style=&#123;&#123;<span class="attr">width</span>: <span class="number">50</span>, <span class="attr">height</span>: <span class="number">50</span>, <span class="attr">backgroundColor</span>: <span class="string">'powderblue'</span>&#125;&#125; /&gt;</span><br><span class="line">        <span class="comment">// 弹性宽高</span></span><br><span class="line">        &lt;View style=&#123;&#123;<span class="attr">flex</span>: <span class="number">2</span>, <span class="attr">backgroundColor</span>: <span class="string">'skyblue'</span>&#125;&#125; /&gt;</span><br><span class="line">        &lt;View style=&#123;&#123;<span class="attr">flex</span>: <span class="number">3</span>, <span class="attr">backgroundColor</span>: <span class="string">'steelblue'</span>&#125;&#125; /&gt;</span><br><span class="line">      &lt;<span class="regexp">/View&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">AppRegistry.registerComponent('AwesomeProject', () =&gt; FlexDimensionsBasics);</span></span><br></pre></td></tr></table></figure>
<h3 id="flexbox-布局"><a href="#flexbox-布局" class="headerlink" title="flexbox 布局"></a><a href="https://reactnative.cn/docs/0.51/layout-with-flexbox.html#content" target="_blank" rel="noopener">flexbox 布局</a></h3><p>一般用以下三个属性就可以完成布局的要求了，完整的布局样式属性参照 <a href="https://reactnative.cn/docs/0.51/layout-props.html" target="_blank" rel="noopener">这篇文章</a>（也可以参考<a href="https://weibo.com/1712131295/CoRnElNkZ?ref=collection&amp;type=comment#_rnd1526464804589" target="_blank" rel="noopener">图解布局模式</a>）：</p>
<ol>
<li><code>flexDirection</code>: 定义布局主轴。<code>row</code>（水平轴）、<code>column</code>（default）</li>
<li><code>justifyContent</code>: 定义主轴上元素排列的方式。<code>flex-start</code>、<code>center</code>、<code>flex-end</code>、<code>space-around</code>（元素在主轴上均匀分布）、<code>space-between</code>(元素间距均匀分布)</li>
<li><code>alignItems</code>: 定义子元素在次轴（与主轴垂直）上的排列方式。<code>flex-start</code>、<code>flex-end</code>、<code>center</code>、<code>stretch</code>（要使 <code>stretch</code> 选项生效的话，子元素在次轴方向上不能有固定的尺寸）</li>
</ol>
<h1 id="核心组件-components"><a href="#核心组件-components" class="headerlink" title="核心组件 components"></a>核心组件 components</h1><p>react native 提供了很多内置的 components，社区也有很多开发者提供有很多实用的 components（可以直接从 npm 搜 <a href="https://www.npmjs.com/search?q=react-native&amp;page=1&amp;ranking=optimal" target="_blank" rel="noopener"><code>react native</code></a>，或者查看 <a href="http://www.awesome-react-native.com/#components" target="_blank" rel="noopener">awesome react native</a> 里总结的一些列表）</p>
<p>大部分组件，最终都是基于下边这些基本组件写的。<a href="https://facebook.github.io/react-native/docs/components-and-apis.html#user-interface" target="_blank" rel="noopener">components &amp; apis</a> 总结了 basic components、用于 ui 渲染的、list 的、ios specific 的、android specific 的（学习这些相当于是学习 html 基本组件的过程）</p>
<h2 id="View"><a href="#View" class="headerlink" title="View"></a><a href="https://reactnative.cn/docs/0.51/view.html#content" target="_blank" rel="noopener">View</a></h2><p>View 常用作其他组件的容器，来帮助控制布局和样式（类似于 div）</p>
<h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a><a href="https://reactnative.cn/docs/0.51/text.html#content" target="_blank" rel="noopener">Text</a></h2><p>写文本的</p>
<h2 id="TextInput"><a href="#TextInput" class="headerlink" title="TextInput"></a><a href="https://reactnative.cn/docs/0.51/textinput.html#content" target="_blank" rel="noopener">TextInput</a></h2><p>是一个允许用户输入文本的基础组件</p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a><a href="https://reactnative.cn/docs/0.51/image.html#content" target="_blank" rel="noopener">Image</a></h2><p>放图片的</p>
<h2 id="SrollView"><a href="#SrollView" class="headerlink" title="SrollView"></a><a href="https://reactnative.cn/docs/0.51/scrollview.html#content" target="_blank" rel="noopener">SrollView</a></h2><p><code>ScrollView</code> 是一个通用的可滚动的容器，你可以在其中放入多个组件和视图，而且这些组件并不需要是同类型的。<code>ScrollView</code> 不仅可以垂直滚动，还能水平滚动（通过 <code>horizontal</code> 属性来设置）。</p>
<h2 id="StyleSheet"><a href="#StyleSheet" class="headerlink" title="StyleSheet"></a><a href="https://reactnative.cn/docs/0.51/stylesheet.html#content" target="_blank" rel="noopener">StyleSheet</a></h2><p>这其实是一个接口</p>
<p>StyleSheet提供了一种类似CSS样式表的抽象</p>
<h2 id="FlatList"><a href="#FlatList" class="headerlink" title="FlatList"></a><a href="https://reactnative.cn/docs/0.51/flatlist.html#content" target="_blank" rel="noopener">FlatList</a></h2><p>react native 有几种用于显示长列表的组件：<code>flatlist</code>、<code>sectionlist</code>、<code>scrollview</code> 等。</p>
<p><code>FlatList</code> 组件用于显示一个垂直的滚动列表，其中的元素之间结构近似而仅数据不同，且元素个数可以增删。和 <code>ScrollView</code> 不同的是，<code>FlatList</code> 并不立即渲染所有元素，而是优先渲染屏幕上可见的元素。而那些已经渲染好了但移动到了屏幕之外的元素，则会从原生视图结构中移除（以提高性能）.</p>
<p><code>FlatList</code> 组件必须的两个属性是 <code>data</code> 和 <code>renderItem</code>。<code>data</code> 是列表的数据源，而 <code>renderItem</code> 则从数据源中逐个解析数据，然后返回一个设定好格式的组件来渲染。</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">FlatListBasics</span> <span class="keyword">extends</span> <span class="title">Component</span> </span>&#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;View style=&#123;styles.container&#125;&gt;</span><br><span class="line">        &lt;FlatList</span><br><span class="line">          data=&#123;[</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Devin'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Jackson'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'James'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Joel'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'John'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Jillian'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Jimmy'</span>&#125;,</span><br><span class="line">            &#123;<span class="attr">key</span>: <span class="string">'Julie'</span>&#125;,</span><br><span class="line">          ]&#125;</span><br><span class="line">          renderItem=&#123;(&#123;item&#125;) =&gt; <span class="xml"><span class="tag">&lt;<span class="name">Text</span> <span class="attr">style</span>=<span class="string">&#123;styles.item&#125;</span>&gt;</span>&#123;item.key&#125;<span class="tag">&lt;/<span class="name">Text</span>&gt;</span></span>&#125;</span><br><span class="line">        /&gt;</span><br><span class="line">      &lt;<span class="regexp">/View&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="SectionList"><a href="#SectionList" class="headerlink" title="SectionList"></a><a href="https://reactnative.cn/docs/0.51/sectionlist.html#content" target="_blank" rel="noopener">SectionList</a></h2><p>如果要渲染的是一组需要分组的数据，也许还带有分组标签的，那么 <code>SectionList</code> 将是个不错的选择</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;View style=&#123;styles.container&#125;&gt;</span><br><span class="line">        &lt;SectionList</span><br><span class="line">          sections=&#123;[</span><br><span class="line">            &#123;<span class="attr">title</span>: <span class="string">'D'</span>, <span class="attr">data</span>: [<span class="string">'Devin'</span>]&#125;,</span><br><span class="line">            &#123;<span class="attr">title</span>: <span class="string">'J'</span>, <span class="attr">data</span>: [<span class="string">'Jackson'</span>, <span class="string">'James'</span>, <span class="string">'Jillian'</span>, <span class="string">'Jimmy'</span>, <span class="string">'Joel'</span>, <span class="string">'John'</span>, <span class="string">'Julie'</span>]&#125;,</span><br><span class="line">          ]&#125;</span><br><span class="line">          renderItem=&#123;(&#123;item&#125;) =&gt; <span class="xml"><span class="tag">&lt;<span class="name">Text</span> <span class="attr">style</span>=<span class="string">&#123;styles.item&#125;</span>&gt;</span>&#123;item&#125;<span class="tag">&lt;/<span class="name">Text</span>&gt;</span></span>&#125;</span><br><span class="line">          renderSectionHeader=&#123;(&#123;section&#125;) =&gt; <span class="xml"><span class="tag">&lt;<span class="name">Text</span> <span class="attr">style</span>=<span class="string">&#123;styles.sectionHeader&#125;</span>&gt;</span>&#123;section.title&#125;<span class="tag">&lt;/<span class="name">Text</span>&gt;</span></span>&#125;</span><br><span class="line">        /&gt;</span><br><span class="line">      &lt;<span class="regexp">/View&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br></pre></td></tr></table></figure>
<h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><p>接口其实是仅提供接口功能的简单组件。这些组件可能没有渲染功能。</p>
]]></content>
      <tags>
        <tag>react</tag>
        <tag>react native</tag>
      </tags>
  </entry>
  <entry>
    <title>python subprocess</title>
    <url>/2018/09/03/python-subprocess/</url>
    <content><![CDATA[<p>Synchronous vs multiprocessing vs multithreading vs async</p>
<p>Concurrency vs Parralism.</p>
<p>asyncio &amp; threading can run multiple I/O operations at the same time.</p>
<p>Async runs one block of code at a time while threading just one line of code at a time. With async, we have better control of when the execution is given to other block of code but we have to release the execution ourselves.</p>
<ul>
<li><strong>IO bound problems</strong>: use async if your libraries support it and if not, use threading.</li>
<li><strong>CPU bound problems</strong>: use multi-processing.</li>
<li><strong>None above is a problem:</strong> you are probably just fine with synchronous code. You may still want to use async to have the feeling of responsiveness in case your code interacts with a user.</li>
</ul>
<p>Reference: </p>
<ul>
<li><a href="http://www.cnblogs.com/vamei/archive/2012/09/23/2698014.html" target="_blank" rel="noopener">blog: subprocess</a></li>
</ul>
<p>subprocess.call()<br>父进程等待子进程完成<br>返回退出信息(returncode，相当于exit code，见<a href="http://www.cnblogs.com/vamei/archive/2012/09/20/2694466.html" target="_blank" rel="noopener">Linux进程基础</a>)</p>
<p>subprocess.check_call()</p>
<p>父进程等待子进程完成</p>
<p>返回0</p>
<p>检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查(见<a href="http://www.cnblogs.com/vamei/archive/2012/07/10/2582787.html" target="_blank" rel="noopener">Python错误处理</a>)。</p>
<p>subprocess.check_output()</p>
<p>父进程等待子进程完成</p>
<p>返回子进程向标准输出的输出结果</p>
<p>检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。</p>
<ul>
<li><a href="https://docs.python.org/3.7/library/subprocess.html" target="_blank" rel="noopener">official doc: subprocess</a></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">subprocess.run(args, *, stdin=<span class="literal">None</span>, input=<span class="literal">None</span>, stdout=<span class="literal">None</span>, stderr=<span class="literal">None</span>, capture_output=<span class="literal">False</span>, shell=<span class="literal">False</span>, cwd=<span class="literal">None</span>, timeout=<span class="literal">None</span>, check=<span class="literal">False</span>, encoding=<span class="literal">None</span>, errors=<span class="literal">None</span>, text=<span class="literal">None</span>, env=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://docs.python.org/3.7/library/subprocess.html" target="_blank" rel="noopener">config env in subprojcess</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new_env &#x3D; os.environ.copy()</span><br><span class="line">new_env[&#39;MEGAVARIABLE&#39;] &#x3D; &#39;MEGAVALUE&#39;</span><br><span class="line">subprocess.Popen(&#39;path&#39;, env&#x3D;new_env)</span><br></pre></td></tr></table></figure>
<p>an example to use subprojcess:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">                subprocess.check_output([<span class="string">'java'</span>,</span><br><span class="line">                                         <span class="string">'-DCOMMAND='</span> + command,</span><br><span class="line">                                         <span class="string">'-DTENANT_ID='</span> + tenant_id,</span><br><span class="line">                                         <span class="string">'-DMODEL_ID='</span> + model_id,</span><br><span class="line">                                         <span class="string">'-jar'</span>, <span class="string">'xxx.jar'</span>,</span><br><span class="line">                                         ])</span><br><span class="line">            <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">                db_util.reopx_solver - <span class="number">1.0</span> - SNAPSHOT - all.jarport_status(tenant_id, model_id=model_id,</span><br><span class="line">                                                                           msg=<span class="string">f'Failed runner for "<span class="subst">&#123;command&#125;</span>"...'</span>)</span><br><span class="line">                logger.exception(<span class="string">'Could not start solver jar: '</span>, e.output)</span><br><span class="line">                <span class="keyword">raise</span> OPXException(<span class="string">f'Could not start runner for "<span class="subst">&#123;command&#125;</span>".'</span>, e.stderr)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>the jar must be in the same directory as parent process dir</p>
</blockquote>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>responsive-web-design</title>
    <url>/2018/12/13/responsive-web-design/</url>
    <content><![CDATA[<p>自适应一般是设定基准值，宽、高、字体大小都指定为基准值的百分比。当基准值改变时，页面元素、宽高也会按比例变化。</p>
<h1 id="自适应宽度"><a href="#自适应宽度" class="headerlink" title="自适应宽度"></a>自适应宽度</h1><h2 id="不使用绝对宽度"><a href="#不使用绝对宽度" class="headerlink" title="不使用绝对宽度"></a>不使用绝对宽度</h2><p>网页宽度默认等于屏幕宽度。所以大部分时候只要不适用绝对宽度即可实现自适应宽度：</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">body</span>: &#123;</span><br><span class="line">	width: <span class="number">100%</span>;</span><br><span class="line">	<span class="comment">// or width: auto;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果元素是图片，也可以使用 <code>max-width</code> 属性，参见<a href="https://www.w3schools.com/css/css_rwd_images.asp" target="_blank" rel="noopener">responsive web design: image</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img &#123;</span><br><span class="line">  max-width: 100%;</span><br><span class="line">  height: auto;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="使用-media"><a href="#使用-media" class="headerlink" title="使用 media"></a>使用 media</h2><p>这适用于需要针对不同的屏幕，显示不同的排版。利用 <a href="https://developer.mozilla.org/zh-CN/docs/Web/CSS/@media" target="_blank" rel="noopener"><code>@media</code></a> 的 css 规则，可实现根据一个或多个基于设备类型、具体特点和环境的媒体查询来应用样式。</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Media query */</span></span><br><span class="line"><span class="keyword">@media</span> screen <span class="keyword">and</span> (min-width: <span class="number">900px</span>) &#123;</span><br><span class="line">  <span class="selector-tag">article</span> &#123;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">1rem</span> <span class="number">3rem</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Nested media query */</span></span><br><span class="line"><span class="keyword">@supports</span> (display: flex) &#123;</span><br><span class="line">  <span class="keyword">@media</span> screen <span class="keyword">and</span> (min-width: <span class="number">900px</span>) &#123;</span><br><span class="line">    <span class="selector-tag">article</span> &#123;</span><br><span class="line">      <span class="attribute">display</span>: flex;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="css-规则"><a href="#css-规则" class="headerlink" title="css 规则"></a>css 规则</h3><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/CSS/At-rule" target="_blank" rel="noopener">css 规则</a> 相当于给 css 增加的函数。原本 css 只是简单的 json 对象，现在希望 css 更多样化、更容易维护，所以增加了 css 规则。</p>
<p><code>@media</code> 这种属于条件规则组，即接收两个参数，第一个参数是个 bool 条件，第二个参数是执行的语句。仅当条件为 true 时，其后的语句才会执行。</p>
<h1 id="自适应高度"><a href="#自适应高度" class="headerlink" title="自适应高度"></a>自适应高度</h1><p>同样的，要使得高度自适应，也必须指定一个基准值。但不同于宽度（默认是屏幕宽度），高度通常是不固定的，要确定基准值也就麻烦些。</p>
<h2 id="父容器的高度可确定"><a href="#父容器的高度可确定" class="headerlink" title="父容器的高度可确定"></a>父容器的高度可确定</h2><blockquote>
<p>For the height of a div to be responsive, it must be inside a parent element with a defined height to derive it’s relative height from.</p>
</blockquote>
<p>如果父容器的高度可确定，则同上，采用百分比即可实现自适应。</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line">parent: &#123;</span><br><span class="line">    height: <span class="number">100px</span>;</span><br><span class="line">    </span><br><span class="line">    child: &#123;</span><br><span class="line">        height: <span class="number">80%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="根据宽度变化，同比例改变高度"><a href="#根据宽度变化，同比例改变高度" class="headerlink" title="根据宽度变化，同比例改变高度"></a>根据宽度变化，同比例改变高度</h2><p>页面宽度的自适应容易实现，可以基于宽度的变化，同比例更改高度。</p>
<h3 id="利用-padding"><a href="#利用-padding" class="headerlink" title="利用 padding"></a>利用 <code>padding</code></h3><p>参见<a href="https://stackoverflow.com/questions/11535827/responsive-height-proportional-to-width" target="_blank" rel="noopener">responsive height proportional to width</a>，设定 <code>height</code> 为 0，<code>padding</code> 为百分比</p>
<blockquote>
<p>In all browsers, when padding is specified in %, it’s calculated relative to the parent element’s width. </p>
</blockquote>
<p>Html:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content"</span>&gt;</span></span><br><span class="line">    Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content2"</span>&gt;</span></span><br><span class="line">Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Css:</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.content</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">padding-bottom</span>: <span class="number">30%</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="number">#7ad</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.wrapper</span> &#123;</span><br><span class="line">    <span class="attribute">position</span>: relative;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">padding-bottom</span>: <span class="number">30%</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="number">#7ad</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.wrapper</span> <span class="selector-class">.content2</span> &#123;</span><br><span class="line">    <span class="attribute">position</span>: absolute;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="number">#7da</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="利用-viewport-units"><a href="#利用-viewport-units" class="headerlink" title="利用 viewport units"></a>利用 viewport units</h3><p>新版本的浏览器支持 <a href="https://caniuse.com/#feat=viewport-units" target="_blank" rel="noopener">viewport units</a>，eg.</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="attribute">width</span>: <span class="number">100vw</span>;</span><br><span class="line"><span class="attribute">height</span>: <span class="number">30vw</span>; <span class="comment">/* 30% of width */</span></span><br></pre></td></tr></table></figure>
<h2 id="使用-media-1"><a href="#使用-media-1" class="headerlink" title="使用 media"></a>使用 media</h2><p>同宽度，指定不同 media 情况下，高度显示不同。</p>
]]></content>
      <tags>
        <tag>css</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title>python basic</title>
    <url>/2019/03/26/python-basic/</url>
    <content><![CDATA[<p><a href="https://www.sololearn.com/play/python" target="_blank" rel="noopener">great free learning website with quiz</a></p>
<p><a href="http://pythontutor.com/live.html#mode=edit" target="_blank" rel="noopener">great online editor &amp; debugger</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41381773" target="_blank" rel="noopener">Python学习资料/文章/指南整理</a></p>
<h1 id="Zen-of-Python"><a href="#Zen-of-Python" class="headerlink" title="Zen of Python"></a>Zen of Python</h1><p>By typing <code>import this</code>, you can see the zen of python. Some need more explanation:</p>
<p>Explicit is better than implicit: It is best to spell out exactly what your code is doing. This is why adding a numeric string to an integer requires explicit conversion, rather than having it happen behind the scenes, as it does in other languages.<br>Flat is better than nested: Heavily nested structures (lists of lists, of lists, and on and on…) should be avoided.<br>Errors should never pass silently: In general, when an error occurs, you should output some sort of error message, rather than ignoring it.</p>
<h2 id="PEP"><a href="#PEP" class="headerlink" title="PEP"></a>PEP</h2><p><strong>Python Enhancement Proposals (PEP)</strong> are suggestions for improvements to the language, made by experienced Python developers.</p>
<p><strong>PEP 8</strong> is a style guide on the subject of writing readable code<br><strong>PEP 20</strong>: The Zen of Python<br><strong>PEP 257</strong>: Style Conventions for Docstrings</p>
<h3 id="Naming-conventions"><a href="#Naming-conventions" class="headerlink" title="Naming conventions"></a>Naming conventions</h3><p><strong>PEP 8</strong> is a style guide on the subject of writing readable code. It contains a number of guidelines in reference to variable names, which are summarized here:<br>- modules should have short, all-lowercase names;<br>- class names should be in the CapWords style;<br>- most variables and function names should be lowercase_with_underscores;<br>- constants (variables that never change value) should be CAPS_WITH_UNDERSCORES;<br>- names that would clash with Python keywords (such as ‘class’ or ‘if’) should have a trailing underscore.</p>
<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><h2 id="Brew-install"><a href="#Brew-install" class="headerlink" title="Brew install"></a>Brew install</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ brew install python</span><br></pre></td></tr></table></figure>
<h2 id="install-multiple-version"><a href="#install-multiple-version" class="headerlink" title="install multiple version"></a>install multiple version</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use pyenv to install multiple version</span></span><br><span class="line">$ brew update</span><br><span class="line">$ brew install pyenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clone the repository to to get the latest version of pyenv</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/pyenv/pyenv.git ~/.pyenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># define envs</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'export PYENV_ROOT="$HOME/.pyenv"'</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'export PATH="$PYENV_ROOT/bin:$PATH"'</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># get all versions that can be installed</span></span><br><span class="line">$ pyenv install --list</span><br><span class="line">$ pyenv install 3.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># all current installed</span></span><br><span class="line">$ pyenv versions</span><br><span class="line"><span class="comment"># current active</span></span><br><span class="line">$ pyenv version</span><br><span class="line"></span><br><span class="line"><span class="comment"># set as gloabl version</span></span><br><span class="line">$ pyenv global 3.7</span><br><span class="line"><span class="comment"># set a local version </span></span><br><span class="line"><span class="comment"># This command creates a .python-version file in your current directory. If you have pyenv active in your environment, this file will automatically activate this version for you.</span></span><br><span class="line">$ pyenv <span class="built_in">local</span> 3.7</span><br><span class="line"><span class="comment"># set as shell version</span></span><br><span class="line"><span class="comment"># This command activates the version specified by setting the PYENV_VERSION environment variable. This command overwrites any applications or global settings you may have. If you want to deactivate the version, you can use the --unset flag.</span></span><br><span class="line">$ pyenv shell 3.7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># check the version</span></span><br><span class="line">$ python3 --version</span><br><span class="line"></span><br><span class="line"><span class="comment"># add this to ~/.zshrc if the global command not work, to active the pyenv shell features</span></span><br><span class="line">$ <span class="built_in">eval</span> <span class="string">"<span class="variable">$(pyenv init -)</span>"</span></span><br></pre></td></tr></table></figure>
<p><img src="https://files.realpython.com/media/pyenv-pyramid.d2f35a19ded9.png" alt="Pyenv pyramid for order of resolution"></p>
<p>Other pythons installed:</p>
<ul>
<li>/usr/local/bin</li>
<li>/usr/local/Cellar</li>
</ul>
<h1 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h1><p><a href="https://pypi.org/project/pip/" target="_blank" rel="noopener">pip</a> is the package installer for python.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ pip install package-name</span><br></pre></td></tr></table></figure>
<p>On windows, to use <code>pip</code> after python installation, you need to config both the python &amp; pip path.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:c/users/xiaoming/AppData/Programs/Python/Python37:c/users/xiaoming/AppData/Programs/Python/Python37/Scripts</span><br></pre></td></tr></table></figure>
<h2 id="common-used-commands"><a href="#common-used-commands" class="headerlink" title="common used commands"></a>common used commands</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># list all installed packages</span></span><br><span class="line">$ pip freeze</span><br><span class="line"></span><br><span class="line"><span class="comment"># downgrade pip</span></span><br><span class="line">$ python -m install pip=20.2.4</span><br><span class="line"><span class="comment"># upgrade pip</span></span><br><span class="line">$ python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure>
<h1 id="Packaging"><a href="#Packaging" class="headerlink" title="Packaging"></a>Packaging</h1><p>In Python, the term <strong>packaging</strong> refers to putting modules you have written in a standard format, so that other programmers can install and use them with ease.<br>This involves use of the modules <strong>setuptools</strong> and <strong>distutils</strong>.</p>
<ol>
<li><p>organize existing files correctly. Place all of the files you want to put in a library in the same parent directory. This directory should also contain a file called <strong>__init__.py</strong>, which can be blank but must be present in the directory. <code>__init__.py</code> turns a directory to a module.<br>This directory goes into another directory containing the readme and license, as well as an important file called <strong>setup.py</strong>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SoloLearn&#x2F;</span><br><span class="line"> LICENSE.txt</span><br><span class="line"> README.txt</span><br><span class="line"> setup.py</span><br><span class="line"> sololearn&#x2F;</span><br><span class="line">    __init__.py</span><br><span class="line">    sololearn.py</span><br><span class="line">    sololearn2.py</span><br></pre></td></tr></table></figure>
</li>
<li><p>Write the <strong>setup.py</strong> file. This contains information necessary to assemble the package so it can be uploaded to <strong>PyPI</strong> and installed with <strong>pip</strong> (name, version, etc.).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> setup(</span><br><span class="line">   name=<span class="string">'SoloLearn'</span>, </span><br><span class="line">   version=<span class="string">'0.1dev'</span>,</span><br><span class="line">   packages=[<span class="string">'sololearn'</span>,],</span><br><span class="line">   license=<span class="string">'MIT'</span>, </span><br><span class="line">   long_description=open(<span class="string">'README.txt'</span>).read(),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Write Other Files   </p>
</li>
<li><p>Build a source distribution, use the command line to navigate to the directory containing setup.py, and run the command <code>python setup.py sdist</code>.</p>
<ol>
<li>Run <code>python setup.py bdist</code> or, for Windows, <code>python setup.py bdist_wininst</code> to build a binary distribution.</li>
</ol>
</li>
<li><p>Upload the package to <strong>PyPI</strong>. Use <code>python setup.py register</code>, followed by <code>python setup.py sdist upload</code> to upload a package.</p>
</li>
<li><p>install a package with <code>python setup.py install</code>.</p>
</li>
</ol>
<h2 id="init-py"><a href="#init-py" class="headerlink" title="__init__.py"></a>__init__.py</h2><p><a href="https://www.cnblogs.com/Lands-ljk/p/5880483.html" target="_blank" rel="noopener">Python <strong>init</strong>.py 作用详解</a></p>
<p><code>__init__.py</code> 文件的作用是将文件夹变为一个Python模块,Python 中的每个模块的包中，都有<code>__init__.py</code> 文件。</p>
<p>通常<code>__init__.py</code> 文件为空，但是我们还可以为它增加其他的功能。我们在导入一个包时，实际上是导入了它的<code>__init__.py</code>文件。这样我们可以在<code>__init__.py</code>文件中批量导入我们所需要的模块，而不再需要一个一个的导入。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># package</span><br><span class="line"># __init__.py</span><br><span class="line">import re</span><br><span class="line">import urllib</span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># a.py</span><br><span class="line">import package </span><br><span class="line">print(package.re, package.urllib, package.sys, package.os)</span><br></pre></td></tr></table></figure>
<p>注意这里访问<code>__init__.py</code>文件中的引用文件，需要加上包名。</p>
<p><code>__init__.py</code>中还有一个重要的变量，<code>__all__</code>, 它用来将模块全部导入。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># __init__.py</span><br><span class="line">__all__ &#x3D; [&#39;os&#39;, &#39;sys&#39;, &#39;re&#39;, &#39;urllib&#39;]</span><br><span class="line"></span><br><span class="line"># a.py</span><br><span class="line">from package import *</span><br></pre></td></tr></table></figure>
<p>这时就会把注册在<code>__init__.py</code>文件中<code>__all__</code>列表中的模块和包导入到当前文件中来。</p>
<p>可以了解到，<code>__init__.py</code>主要控制包的导入行为。</p>
<h2 id="setup-py"><a href="#setup-py" class="headerlink" title="setup.py"></a>setup.py</h2><p><a href="https://setuptools.readthedocs.io/en/latest/" target="_blank" rel="noopener">setuptools</a> is the packaging tool for python (like maven/gradle for java?)</p>
<p>When coding in local, you can use <code>pip install</code> to install dependency. However, when deploy a python project, you need to package all dependency, modules into one project. You need <code>setup.py</code>.</p>
<p>When organize python in modules, you need to know and import the function in other modules. In compilation stage, you need to feel the other modules&amp;packages. <code>setup.py</code> does the magic. (This is my guess)</p>
<h2 id="Packaging-for-users"><a href="#Packaging-for-users" class="headerlink" title="Packaging for users"></a>Packaging for users</h2><p>For normal users who don’t have python on computer, we need to convert scripts to executables for them.</p>
<p>For Windows, <strong>py2exe</strong> or <strong>PyInstaller</strong> or <strong>cx_Freeze</strong> can be used to package a Python script, along with the libraries it requires, into a single executable.<br>For Macs, use <strong>py2app</strong>, <strong>PyInstaller</strong> or <strong>cx_Freeze</strong>.</p>
<h1 id="Some-special-grammars"><a href="#Some-special-grammars" class="headerlink" title="Some special grammars"></a>Some special grammars</h1><h2 id="if-name-‘-main-’"><a href="#if-name-‘-main-’" class="headerlink" title="if _name__ == ‘\_main__’"></a>if _<em>name__ == ‘\</em>_main__’</h2><p><a href="https://blog.csdn.net/yjk13703623757/article/details/77918633" target="_blank" rel="noopener">如何简单地理解Python中的if _<em>name__ == ‘\</em>_main__’</a></p>
<p>If you are <code>test.py</code>, then other module knows you as <code>__name__==&#39;test&#39;</code>, and you know yourself as <code>__name==&#39;__main__&#39;</code>.</p>
<p>The code block in <code>if __name__ == &#39;__main__&#39;</code> will only be executed when the file is executed directly. If other module import <code>test.py</code> as module, the code block won’t be executed.</p>
<p>It’s kind of like main function in java/c, but with big difference. Python is <strong>a script language</strong> — interpretive execution, which can be run without any so-called main entry. You don’t need to make <code>test.py</code> runnable by providing <code>main</code>, but you can provide <code>if __name__ == &#39;__main__&#39;</code> if you want this block only executed when called directly rather than as module.</p>
<h2 id="else"><a href="#else" class="headerlink" title="else"></a>else</h2><p>The <strong>else</strong> statement can be used in:</p>
<ul>
<li>the <strong>if</strong> statement</li>
<li>a <strong>for</strong> or <strong>while</strong> loop, the code within it is called if the loop finishes normally / completely (when a <strong>break</strong> statement does not cause an exit from the loop).</li>
<li>the <strong>try/except</strong> statements, the code within it is only executed if no error occurs in the <strong>try</strong> statement.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">if_else</span><span class="params">(a)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> a == <span class="string">'if'</span>:</span><br><span class="line">    print(<span class="string">'this is in if'</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'this is not if!'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_else</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">2</span>:</span><br><span class="line">      <span class="comment"># print('Will get here. The else won\'t be executed')</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'first for finish completely'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">10</span>:</span><br><span class="line">      <span class="comment"># print('Never get here. The else will be executed')</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'second for finish completely'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_else</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    print(<span class="number">5</span>/<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'there is exception, the first else won\'t be executed'</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'first try finished successfully'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    print(<span class="number">5</span>/<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'there is no exception, the else will be executed'</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'second try finished successfully'</span>)</span><br><span class="line"></span><br><span class="line">if_else(<span class="string">'not_if'</span>) <span class="comment"># this is not if!</span></span><br><span class="line">for_else() <span class="comment"># second for finish completely</span></span><br><span class="line">try_else()</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">there is exception, the first else won't be executed</span></span><br><span class="line"><span class="string">5.0</span></span><br><span class="line"><span class="string">second try finished successfully</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h1 id="class"><a href="#class" class="headerlink" title="class"></a>class</h1><p>Classes are created using the keyword <strong>class</strong> and an indented block, which contains class <strong>methods</strong> (which are functions). All methods must have <strong>self</strong> as their first parameter, you do not need to include it when you call the methods. Within a method definition, <strong>self</strong> refers to the instance calling the method.</p>
<p>To inherit a class from another class, put the superclass name in parentheses after the class name.</p>
<p>Classes can also have <strong>class attributes</strong>, created by assigning variables within the body of the class. These can be accessed either from instances of the class, or the class itself.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, color)</span>:</span></span><br><span class="line">    self.name = name</span><br><span class="line">    self.color = color</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shout</span><span class="params">(self)</span>:</span></span><br><span class="line">    print(<span class="string">'ha'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animal)</span>:</span></span><br><span class="line">  legs = <span class="number">4</span>    <span class="comment"># the class attribute</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shout</span><span class="params">(self)</span>:</span></span><br><span class="line">    print(<span class="string">'wong'</span>)</span><br><span class="line">    super().shout()    <span class="comment"># call super method</span></span><br><span class="line"></span><br><span class="line">Dog.legs    <span class="comment"># 4</span></span><br><span class="line">d = Dog(<span class="string">'heidou'</span>, <span class="string">'black'</span>)</span><br><span class="line">d.legs    <span class="comment"># 4</span></span><br><span class="line">d.name     <span class="comment"># 'heidou'</span></span><br><span class="line">d.shout()    </span><br><span class="line"><span class="comment"># wong</span></span><br><span class="line"><span class="comment"># ha</span></span><br></pre></td></tr></table></figure>
<h2 id="new-vs-init"><a href="#new-vs-init" class="headerlink" title="__new__ vs __init__"></a><code>__new__</code> vs <code>__init__</code></h2><p><a href="https://www.agiliq.com/blog/2012/06/__new__-python/" target="_blank" rel="noopener">__new__ in python</a></p>
<p>_<em>new__ is a static method which creates an instance. It allocates memory for an object. \</em>_init__ initialize the value of the object (instance). <code>__new__</code> is rarely overriden.</p>
<p>When you are instantiate an instance by calling the class, the <code>__new__</code> gets called first to create the object in memory, and then <code>__init__</code> is called to initialize it.</p>
<p>__new__ must return the created object. Only when <code>__new__</code> returns the created instance then <code>__init__</code> gets called. If <code>__new__</code> does not return an instance then <code>__init__</code> would not be called.</p>
<p>The <code>__new__</code> definition:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__new__(cls, *args, **kwargs)</span><br></pre></td></tr></table></figure>
<p>An example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, *args, **kwargs)</span>:</span></span><br><span class="line">    print(cls)</span><br><span class="line">    print(args)</span><br><span class="line">    print(kwargs)</span><br><span class="line">    obj = object.__new__(cls)</span><br><span class="line">    setattr(obj, <span class="string">'created_at'</span>, dt.datetime.now())</span><br><span class="line">    <span class="keyword">return</span> obj <span class="comment"># if no return here, __init__ won't be called later</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, named)</span>:</span></span><br><span class="line">    print(<span class="string">'in init'</span>)</span><br><span class="line">    self.a = a</span><br><span class="line">    self.b = named</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'a=&#123;0&#125;, b=&#123;1&#125;, created_at: &#123;2&#125;'</span>.format(self.a, self.b, self.created_at)</span><br><span class="line"></span><br><span class="line">a = A(<span class="number">1</span>, named=<span class="number">2</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;class '__main__.A'&gt;</span></span><br><span class="line"><span class="string">(1,)</span></span><br><span class="line"><span class="string">&#123;'named': 2&#125;</span></span><br><span class="line"><span class="string">in init</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">print(a) <span class="comment"># a=1, b=2, created_at: 2020-10-19 15:22:19.558501</span></span><br></pre></td></tr></table></figure>
<h2 id="del"><a href="#del" class="headerlink" title="__del__"></a><code>__del__</code></h2><p>Destruction of an object occurs when its <strong>reference count</strong> reaches zero.</p>
<p>The <strong>del</strong> statement reduces the reference count of an object by one, and this often leads to its deletion.<br>The magic method for the <strong>del</strong> statement is <code>__del__</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">5</span></span><br><span class="line"><span class="keyword">del</span> a</span><br></pre></td></tr></table></figure>
<h2 id="private"><a href="#private" class="headerlink" title="private"></a>private</h2><p>The Python philosophy is often stated as <strong>“we are all consenting adults here”</strong>, meaning that you shouldn’t put arbitrary restrictions on accessing parts of a class. Hence there are no ways of enforcing a method or attribute be strictly private. However, there are ways to discourage people from accessing parts of a class, such as by denoting that it is an implementation detail, and should be used at their own risk.</p>
<p>Weakly private methods and attributes have a <strong>single underscore</strong> at the beginning. This signals that they are private, and <strong>shouldn’t be</strong> used by external code. Its only actual effect is that <strong>from module_name import *</strong> won’t import variables that start with a single underscore.</p>
<p>Strongly private methods and attributes have a <strong>double underscore</strong> at the beginning of their names. This causes their names to be mangled, which means that they can’t be accessed from outside the class by the name. The purpose of this isn’t to ensure that they are kept private, but to avoid bugs if there are subclasses that have methods or attributes with the same names. Basically, Python protects those members by internally changing the name to include the class name.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">  __egg = <span class="number">7</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a)</span>:</span></span><br><span class="line">    self.__a = a</span><br><span class="line"></span><br><span class="line">A._A__egg <span class="comment"># 7</span></span><br><span class="line">a = A(<span class="string">'a'</span>)</span><br><span class="line">a._A__a <span class="comment"># 'a'</span></span><br></pre></td></tr></table></figure>
<h2 id="class-methods-vs-instance-methods-vs-static-methods"><a href="#class-methods-vs-instance-methods-vs-static-methods" class="headerlink" title="class methods vs instance methods vs static methods"></a>class methods vs instance methods vs static methods</h2><p><strong>Instance methods</strong> are called by a instance, which is passed to the <strong>self</strong> parameter of the method.</p>
<p><strong>Class methods</strong> are called by a class, which is passed to the <strong>cls</strong> parameter of the method.<br>A common use of these are factory methods, which instantiate an instance of a class, using different parameters than those usually passed to the class constructor.<br>Class methods are marked with a <strong>classmethod decorator</strong>.</p>
<p><strong>Static methods</strong> are similar to class methods, except they don’t receive any additional arguments; they are identical to normal functions that belong to a class.<br>They are marked with the <strong>staticmethod</strong> decorator.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">    self.a = a</span><br><span class="line">    self.b = b</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">area</span><span class="params">(self)</span>:</span></span><br><span class="line">    print(self.a * self.b)</span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">new_square</span><span class="params">(cls, a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cls(a, a)</span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validate_int</span><span class="params">(a)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> isinstance(a, int), <span class="string">'must be int'</span></span><br><span class="line"></span><br><span class="line">r = Rectangle(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">r.area() <span class="comment"># 20</span></span><br><span class="line">s = Rectangle.new_square(<span class="number">4</span>)</span><br><span class="line">s.area() <span class="comment"># 16</span></span><br><span class="line">Rectangle.validate_int(<span class="string">'fd'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><p><strong>Properties</strong> are created by putting the <strong>property</strong> decorator above a method, which means when the instance attribute with the same name as the method is accessed, the method will be called instead.<br>One common use of a property is to make an attribute <strong>read-only</strong>.</p>
<p>Properties can also be set by defining <strong>setter/getter</strong> functions.<br>The <strong>setter</strong> function sets the corresponding property’s value. To define a <strong>setter</strong>, you need to use a decorator of the same name as the property, followed by a dot and the <strong>setter</strong> keyword.</p>
<p>​<code>`</code>python<br>class Pizza:<br>  def <strong>init</strong>(self, a):<br>    self._a = a<br>  @property<br>  def a_list(self):<br>    return [self._a] * 10<br>  @a_list.setter<br>  def a_list(self, value):<br>    self._a = value[0]</p>
<p>p = Pizza(7)<br>p.a_list # [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]<br>p.a_list = [1]<br>p.a_list # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Magic Methods</span><br><span class="line"></span><br><span class="line">**Magic methods** are special methods which have **double underscores** at the beginning and end of their names. They are used to create functionality that can&#39;t be represented as a normal method.</span><br><span class="line"></span><br><span class="line">&#96;__init__&#96; is the constructor.</span><br><span class="line"></span><br><span class="line">Magic methods for common operators (operator overload):</span><br><span class="line"></span><br><span class="line">&#96;__add__&#96; for -</span><br><span class="line">&#96;__sub__&#96; for -</span><br><span class="line">&#96;__mul__&#96; for *</span><br><span class="line">&#96;__truediv__&#96; for &#x2F;</span><br><span class="line">&#96;__floordiv__&#96; for &#x2F;&#x2F;</span><br><span class="line">&#96;__mod__&#96; for %</span><br><span class="line">&#96;__pow__&#96; for **</span><br><span class="line">&#96;__and__&#96; for &amp;</span><br><span class="line">&#96;__xor__&#96; for ^</span><br><span class="line">&#96;__or__&#96; for |</span><br><span class="line"></span><br><span class="line">Magic methods for comparisons (If &#96;__ne__&#96; is not implemented, it returns the opposite of &#96;__eq__&#96;):</span><br><span class="line"></span><br><span class="line">&#96;__lt__&#96; for &lt;</span><br><span class="line">&#96;__le__&#96; for &lt;&#x3D;</span><br><span class="line">&#96;__eq__&#96; for &#x3D;&#x3D;</span><br><span class="line">&#96;__ne__&#96; for !&#x3D;</span><br><span class="line">&#96;__gt__&#96; for &gt;</span><br><span class="line">&#96;__ge__&#96; for &gt;&#x3D;</span><br><span class="line"></span><br><span class="line">Magic methods for making classes act like containers:</span><br><span class="line">&#96;__len__&#96; for len()</span><br><span class="line">&#96;__getitem__&#96; for indexing</span><br><span class="line">&#96;__setitem__&#96; for assigning to indexed values</span><br><span class="line">&#96;__delitem__&#96; for deleting indexed values</span><br><span class="line">&#96;__iter__&#96; for iteration over objects (e.g., in for loops)</span><br><span class="line">&#96;__contains__&#96; for in</span><br><span class="line"></span><br><span class="line">Magic methods for converting objects to built-in types:</span><br><span class="line">&#96;__int__&#96; for int()</span><br><span class="line">&#96;__str__&#96; for str()</span><br><span class="line"></span><br><span class="line">### &#96;r&#96; methods&lt;a name &#x3D; &quot;r-methods&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">There are equivalent **r** methods for all magic methods that overloads operators. For example, the expression **x + y** is translated into **x.__add__(y)**. However, if x hasn&#39;t implemented __add__, and x and y are of different types, then **y.__radd__(x)** is called.</span><br><span class="line"></span><br><span class="line">&quot;r&quot; stands for &quot;right&quot;, meaning that the operator passed is on the right. If the conversion of the 1st type to the 2nd isn&#39;t supported, it simply tries calling the inverse one (2nd conversion to the 1st) for the other type.</span><br><span class="line"></span><br><span class="line">**r** methods can be used when you want to overload operator between a thirt lib type and your type. Since you may not modify the third library, you can add the reverse **r** method in your type.</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">class PairNumber:</span><br><span class="line">def __init__(self, n1, n2):</span><br><span class="line">  self.n1 &#x3D; n1</span><br><span class="line">  self.n2 &#x3D; n2</span><br><span class="line">def __floordiv__(self, other):</span><br><span class="line">  return PairNumber(self.n1&#x2F;&#x2F;other.n1, self.n2 &#x2F;&#x2F; other.n2)</span><br><span class="line">def __rtruediv__(self, other):</span><br><span class="line">  return PairNumber(other&#x2F;self.n1, other&#x2F;self.n2)</span><br><span class="line">def __repr__(self):</span><br><span class="line">  return &#39;PairNumber&#123;0&#125;&#39;.format((self.n1, self.n2))</span><br><span class="line">def __str__(self):</span><br><span class="line">  return str((self.n1, self.n2))</span><br><span class="line"></span><br><span class="line">class IntPairNumber(PairNumber):</span><br><span class="line">def __int__(self):</span><br><span class="line">  return self.n1 + self.n2</span><br><span class="line"></span><br><span class="line">p1 &#x3D; IntPairNumber(4, 10)</span><br><span class="line">p2 &#x3D; IntPairNumber(3, 19)</span><br><span class="line">p1 &#x2F;&#x2F; p2 # PairNumber(1, 0)</span><br><span class="line">print(p1 &#x2F;&#x2F; p2) # (1, 0)</span><br><span class="line">str(2 &#x2F; p1)        # PairNumber(0.5, 0.2)</span><br><span class="line">int(p1)                # 14</span><br></pre></td></tr></table></figure>
<h3 id="call-method"><a href="#call-method" class="headerlink" title="__call__ method"></a><code>__call__</code> method</h3><p><code>__call__</code> method can call an object as a function. Thus you can transfer the object as a func parameter in a function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PairNumber</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n1, n2)</span>:</span></span><br><span class="line">    self.n1 = n1</span><br><span class="line">    self.n2 = n2</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.n1 + self.n2 + other</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addTwice</span><span class="params">(func, other)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> func(other) + other</span><br><span class="line"></span><br><span class="line">p1 = PairNumber(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">p1(<span class="number">4</span>)        <span class="comment"># 7</span></span><br><span class="line">addTwice(p1, <span class="number">4</span>)    <span class="comment"># 11</span></span><br></pre></td></tr></table></figure>
<h3 id="str-vs-repr"><a href="#str-vs-repr" class="headerlink" title="__str__ vs __repr__"></a><code>__str__</code> vs <code>__repr__</code></h3><p>In short, the goal of <code>__repr__</code> is to be unambiguous and <code>__str__</code> is to be readable. If <code>__repr__</code> is defined, and <code>__str__</code> is not, the object will behave as though <code>__str__=__repr__</code>.</p>
<p>The commandline will show the content in <code>__repr__</code> when you simply type the object. And when print(some_obj), it will show the content in <code>__str__</code> of that object. So many would say: <code>__repr__</code> is for developers, <code>__str__</code> is for customers. e.g. obj = uuid.uuid1(), obj._<em>str\</em>_() is “2d7fc7f0-7706-11e9-94ae-0242ac110002” and obj.__repr__() is “UUID(‘2d7fc7f0-7706-11e9-94ae-0242ac110002’)”.</p>
<p><strong>To sum up</strong>: implement <code>__repr__</code> for any class you implement. This should be second nature. Implement <code>__str__</code> if you think it would be useful to have a string version which errs on the side of readability.</p>
<p>See example in <a href="#r-methods">r <strong>method</strong></a></p>
<h1 id="Opening-Files"><a href="#Opening-Files" class="headerlink" title="Opening Files"></a>Opening Files</h1><p>You can specify the <strong>mode</strong> used to open a file by applying a second argument to the <strong>open</strong> function.<br>Sending “r” means open in read mode, which is the default.<br>Sending “w” means write mode, for <strong><em>rewriting</em></strong> the contents of a file.<br>Sending “a” means append mode, for adding new content to the end of the file.</p>
<p>Adding “b” to a mode opens it in <strong>binary</strong> mode, which is used for non-text files (such as image and sound files).<br><strong>For example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># write mode</span></span><br><span class="line">open(<span class="string">"filename.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read mode</span></span><br><span class="line">open(<span class="string">"filename.txt"</span>, <span class="string">"r"</span>)</span><br><span class="line">open(<span class="string">"filename.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># binary write mode</span></span><br><span class="line">open(<span class="string">"filename.txt"</span>, <span class="string">"wb"</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>You can use the + sign with each of the modes above to give them extra access to files. For example, r+ opens the file for both reading and writing.</p>
<p>Use <code>help(open)</code> to see the complete file modes supported.</p>
</blockquote>
<ul>
<li><p>“r”<br>Read from file - YES<br>Write to file - NO<br><strong>Create file if not exists - NO</strong><br>Truncate file to zero length - NO<br>Cursor position - BEGINNING</p>
</li>
<li><p>“r+”  ====&gt; Opens a file for both reading and writing (write from the current cursor, that is, if you already call <code>file.read()</code>, then it means cursor is already at the end, then the writing is appending. However, if you start with <code>file.write()</code>, the cursor is at the beginning, thus it will <strong><em>rewrite</em></strong> from the beginning)<br>Read from file - YES<br>Write to file - YES<br><strong>Create file if not exists - NO</strong><br><strong>Truncate file to zero length - NO</strong><br>Cursor position - BEGINNING</p>
</li>
<li><p>“w”<br>Read from file - NO<br>Write to file - YES<br><strong>Create file if not exists - YES</strong><br><strong>Truncate file to zero length - YES</strong><br>Cursor position - BEGINNING</p>
</li>
<li><p>“w+” ===&gt; Opens a file for both writing and reading<br>Read from file - YES<br>Write to file - YES<br><strong>Create file if not exists - YES</strong><br><strong>Truncate file to zero length - YES</strong><br>Cursor position - BEGINNING</p>
</li>
<li><p>“a”<br>Read from file - NO<br>Write to file - YES<br><strong>Create file if not exists - YES</strong><br><strong>Truncate file to zero length - NO</strong><br>Cursor position - END</p>
</li>
<li><p>“a+” ===&gt; Opens a file for both appending and reading<br>Read from file - YES<br>Write to file - YES<br><strong>Create file if not exists - YES</strong><br><strong>Truncate file to zero length - NO</strong><br>Cursor position - END</p>
</li>
</ul>
<h1 id="Iterable-containers"><a href="#Iterable-containers" class="headerlink" title="Iterable containers"></a>Iterable containers</h1><h2 id="List-string-tuple-slicing"><a href="#List-string-tuple-slicing" class="headerlink" title="List/string/tuple slicing"></a>List/string/tuple slicing</h2><p>slice can have 3 parameters:</p>
<ol>
<li>Start index (included, count from end of the list if negative, default as 0)</li>
<li>End index (excluded, count from end of the list if negative, default as end of the list)</li>
<li>Step</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">str    = <span class="string">'0123456'</span></span><br><span class="line">tp = tuple(str)    <span class="comment"># ('0', '1', '2', '3', '4', '5', '6')</span></span><br><span class="line">lst = list(str) <span class="comment"># ['0', '1', '2', '3', '4', '5', '6']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get first 2</span></span><br><span class="line">str[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get last 2</span></span><br><span class="line">str[<span class="number">5</span>:]</span><br><span class="line">str[<span class="number">-2</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get odd number (the third number is the step)</span></span><br><span class="line">str[<span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># reverse</span></span><br><span class="line">str[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the 3 number from 1(included) and reverse it</span></span><br><span class="line">str[<span class="number">3</span>:<span class="number">0</span>:<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="List-Comprehensions"><a href="#List-Comprehensions" class="headerlink" title="List Comprehensions"></a>List Comprehensions</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cubes = [i**<span class="number">3</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]    <span class="comment"># [0, 1, 8, 27, 64]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with if statement</span></span><br><span class="line">evenSquares = [i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>) <span class="keyword">if</span> i ** <span class="number">2</span> % <span class="number">2</span> == <span class="number">0</span>] <span class="comment"># [0, 4, 16]</span></span><br><span class="line">evenSquares = [i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>,<span class="number">2</span>)] <span class="comment"># [0, 4, 16]</span></span><br><span class="line">evens =  [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>) <span class="keyword">if</span> i%<span class="number">2</span> == <span class="number">0</span>] <span class="comment"># [0, 2, 4]</span></span><br></pre></td></tr></table></figure>
<h3 id="all-amp-any"><a href="#all-amp-any" class="headerlink" title="all &amp; any"></a>all &amp; any</h3><p>use all() / any() to check a list of bool.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">if</span> all([i &gt; <span class="number">5</span> <span class="keyword">for</span> i <span class="keyword">in</span> nums]):</span><br><span class="line">   print(<span class="string">"All larger than 5"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> any([i % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> nums]):</span><br><span class="line">   print(<span class="string">"At least one is even"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a tuple</span></span><br><span class="line">t1 = <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line">t2 = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t1 == t2 <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># unpack a tuple</span></span><br><span class="line">a,b,*c,d = range(<span class="number">10</span>)</span><br><span class="line">a <span class="comment"># 0</span></span><br><span class="line">b <span class="comment"># 1</span></span><br><span class="line">c <span class="comment"># [2,3,4,5,6,7,8]</span></span><br><span class="line">d <span class="comment"># 9</span></span><br></pre></td></tr></table></figure>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p><strong>Generators</strong> are a type of iterable, like lists or tuples. Unlike lists, they <strong>don’t allow indexing with arbitrary indices</strong>, but they can still be iterated through with <strong>for</strong> loops.<br>They can be created using functions and the <strong>yield</strong> statement.</p>
<p>Using <strong>generators</strong> results in improved performance, which is the result of the lazy (on demand) generation of values, which translates to lower memory usage. Furthermore, we do not need to wait until all the elements have been generated before we start to use them.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spell</span><span class="params">()</span>:</span></span><br><span class="line">  word = <span class="string">''</span></span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> <span class="string">'spam'</span>:</span><br><span class="line">    word += c</span><br><span class="line">    <span class="keyword">yield</span> word</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> spell():</span><br><span class="line">  print(w)</span><br><span class="line"></span><br><span class="line">print(list(spell()))    <span class="comment"># to normal list ['s', 'sp', 'spa', 'spam']</span></span><br></pre></td></tr></table></figure>
<h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>Sets can be combined using mathematical operations (list, tuple, dict do not support these operators)<br>The <strong>union</strong> operator <strong>|</strong> combines two sets to form a new one containing items in either.<br>The <strong>intersection</strong> operator <strong>&amp;</strong> gets items only in both.<br>The <strong>difference</strong> operator <strong>-</strong> gets items in the first set but not in the second.<br>The <strong>symmetric difference</strong> operator <strong>^</strong> gets items in either set, but not both.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;</span><br><span class="line">s2 = &#123;<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>&#125;</span><br><span class="line"></span><br><span class="line">print(s1 | s2) <span class="comment"># union: &#123;1,2,3,4,5,6,7&#125;</span></span><br><span class="line">print(s1 &amp; s2) <span class="comment"># intersection: &#123;3,4,5&#125;</span></span><br><span class="line">print(s1 - s2) <span class="comment"># in s1 but not s2: &#123;1,2&#125;</span></span><br><span class="line">print(s2 - s1) <span class="comment"># in s2 but not s1: &#123;6,7&#125;</span></span><br><span class="line">print(s1 ^ s2) <span class="comment"># not in s1 or not in s2: &#123;1,2,6,7&#125;</span></span><br></pre></td></tr></table></figure>
<h1 id="String-format"><a href="#String-format" class="headerlink" title="String format"></a>String format</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">####### hello world hello</span></span><br><span class="line"><span class="string">'&#123;0&#125; &#123;1&#125; &#123;0&#125;'</span>.format(<span class="string">'hello'</span>, <span class="string">'world'</span>)</span><br><span class="line"><span class="string">'&#123;x&#125; &#123;y&#125; &#123;x&#125;'</span>.format(x=<span class="string">'hello'</span>, y=<span class="string">'world'</span>)    <span class="comment"># use name</span></span><br><span class="line"></span><br><span class="line"><span class="string">r'fdjks/fjdkls'</span> <span class="comment"># r means raw, so you don't need to escape any character</span></span><br></pre></td></tr></table></figure>
<h1 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h1><h2 id="Function-Arguments"><a href="#Function-Arguments" class="headerlink" title="Function Arguments"></a>Function Arguments</h2><p>Using <strong><code>*args</code></strong> as a function parameter enables you to pass an arbitrary number of arguments to that function. The arguments are then accessible as the <strong>tuple</strong> args in the body of the function.</p>
<p><strong><em>\</em>kwargs</strong> (standing for keyword arguments) allows you to handle named arguments that you have not defined in advance. The keyword arguments return a dictionary in which the keys are the argument names, and the values are the argument values.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">some_func</span><span class="params">(a, some_default=<span class="string">'this is a default value'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">  print(a)</span><br><span class="line">  print(some_default)</span><br><span class="line">  print(args)</span><br><span class="line">  print(kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">some_func(<span class="string">'this is a'</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">this is a</span></span><br><span class="line"><span class="string">this is a default value</span></span><br><span class="line"><span class="string">()</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">some_func(<span class="string">'this is a'</span>, <span class="string">'value rather than default'</span>, <span class="string">'first arg'</span>, <span class="string">'second arg'</span>, first_kwarg=<span class="string">'1'</span>, second_kwarg=<span class="string">'2'</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">this is a</span></span><br><span class="line"><span class="string">value rather than default</span></span><br><span class="line"><span class="string">('first arg', 'second arg')</span></span><br><span class="line"><span class="string">&#123;'first_kwarg': '1', 'second_kwarg': '2'&#125;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h2><p>A lambda defines an anoymous function. It consists of the <strong>lambda</strong> keyword followed by a list of arguments, a colon, and the <strong>expression</strong> to evaluate and return.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## use named function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cube</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x ** <span class="number">3</span>    </span><br><span class="line">print(cube(<span class="number">3</span>))    <span class="comment"># 27</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## use lambda</span></span><br><span class="line">cube = <span class="keyword">lambda</span> x: x ** <span class="number">3</span></span><br><span class="line">print(cube(<span class="number">3</span>))    <span class="comment"># 27</span></span><br></pre></td></tr></table></figure>
<h3 id="Common-used-functors"><a href="#Common-used-functors" class="headerlink" title="Common used functors:"></a>Common used functors:</h3><p>map, filter</p>
<p>module <strong>itertools</strong> is a standard library that contains several functions that are useful in FP.</p>
<p>One type of function it produces is infinite iterators.<br>The function <strong>count</strong> counts up infinitely from a value.<br>The function <strong>cycle</strong> infinitely iterates through an iterable (for instance a list or string).<br>The function <strong>repeat</strong> repeats an object, either infinitely or a specific number of times.<br><strong>takewhile -</strong> takes items from an iterable while a predicate function remains true;<br><strong>chain -</strong> combines several iterables into one long one;<br><strong>accumulate -</strong> returns a running total of values in an iterable.<br><strong>product</strong> - get possible combinations of some iterables<br><strong>permutation</strong> - get permutation of an iterable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> accumulate, takewhile, product, permutations</span><br><span class="line"></span><br><span class="line">nums = list(accumulate(range(<span class="number">5</span>)))    <span class="comment"># [0,1,3,6,10]</span></span><br><span class="line">list(takewhile(<span class="keyword">lambda</span> x: x &lt;<span class="number">6</span>, nums)) <span class="comment"># [0,1,3]</span></span><br><span class="line"></span><br><span class="line">letters = (<span class="string">'a'</span>, <span class="string">'b'</span>)</span><br><span class="line">list(product(letters, range(<span class="number">2</span>)))    <span class="comment"># [(a,0), (a,1), (b,0), (b,1)] The result is always iterable of tuple</span></span><br><span class="line">list(permutations(letters))    <span class="comment"># [('a', 'b'), ('b', 'a')]</span></span><br></pre></td></tr></table></figure>
<h2 id="Decorator"><a href="#Decorator" class="headerlink" title="Decorator"></a>Decorator</h2><p>Decorators provide a way to modify functions using other functions.</p>
<p>Python provides support to wrap a function in a decorator by pre-pending the function definition with a decorator name and the @ symbol.</p>
<blockquote>
<p>You can use a decorator if you want to modify more than one function in the same way. It’s the common template of multiple functions.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="comment">## define two decorators</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_print_deca</span><span class="params">(func)</span>:</span></span><br><span class="line">  <span class="comment">## this annotation make the callback return the original func's name instead of the wrap function's name.</span></span><br><span class="line"><span class="meta">  @wraps</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">()</span>:</span></span><br><span class="line">    x = int(input(<span class="string">'x: '</span>))</span><br><span class="line">    y = int(input(<span class="string">'y: '</span>))</span><br><span class="line">    res = func(x, y)</span><br><span class="line">    print(<span class="string">'the res: &#123;0&#125;'</span>.format(res))</span><br><span class="line">  <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"><span class="comment">## use a decorator, and replace the origin method with the decorated one</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x,y)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x+y</span><br><span class="line">add = input_print_deca(add)    <span class="comment"># if the add method is a function from third library, you may want to add function to it, this way will help. However, you don't need to replace the original method, and you can just assign it to a new one.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## use @ grammar to quick add common code to a function. This has the same effect as the above equation.</span></span><br><span class="line"><span class="meta">@input_print_deca</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff</span><span class="params">(x, y)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x - y</span><br><span class="line"><span class="meta">@input_print_deca</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span><span class="params">(x, y)</span>:</span></span><br><span class="line">  <span class="keyword">assert</span> y != <span class="number">0</span>, <span class="string">'cannot divide zero'</span></span><br><span class="line">    <span class="keyword">return</span> x / y</span><br><span class="line"></span><br><span class="line"><span class="comment">## call the decorated functions, notice now there's no input</span></span><br><span class="line">add()</span><br><span class="line">diff()</span><br><span class="line">divide()</span><br></pre></td></tr></table></figure>
<p>A single function can have multiple decorators. They are used from top to down. Everytime the original func is called, the current decorator is used.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## define anther two decorators</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">success_deca</span><span class="params">(func)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    print(<span class="string">'in success deca'</span>)</span><br><span class="line">    res = func(x,y)</span><br><span class="line">    print(<span class="string">'yay success!'</span> + <span class="string">'*'</span> * <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line">  <span class="keyword">return</span> wrap</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">success_deca2</span><span class="params">(func)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">wrap</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    print(<span class="string">'in success deca2'</span>)</span><br><span class="line">    res = func(x,y)</span><br><span class="line">    print(<span class="string">'yay success 2'</span> + <span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line">  <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"><span class="comment">## use all these decorators</span></span><br><span class="line"><span class="meta">@input_print_deca</span></span><br><span class="line"><span class="meta">@success_deca</span></span><br><span class="line"><span class="meta">@success_deca</span></span><br><span class="line"><span class="meta">@success_deca2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x - y</span><br><span class="line"></span><br><span class="line"><span class="comment">## call the decorated func</span></span><br><span class="line">diff()</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">x: 5        # the call to diff() trigger the input_print_deca decorator</span></span><br><span class="line"><span class="string">y: 6</span></span><br><span class="line"><span class="string">in success deca            # in input_print_deca, the call to func trigger the first success_deca decorator</span></span><br><span class="line"><span class="string">in success deca            # in success_deca, the call to func trigger the second success_deca decorator</span></span><br><span class="line"><span class="string">in success deca2        # in the second success_deca, the call to func trigger the success_deca2 decorator</span></span><br><span class="line"><span class="string">yay success 2----------    # the success_deca2 first return</span></span><br><span class="line"><span class="string">yay success!**********    # the second success_deca return</span></span><br><span class="line"><span class="string">yay success!**********    # the first success_deca return</span></span><br><span class="line"><span class="string">the res: -1                            # the input_print_deca return</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="Generator-1"><a href="#Generator-1" class="headerlink" title="Generator"></a>Generator</h2><p>see <strong>Iterable Containers / Generator</strong> above.</p>
<h2 id="async-await"><a href="#async-await" class="headerlink" title="async/await"></a>async/await</h2><p>some links: </p>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=E-1Y4kSsAFc" target="_blank" rel="noopener">Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - youtube</a></p>
</li>
<li><p><a href="https://github.com/dabeaz/curio" target="_blank" rel="noopener">GitHub - dabeaz/curio: Good Curio!</a> a library to separate the asynchronus world and synchronus world by the author of the above video.</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/38575715" target="_blank" rel="noopener">Python Asyncio与多线程/多进程那些事</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/27258289" target="_blank" rel="noopener">Python Async/Await入门指南</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25354747" target="_blank" rel="noopener">从0到1，Python网络编程的入门之路</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25228075" target="_blank" rel="noopener">从0到1，Python异步编程的演进之路</a></p>
<p><code>async</code> create a coroutine which should be excuted when called. Use <code>await</code> in an async function to hung up the coroutine itself until the awaited coroutine finished. <code>await</code> can only be used in <code>async</code> functions. The  object after <code>await</code> must be an <code>Awaitable</code> (as long as you implement <code>__await__()</code>, it’s an <code>Awaitable</code>.  <code>Coroutine</code> extends from <code>Awaitable</code>)</p>
<p><code>async</code> can be used anywhere except for:</p>
</li>
</ul>
<ul>
<li><p>lambda</p>
</li>
<li><p>list comprehension (available since python 3.6)</p>
</li>
<li><p>default methods (e.g. <code>__init__()</code>), but we can use metaclass to make <code>__init__</code> awaitable.</p>
</li>
</ul>
<h3 id="eventloop"><a href="#eventloop" class="headerlink" title="eventloop"></a>eventloop</h3><p>The essence of aysncio is an eventloop. All awaitables are added to the eventloop and  wait for executing sequentially. If an event is interrupted by IO or sth, another event will be executed. </p>
<p>For every thread, it has its own eventloop. Events in different eventloops cannot communicate.</p>
<p>So basically, events in asyncio are executed sequentially, whereas their requests to IO may be paralled.</p>
<p>And the <code>await</code> keyword means to emit the Awaitable event at once and the program will wait until it finishes. Thus if we want to emit two or more Awaitable events at almost the same time, we use <code>asyncio.create_task(awaitable)</code>. </p>
<blockquote>
<p>(function) create_task: (coro: Generator[Any, None, _T@create_task] | Coroutine[Any, Any, _T@create_task], *, name: str | None = …) -&gt; Task[_T@create_task]</p>
<hr>
<p>Schedule the execution of a coroutine object in a spawn task. </p>
<p>Return a Task object.</p>
</blockquote>
<p>When using <code>create_task</code>, the Awaitable is told to be emited as soon as possible. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time, time_ns</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ns_to_time</span><span class="params">(nanoseconds)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> timedelta(microseconds=(nanoseconds/<span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">now</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">return</span> ns_to_time(time_ns())</span><br><span class="line"></span><br><span class="line">first_times = []</span><br><span class="line">second_times = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">print_first</span><span class="params">(n, if_print = True)</span>:</span></span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;now()&#125;</span>: first print start.'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        anow = time_ns()</span><br><span class="line">        first_times.append(anow)</span><br><span class="line">        <span class="keyword">if</span> if_print:</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;ns_to_time(anow)&#125;</span>: first-<span class="subst">&#123;i&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.1</span>)</span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;now()&#125;</span>: first print end.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">print_second</span><span class="params">(n, if_print = True)</span>:</span></span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;now()&#125;</span>: second print start.'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        anow = time_ns()</span><br><span class="line">        second_times.append(anow)</span><br><span class="line">        <span class="keyword">if</span> if_print:</span><br><span class="line">            print(<span class="string">f'<span class="subst">&#123;ns_to_time(anow)&#125;</span>: second-<span class="subst">&#123;i&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.1</span>)</span><br><span class="line">    print(<span class="string">f'<span class="subst">&#123;now()&#125;</span>: second print end.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># task1 &amp; task2 are emitted immediately</span></span><br><span class="line">    task1 = asyncio.create_task(print_first(<span class="number">5</span>))</span><br><span class="line">    task2 = asyncio.create_task(print_second(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># wait for the two tasks to finish, or the program will end while the tasks are still running.</span></span><br><span class="line">    <span class="keyword">await</span> task1</span><br><span class="line">    <span class="keyword">await</span> task2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># no intersection and the print result shows that all the events are executed sequentially</span></span><br><span class="line">    print(first_times)</span><br><span class="line">    print(second_times)</span><br><span class="line">    print(<span class="string">f'the intersected times: <span class="subst">&#123;set(first_times) &amp; set(second_times)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure>
<h3 id="async-amp-decorator"><a href="#async-amp-decorator" class="headerlink" title="async &amp; decorator"></a>async &amp; decorator</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个 anotation, 可以根据 function 是否在 async 环境运行</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_coro</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> bool(sys._getframe(n).f_code.co_flags &amp; <span class="number">0x80</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(coro)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        coro.send(<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">except</span> StopIteration <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> e.value</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">awaitable</span><span class="params">(syncfunc)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorator</span><span class="params">(asyncfunc)</span>:</span></span><br><span class="line"><span class="meta">        @wraps(asyncfunc)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> from_coro(<span class="number">2</span>):</span><br><span class="line">                print(<span class="string">'from coro...'</span>)</span><br><span class="line">                <span class="keyword">return</span> asyncfunc(*args, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'not from coro...'</span>)</span><br><span class="line">                <span class="keyword">return</span> syncfunc(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spam</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'the blue one'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@awaitable(spam)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">spam</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'the red one'</span>)</span><br><span class="line"></span><br><span class="line">spam()</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">await</span> spam()</span><br></pre></td></tr></table></figure>
<h3 id="Awaitable-amp-Coroutine-class"><a href="#Awaitable-amp-Coroutine-class" class="headerlink" title="Awaitable &amp; Coroutine class"></a>Awaitable &amp; Coroutine class</h3><p>The object after <code>await</code> must be an <code>Awaitable</code> (as long as you implement <code>__await__()</code>, it’s an <code>Awaitable</code>. <code>Coroutine</code> extends from <code>Awaitable</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The abstract `Awaitable` and `Coroutine` class:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Awaitable</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    __slots__ = ()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__await__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__subclasshook__</span><span class="params">(cls, C)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> Awaitable:</span><br><span class="line">            <span class="keyword">return</span> _check_methods(C, <span class="string">"__await__"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Coroutine</span><span class="params">(Awaitable)</span>:</span></span><br><span class="line">    __slots__ = ()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">send</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">throw</span><span class="params">(self, typ, val=None, tb=None)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__subclasshook__</span><span class="params">(cls, C)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> Coroutine:</span><br><span class="line">            <span class="keyword">return</span> _check_methods(C, <span class="string">'__await__'</span>, <span class="string">'send'</span>, <span class="string">'throw'</span>, <span class="string">'close'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">NotImplemented</span></span><br></pre></td></tr></table></figure>
<h2 id="Some-convinent-functions"><a href="#Some-convinent-functions" class="headerlink" title="Some convinent functions"></a>Some convinent functions</h2><ul>
<li><code>help</code>: eg. <code>help(Exception)</code> or <code>help(e)</code>, to get the methods &amp; data in a class/object</li>
<li><code>dir</code> : eg. <code>dir(Exception)</code> or <code>dir(e)</code>, is a simple version of <code>help</code>, shows all members as a string</li>
<li><code>inspect.getmro(type(e))</code>: get the class hierachy of a type</li>
</ul>
<h1 id="Major-3rd-Party-Libraries"><a href="#Major-3rd-Party-Libraries" class="headerlink" title="Major 3rd-Party Libraries"></a>Major 3rd-Party Libraries</h1><p><strong>Django</strong>: The most frequently used web framework written in Python, Django powers websites that include Instagram and Disqus. It has many useful features, and whatever features it lacks are covered by extension packages.<br><strong>CherryPy</strong> and <strong>Flask</strong> are also popular web frameworks.<br><strong>BeautifulSoup</strong> is very useful when scraping data from websites, and leads to better results than building your own scraper with regular expressions.</p>
<p>A number of third-party modules are available that make it much easier to carry out scientific and mathematical computing with Python.<br>The module <strong>matplotlib</strong> allows you to create graphs based on data in Python.<br>The module <strong>NumPy</strong> allows for the use of multidimensional arrays that are much faster than the native Python solution of nested lists. It also contains functions to perform mathematical operations such as matrix transformations on the arrays.<br>The library <strong>SciPy</strong> contains numerous extensions to the functionality of <strong>NumPy</strong>.</p>
<p>Python can also be used for <strong>game development</strong>.<br>Usually, it is used as a scripting language for games written in other languages, but it can be used to make games by itself.<br>For 3D games, the library <strong>Panda3D</strong> can be used. For 2D games, you can use <strong>pygame</strong>.</p>
<h1 id="Issues"><a href="#Issues" class="headerlink" title="# Issues"></a># Issues</h1><h2 id="Make-class-method-return-self-type"><a href="#Make-class-method-return-self-type" class="headerlink" title="Make class method return self type"></a>Make class method return self type</h2><p><a href="https://stackoverflow.com/questions/33533148/how-do-i-type-hint-a-method-with-the-type-of-the-enclosing-class" target="_blank" rel="noopener">https://stackoverflow.com/questions/33533148/how-do-i-type-hint-a-method-with-the-type-of-the-enclosing-class</a></p>
<p>If you are using Python 3.10 or later, it just works. As of today (2019) in 3.7+ you must turn this feature on using a future statement (<code>from __future__ import annotations</code>) - for Python 3.6 or below use a string.</p>
<h3 id="Python-3-7-from-future-import-annotations"><a href="#Python-3-7-from-future-import-annotations" class="headerlink" title="Python 3.7+: from __future__ import annotations"></a>Python 3.7+: <code>from __future__ import annotations</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Position</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other: Position)</span> -&gt; Position:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h3 id="Python-lt-3-7-use-a-string"><a href="#Python-lt-3-7-use-a-string" class="headerlink" title="Python &lt;3.7: use a string"></a>Python &lt;3.7: use a string</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Position</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other: <span class="string">'Position'</span>)</span> -&gt; 'Position':</span></span><br><span class="line">       ...</span><br></pre></td></tr></table></figure>
<h2 id="Circular-import"><a href="#Circular-import" class="headerlink" title="Circular import"></a>Circular import</h2><p><a href="https://zhuanlan.zhihu.com/p/66228942" target="_blank" rel="noopener">circular import</a></p>
<p>modify the position of import to fix the issues</p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>sqoop</title>
    <url>/2020/11/10/sqoop/</url>
    <content><![CDATA[<h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p>Sqoop: sq are the first two of “sql”, oop are the last three of “hadoop”. It <strong>transfers bulk data between hdfs and relational database servers</strong>. It supports:</p>
<ul>
<li>Full Load</li>
<li>Incremental Load</li>
<li>Parallel Import/Export (throught mapper jobs)</li>
<li>Compression</li>
<li>Kerberos Security Integration</li>
<li>Data  loading directly to HIVE</li>
</ul>
<blockquote>
<p>Sqoop cannot import .csv files into hdfs/hive. It only support databases / mainframe datasets import.</p>
</blockquote>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>Sqoop provides CLI, thus you can use a simple command to conduct import/export.</p>
<p>The import/export are executes in fact through map tasks.</p>
<p><img src="/images/sqoop-20201110134912159.png" alt="sqoop-20201110134912159.png"></p>
<p>When Import from DB:</p>
<ul>
<li>it split to some map tasks. And each map task will connect to DB, and fetch some rows/tables, and write it to a file into HDFS</li>
</ul>
<p>When export to DB:</p>
<ul>
<li>it also split to some map tasks. And each map task will fetch a HDFS file, and write each record in the file as a row by specified delimiter to some table.</li>
</ul>
<p><img src="/images/sqoop-20201110135220097.png" alt="sqoop-20201110135220097.png"></p>
<h1 id="Sqoop-v-s-Spark-jdbc"><a href="#Sqoop-v-s-Spark-jdbc" class="headerlink" title="Sqoop v.s. Spark jdbc"></a>Sqoop v.s. Spark jdbc</h1><p>sqoop uses mapreduce technique, while spark is a revolutionary engine to replace mapreduce technique with its in-memory execution and DAG smartness. Thus Spark jdbc is way faster than sqoop.</p>
<ol>
<li>You can combine all the read, transform and write operations into one script/program instead of reading it separately through SQOOP in one script and then doing transformation and write in another.</li>
<li>You can define a new split column on the fly (using functions like ORA_HASH) if you want the data to be partitioned in a proper way.</li>
<li>You can control the number of connection to the database. Increasing the number of connection will surely speed up your data import.</li>
</ol>
<h1 id="Common-Commands"><a href="#Common-Commands" class="headerlink" title="Common Commands"></a>Common Commands</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$  sqoop import \</span><br><span class="line">	--connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt;</span><br><span class="line">	--table &lt;my_table name&gt;</span><br><span class="line">	--username &lt;username_for_my_sql&gt; --password &lt;password&gt;</span><br><span class="line">  --target-dir &lt;target dir <span class="keyword">in</span> HDFS <span class="built_in">where</span> data needs to be imported&gt;</span><br><span class="line">  </span><br><span class="line">$  sqoop <span class="built_in">export</span> \</span><br><span class="line">	--connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt;</span><br><span class="line">	--table &lt;my_table name&gt;</span><br><span class="line">	--username &lt;username_for_my_sql&gt; --password &lt;password&gt;</span><br><span class="line">  --<span class="built_in">export</span>-dir &lt;target dir <span class="keyword">in</span> HDFS <span class="built_in">where</span> data needs to be exported&gt;</span><br></pre></td></tr></table></figure>
<h1 id="Incremental-Import"><a href="#Incremental-Import" class="headerlink" title="Incremental Import"></a>Incremental Import</h1><p>增量导入时，sqoop 需要识别到增量数据，有三种方法：</p>
<ol>
<li>根据自增字段识别新数据（append 模式）：可以直接识别新数据并导入到 hive 中</li>
<li>根据修改时间识别新数据（lastmodified 模式）：要求这个字段会随数据的改变而改变，但是似乎只能导入到 hdfs 中，不能直接导入到 hive 中。导入时，可以通过制定<code>--merge-key id</code> 来按 id 字段进行合并，或者之后通过 <code>sqoop merge</code> 功能单独运行</li>
<li>根据 where 或 query 识别新数据：可能之后只能通过 <code>sqoop merge</code> 来 merge 数据</li>
</ol>
<blockquote>
<p>目前 <strong>sqoop 导入时不能识别删除数据</strong>，都需要通过其他方式来解决（对比数据，或者数据上有 delete 标识符时，通过 <a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html" target="_blank" rel="noopener">Incrementally Updating a Table</a> 来实现）</p>
</blockquote>
<h2 id="append-模式"><a href="#append-模式" class="headerlink" title="append 模式"></a>append 模式</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table oms_order \</span><br><span class="line">--target-dir  /tmp/query  \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by id \</span><br><span class="line">--null-string <span class="string">'\\N'</span> \</span><br><span class="line">--null-non-string <span class="string">'\\N'</span> \</span><br><span class="line">--incremental append \	<span class="comment"># append 模式</span></span><br><span class="line">--check-column id \			<span class="comment"># 自增字段</span></span><br><span class="line">--last-value 22 \				<span class="comment"># 自增字段的 last value</span></span><br><span class="line">-m 2</span><br></pre></td></tr></table></figure>
<h2 id="lastmodified-模式"><a href="#lastmodified-模式" class="headerlink" title="lastmodified 模式"></a>lastmodified 模式</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/dicts \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table doit_stu \</span><br><span class="line">--target-dir  /doit_stu/2020-02-09  \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">--split-by id \</span><br><span class="line">--null-string <span class="string">'\\N'</span> \</span><br><span class="line">--null-non-string <span class="string">'\\N'</span> \</span><br><span class="line">--incremental lastmodified \		<span class="comment"># lastmodified 模式</span></span><br><span class="line">--check-column update_time \		<span class="comment"># 时间字段</span></span><br><span class="line">--last-value <span class="string">'2020-02-09 23:59:59'</span> \	<span class="comment"># 上一次获取的数据时间</span></span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">--merge-key id \					<span class="comment">#按照id字段进行合并</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>
<h2 id="条件查询"><a href="#条件查询" class="headerlink" title="条件查询"></a>条件查询</h2><p>这里写的都是全量导入 hive。如果要增量，只能先导入到 hdfs，然后再做 merge</p>
<h3 id="–where"><a href="#–where" class="headerlink" title="–where"></a>–where</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table oms_order \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by id \</span><br><span class="line">-m 2   \</span><br><span class="line">--null-string <span class="string">'\\N'</span> \</span><br><span class="line">--null-non-string <span class="string">'\\N'</span> \</span><br><span class="line">--<span class="built_in">where</span> <span class="string">"delivery_company is null"</span> \	<span class="comment"># filter condition</span></span><br><span class="line">--hive-overwrite</span><br></pre></td></tr></table></figure>
<h3 id="–query"><a href="#–query" class="headerlink" title="–query"></a>–query</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir  /tmp/query  \		<span class="comment"># sqoop 导入数据到 hive，本质就是先将数据导入到 hdfs，然后再去 hive 数据库创建元数据。这里需要手动指定中间临时目标目录（不太清楚为啥）</span></span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">','</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by id \</span><br><span class="line">--null-string <span class="string">'\\N'</span> \</span><br><span class="line">--null-non-string <span class="string">'\\N'</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--query <span class="string">"select id,member_id,order_sn,total_amount,pay_amount,status from oms_order where status=4 and \$CONDITIONS"</span>  \			<span class="comment"># 查询语句，也支持复杂查询</span></span><br><span class="line">-m 2</span><br></pre></td></tr></table></figure>
<h1 id="运行-sqoop-action"><a href="#运行-sqoop-action" class="headerlink" title="运行 sqoop action"></a>运行 sqoop action</h1><p>在数据接入时，特别是连接数据库增量导入数据时，这种周期性任务的执行，有很多种方式：</p>
<ol>
<li>写一个 long running 脚本，不断执行增量 import</li>
<li><a href="https://www.cnblogs.com/xing901022/p/6091306.html" target="_blank" rel="noopener">采用 Oozie 等调度工具来运行</a></li>
</ol>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>test principles</title>
    <url>/2018/10/12/test-principles/</url>
    <content><![CDATA[<p><a href="https://www.petrikainulainen.net/programming/unit-testing/3-reasons-why-we-should-not-use-inheritance-in-our-tests/" target="_blank" rel="noopener">three reasons why we should not use inheritance in tests</a></p>
<p>大概意思是：</p>
<ol>
<li>很多测试里的继承用的不合适。测试也是代码，必须符合继承的原则。</li>
</ol>
<blockquote>
<p>The point of inheritance is to <strong>take advantage of polymorphic behavior NOT to reuse code</strong>, and people miss that, they see inheritance as a cheap way to add behavior to a class. When I design code I like to think about options. When I inherit, I reduce my options. I am now sub-class of that class and cannot be a sub-class of something else. I have permanently fixed my construction to that of the superclass, and I am at a mercy of the super-class changing APIs. My freedom to change is fixed at compile time.</p>
</blockquote>
<p>多态的定义：</p>
<blockquote>
<p>Polymorphism is the provision of a single interface to entities of different types</p>
</blockquote>
<ol start="2">
<li>可能会影响性能。</li>
</ol>
<p>并不是所有的测试可能都会用到测试基类里的东西，而 beforeClass、beforeTest 在执行所有测试时都会被1. 查找，2. 执行，这些都导致了 CPU 的浪费。</p>
<p>此外，其他人也提到，比如 springboottest 中，基类中配置 <code>@SpringBootTest</code> ，最终可能导致在每个测试中配置 <code>@DirtyContext</code>等，严重影响性能。</p>
<ol start="3">
<li>很影响 reading</li>
</ol>
<p>测试即文档，而很多东西却是需要读多个类</p>
<p>这个问题仁者见仁，智者见智。duplicate code 也是问题。两者相权，如何处理？但对于目前的代码，如果没有问题，还是保持现状比较好。对于符合继承规则这点，倒是需要注意的。</p>
<p>作者在 <a href="https://www.petrikainulainen.net/programming/testing/writing-clean-tests-it-starts-from-the-configuration/" target="_blank" rel="noopener">writing clean test</a> 中提出了些解决方法，但感觉还是有些片面。当然他写了一个系列的文章：<a href="https://www.petrikainulainen.net/writing-clean-tests/" target="_blank" rel="noopener">writing clean test series</a>。</p>
<p>另外读书：<a href="https://www.amazon.cn/dp/1935182579/ref=sr_1_2?ie=UTF8&amp;qid=1539322654&amp;sr=8-2&amp;keywords=Effective+Unit+Testing%3A+A+guide+for+Java+developers" target="_blank" rel="noopener">effective unit testing</a></p>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>spark</title>
    <url>/2019/01/10/spark/</url>
    <content><![CDATA[<h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://spark.apache.org/docs/latest/" target="_blank" rel="noopener">spark</a> is a fast and general-purpose cluster computing system like Hadoop Map-reduce.  It runs on the clusters.</p>
<h2 id="Spark-Ecosystem"><a href="#Spark-Ecosystem" class="headerlink" title="Spark Ecosystem"></a>Spark Ecosystem</h2><p><strong><a href="http://data-flair.training/blogs/apache-spark-ecosystem-components/" target="_blank" rel="noopener">The components of Apache Spark Ecosystem</a></strong></p>
<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/07/apache-spark-ecosystem-components.jpg" alt="ecosystem"></p>
<ul>
<li>spark core: <strong>cluster computing system</strong>. Provide API to  write computing functions.</li>
<li><a href="https://spark.apache.org/docs/2.4.0/sql-programming-guide.html" target="_blank" rel="noopener">Spark SQL</a>. SQL for data processing, like hive?</li>
<li><a href="https://spark.apache.org/docs/2.4.0/ml-guide.html" target="_blank" rel="noopener">MLlib</a> for machine learning.</li>
<li><a href="https://spark.apache.org/docs/2.4.0/graphx-programming-guide.html" target="_blank" rel="noopener">GraphX</a> for graph processing</li>
<li><a href="https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming</a>.</li>
</ul>
<h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p>Spark Core is the fundamental unit of the whole Spark project. Its key features are:</p>
<ul>
<li>It is in charge of essential I/O functionalities.</li>
<li>Provide API to defines and manipulate the RDDs. Significant in programming and observing the role of the <strong><a href="http://data-flair.training/blogs/install-apache-spark-multi-node-cluster/" target="_blank" rel="noopener">Spark cluster</a></strong>.</li>
<li>Task dispatching, scheduling</li>
<li>Fault recovery.</li>
<li>It overcomes the snag of<strong><a href="http://data-flair.training/blogs/hadoop-mapreduce-introduction-tutorial-comprehensive-guide/" target="_blank" rel="noopener"> MapReduce</a></strong> by using in-memory computation.</li>
</ul>
<p>Spark makes use of Special data structure known as <strong><a href="http://data-flair.training/blogs/rdd-in-apache-spark/" target="_blank" rel="noopener">RDD (Resilient Distributed Dataset)</a></strong>. Spark Core is distributed execution engine with all the functionality attached on its top. For example, MLlib, <strong><a href="http://data-flair.training/blogs/spark-sql-tutorial/" target="_blank" rel="noopener">SparkSQL</a></strong>, GraphX, <strong><a href="http://data-flair.training/blogs/apache-spark-streaming-comprehensive-guide/" target="_blank" rel="noopener">Spark Streaming</a></strong>. Thus, allows diverse workload on single platform. All the basic functionality of Apache Spark Like <strong><a href="http://data-flair.training/blogs/apache-spark-in-memory-computing/" target="_blank" rel="noopener">in-memory computation</a>, <a href="http://data-flair.training/blogs/apache-spark-streaming-fault-tolerance/" target="_blank" rel="noopener">fault tolerance</a></strong>, memory management, monitoring, task scheduling is provided by Spark Core.<br>Apart from this Spark also provides the basic connectivity with the data sources. For example, <strong><a href="http://data-flair.training/blogs/category/hbase/" target="_blank" rel="noopener">HBase</a></strong>, Amazon S3, <strong><a href="http://data-flair.training/blogs/comprehensive-hdfs-guide-introduction-architecture-data-read-write-tutorial/" target="_blank" rel="noopener">HDFS </a></strong>etc.</p>
<h4 id="Action-Job-Stage-Task"><a href="#Action-Job-Stage-Task" class="headerlink" title="Action, Job, Stage, Task"></a>Action, Job, Stage, Task<a name="action-job-stage-task" /></h4><p><strong>Actions</strong> are RDD’s operation. <em>reduce, collect, takeSample, take, first, saveAsTextfile, saveAsSequenceFile, countByKey, foreach</em> are common actions in Apache spark.</p>
<p>In a Spark application, when you invoke an action on RDD, a <strong>job</strong> is created. Jobs are the main function that has to be done and is submitted to Spark. The jobs are divided into <strong>stages</strong> depending on how they can be separately carried out (mainly on shuffle boundaries). Then, these stages are divided into <strong>tasks</strong>. Tasks are the smallest unit of work that has to be done the executor.</p>
<p>When you call <code>collect()</code> on an RDD or Dataset, the whole data is sent to the <strong>Driver</strong>. This is why you should be careful when calling <code>collect()</code>.</p>
<p> <strong>An example:</strong></p>
<p><a href="https://stackoverflow.com/questions/28973112/what-is-spark-job" target="_blank" rel="noopener">What is Spark Job ?</a></p>
<blockquote>
<p>let’s say you need to do the following:</p>
<ol>
<li>Load a file with people names and addresses into RDD1</li>
<li>Load a file with people names and phones into RDD2</li>
<li>Join RDD1 and RDD2 by name, to get RDD3</li>
<li>Map on RDD3 to get a nice HTML presentation card for each person as RDD4</li>
<li>Save RDD4 to file.</li>
<li>Map RDD1 to extract zipcodes from the addresses to get RDD5</li>
<li>Aggregate on RDD5 to get a count of how many people live on each zipcode as RDD6</li>
<li>Collect RDD6 and prints these stats to the stdout.</li>
</ol>
<p>So,</p>
<ol>
<li>The <strong><em>driver program\</em></strong> is this entire piece of code, running all 8 steps.</li>
<li>Producing the entire HTML card set on step 5 is a <strong><em>job\</em></strong> (clear because we are using the <em>save</em> action, not a transformation). Same with the <em>collect</em> on step 8</li>
<li>Other steps will be organized into <strong><em>stages\</em></strong>, with each job being the result of a sequence of stages. For simple things a job can have a single stage, but the need to repartition data (for instance, the join on step 3) or anything that breaks the locality of the data usually causes more stages to appear. You can think of stages as computations that produce intermediate results, which can in fact be persisted. For instance, we can persist RDD1 since we’ll be using it more than once, avoiding recomputation.</li>
<li>All 3 above basically talk about how the <em>logic</em> of a given algorithm will be broken. In contrast, a <strong><em>task\</em></strong> is a particular <em>piece of data</em> that will go through a given stage, on a given executor.</li>
</ol>
</blockquote>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD 数据模型</p>
<table>
<thead>
<tr>
<th style="text-align:center">属性名</th>
<th style="text-align:center">成员类型</th>
<th style="text-align:center">属性含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">dependencies</td>
<td style="text-align:center">变量</td>
<td style="text-align:center">生成该RDD所依赖的父RDD</td>
</tr>
<tr>
<td style="text-align:center">compute</td>
<td style="text-align:center">方法</td>
<td style="text-align:center">生成该RDD的计算接口</td>
</tr>
<tr>
<td style="text-align:center">partitions</td>
<td style="text-align:center">变量</td>
<td style="text-align:center">该RDD的所有数据分片实体</td>
</tr>
<tr>
<td style="text-align:center">partitioner</td>
<td style="text-align:center">方法</td>
<td style="text-align:center">划分数据分片的规则</td>
</tr>
<tr>
<td style="text-align:center">preferredLocations</td>
<td style="text-align:center">变量</td>
<td style="text-align:center">数据分片的物理位置偏好</td>
</tr>
</tbody>
</table>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><p><a href="https://data-flair.training/blogs/learn-apache-spark-sparkcontext/" target="_blank" rel="noopener">SparkContext</a> is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. <strong>It allows your Spark Application to access Spark Cluster</strong> with the help of Resource Manager. </p>
<p>If you want to create SparkContext, first <strong>SparkConf</strong> should be made. The SparkConf has a configuration parameter that our Spark driver application will pass to SparkContext. Some of these parameter defines properties of Spark driver application. While some are used by Spark to allocate resources on the cluster, like the number, memory size, and cores used by executor running on the worker nodes.<br>In short, <strong>it guides how to access the Spark cluster</strong>. After the creation of a SparkContext object, we can invoke functions such as <strong>textFile, sequenceFile, parallelize</strong> etc.<br>Once the SparkContext is created, it can be used to <strong><a href="http://data-flair.training/blogs/how-to-create-rdds-in-apache-spark/" target="_blank" rel="noopener">create RDDs</a></strong>, broadcast variable, and accumulator, ingress Spark service and run jobs. All these things can be carried out until SparkContext is stopped.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">"Usage: wordcount &lt;file&gt;"</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the builder here defines the sparkConf, and then create a sparkSession with an underlying SparkContext `spark.sparkContext`</span></span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">"PythonWordCount"</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># here by `spark.read.text('some.txt')`, we use SparkContext create an DataFrame</span></span><br><span class="line">    lines = spark.read.text(sys.argv[<span class="number">1</span>]).rdd.map(<span class="keyword">lambda</span> r: r[<span class="number">0</span>])</span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>)) \</span><br><span class="line">                  .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">                  .reduceByKey(add)</span><br><span class="line">    <span class="comment"># this is the spark action `collect`</span></span><br><span class="line">    output = counts.collect()</span><br><span class="line">    <span class="keyword">for</span> (word, count) <span class="keyword">in</span> output:</span><br><span class="line">        print(<span class="string">"%s: %i"</span> % (word, count))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># this in fact stop the sparkContext</span></span><br><span class="line">    spark.stop()</span><br></pre></td></tr></table></figure>
<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/08/functions-of-sparkcontext-in-apache-spark.jpg" alt="10 Important Functions of SparkContext in Apache Spark"></p>
<h1 id="How-does-it-run"><a href="#How-does-it-run" class="headerlink" title="How does it run"></a>How does it run</h1><p>Spark core contains the main api, driver engine, scheduler… to support the cluster computing. The real computing is completed on the cluster. Spark can connect to many cluster managers(spark’s own standalone cluster manager, mesos, yarn) to complete the jobs. Typically, the process is like this:</p>
<ol>
<li>The user submits a spark application using the <code>spark-submit</code> command.</li>
<li>Spark-submit launches the driver program on the same node in (client mode) or on the cluster (cluster mode) and invokes the main method specified by the user.</li>
<li>The driver program contacts the cluster manager to ask for resources to launch executor JVMs based on the configuration parameters supplied.</li>
<li>The cluster manager launches executor JVMs on worker nodes.</li>
<li>The driver process scans through the user application. Based on the RDD actions and transformations in the program, Spark creates an operator graph.</li>
<li>When an action (such as collect) is called, the graph is submitted to a DAG scheduler. The DAG scheduler divides the operator graph into stages.</li>
<li>A stage comprises tasks based on partitions of the input data. The driver sends work to executors in the form of tasks.</li>
<li>The executors process the task and the result sends back to the driver through the cluster manager.</li>
</ol>
<p><img src="https://spark.apache.org/docs/latest/img/cluster-overview.png" alt="cluster mode overview"></p>
<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/08/Internals-of-job-execution-in-spark.jpg" alt="Complete Picture of Apache Spark Job Execution Flow."></p>
<h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><p>You can write spark function (eg. map function, reduce funciton) using Java/scala/python/R API. See <a href="https://spark.apache.org/docs/latest/" target="_blank" rel="noopener">api docs</a>.</p>
<h1 id="Installation-On-Yarn"><a href="#Installation-On-Yarn" class="headerlink" title="Installation On Yarn"></a>Installation On Yarn</h1><p>See <a href="https://spark.apache.org/docs/2.4.0/running-on-yarn.html" target="_blank" rel="noopener">run spark on Yarn</a>, <a href="https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/" target="_blank" rel="noopener">Install, Configure, and Run Spark on Top of a Hadoop YARN Cluster</a></p>
<ol>
<li><p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">downloads page</a> download the spark</p>
</li>
<li><p><code>tar -xvf spark-xxx.tgz</code></p>
</li>
<li><p>configuration</p>
</li>
</ol>
<ul>
<li>config in <code>/conf/spark-env.sh</code></li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># config this to specify the installed HADOOP path</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure>
<ul>
<li>config in <code>/conf/spark-default.conf</code>. (<a href="https://spark.apache.org/docs/2.4.0/configuration.html#spark-properties" target="_blank" rel="noopener">all configuration properties</a>)</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># config the spark master</span></span><br><span class="line">spark.master                     yarn</span><br><span class="line">spark.driver.memory    512m</span><br><span class="line">spark.yarn.am.memory    512m</span><br><span class="line">spark.executor.memory          512m</span><br></pre></td></tr></table></figure>
<h2 id="history-server-config"><a href="#history-server-config" class="headerlink" title="history server config"></a>history server config</h2><p>When the spark job is running, you can access the job log by <code>localhost:4040</code>. When the job is finished, by default, the log is not persisted which means you can’t access it. To access the logs later, need to config the following: (see <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">spark Monitoring and Instrumentation</a> and <a href="https://spark.apache.org/docs/2.4.0/running-on-yarn.html#using-the-spark-history-server-to-replace-the-spark-web-ui" target="_blank" rel="noopener">using history server to replace the spark web ui</a>)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in /conf/spark-default.conf</span></span><br><span class="line"><span class="comment"># config history server</span></span><br><span class="line">spark.ui.filters         org.apache.spark.deploy.yarn.YarnProxyRedirectFilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># tell spark use history server url as the trackint url</span></span><br><span class="line">spark.yarn.historyServer.allowTracking  <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># enable log persistence</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># log write dir. Here use the hdfs dir and you must create the dir in hdfs first</span></span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/spark-logs</span><br><span class="line"></span><br><span class="line"><span class="comment"># log read dir. Sometimes logs are transfered.</span></span><br><span class="line">spark.history.fs.logDirectory     hdfs://localhost:9000/spark-logs</span><br></pre></td></tr></table></figure>
<h2 id="example-execution"><a href="#example-execution" class="headerlink" title="example execution"></a>example execution</h2><ul>
<li>Start histroy server: <code>sbin/start-history-server.sh</code></li>
<li>execute spark job:</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>
<p>Then you can:</p>
<ul>
<li><p>Check the job/application info in yarn: <code>http://localhost:8088/cluster/apps</code></p>
</li>
<li><p>Check the job/application using Spark history server: <code>http://localhost:18080/</code></p>
</li>
</ul>
<h1 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h1><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html#glossary" target="_blank" rel="noopener">glossary</a></p>
<p>NoteThe following table summarizes terms you’ll see used to refer to cluster concepts:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Application</td>
<td style="text-align:left">User program built on Spark. Consists of a <em>driver program</em> and <em>executors</em> on the cluster.</td>
</tr>
<tr>
<td style="text-align:left">Application jar</td>
<td style="text-align:left">A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies<strong>. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</strong></td>
</tr>
<tr>
<td style="text-align:left">Driver program</td>
<td style="text-align:left">The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td style="text-align:left">Cluster manager</td>
<td style="text-align:left">An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</td>
</tr>
<tr>
<td style="text-align:left">Deploy mode</td>
<td style="text-align:left">Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</td>
</tr>
<tr>
<td style="text-align:left">Worker node</td>
<td style="text-align:left">Any node that can run application code in the cluster</td>
</tr>
<tr>
<td style="text-align:left">Executor</td>
<td style="text-align:left">A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</td>
</tr>
<tr>
<td style="text-align:left">Task</td>
<td style="text-align:left">A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td style="text-align:left">Job</td>
<td style="text-align:left">A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. <code>save</code>, <code>collect</code>); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td style="text-align:left">Stage</td>
<td style="text-align:left">Each job gets divided into smaller sets of tasks called <em>stages</em> that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody>
</table>
<h1 id="Deploy-mode"><a href="#Deploy-mode" class="headerlink" title="Deploy mode"></a>Deploy mode</h1><h2 id="yarn-client-vs-yarn-cluster"><a href="#yarn-client-vs-yarn-cluster" class="headerlink" title="yarn-client vs yarn-cluster"></a>yarn-client vs yarn-cluster</h2><p><a href="https://www.cnblogs.com/ittangtang/p/7967386.html" target="_blank" rel="noopener">yarn-client vs yarn-cluster 深度剖析</a></p>
<p><a href="https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use" target="_blank" rel="noopener">stackoverflow</a></p>
<h1 id="spark-shell-vs-spark-submit"><a href="#spark-shell-vs-spark-submit" class="headerlink" title="spark-shell vs spark-submit"></a>spark-shell vs spark-submit</h1><p>Spark shell is only intended to be use for testing and perhaps development of small applications - is only an interactive shell and should not be use to run production spark applications. For production application deployment you should use spark-submit. The last one will also allow you to run applications in yarn-cluster mode</p>
<h1 id="Spark-DataFrame"><a href="#Spark-DataFrame" class="headerlink" title="Spark DataFrame"></a>Spark DataFrame</h1><h2 id="auto-increment-id"><a href="#auto-increment-id" class="headerlink" title="auto increment id"></a>auto increment id</h2><p><a href="https://blog.csdn.net/k_wzzc/article/details/84996172" target="_blank" rel="noopener">two ways for auto increment id</a></p>
<h3 id="Row-number"><a href="#Row-number" class="headerlink" title="Row_number"></a>Row_number</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 设置窗口函数的分区以及排序，因为是全局排序而不是分组排序，所有分区依据为空</span></span><br><span class="line"><span class="comment">  * 排序规则没有特殊要求也可以随意填写</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> spec = <span class="type">Window</span>.partitionBy().orderBy($<span class="string">"lon"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df1 = dataframe.withColumn(<span class="string">"id"</span>, row_number().over(spec))</span><br><span class="line"></span><br><span class="line">df1.show()</span><br></pre></td></tr></table></figure>
<h3 id="rdd-zipWithIndex"><a href="#rdd-zipWithIndex" class="headerlink" title="rdd.zipWithIndex"></a>rdd.zipWithIndex</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在原Schema信息的基础上添加一列 “id”信息</span></span><br><span class="line"> <span class="keyword">val</span> schema: <span class="type">StructType</span> = dataframe.schema.add(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">LongType</span>))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// DataFrame转RDD 然后调用 zipWithIndex</span></span><br><span class="line"> <span class="keyword">val</span> dfRDD: <span class="type">RDD</span>[(<span class="type">Row</span>, <span class="type">Long</span>)] = dataframe.rdd.zipWithIndex()</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = dfRDD.map(tp =&gt; <span class="type">Row</span>.merge(tp._1, <span class="type">Row</span>(tp._2)))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 将添加了索引的RDD 转化为DataFrame</span></span><br><span class="line"> <span class="keyword">val</span> df2 = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"> df2.show()</span><br></pre></td></tr></table></figure>
<h2 id="Add-constant-column"><a href="#Add-constant-column" class="headerlink" title="Add constant column"></a>Add constant column</h2><p><a href="https://stackoverflow.com/questions/32788322/how-to-add-a-constant-column-in-a-spark-dataframe" target="_blank" rel="noopener">add constant column</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.typedLit</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">"some_array"</span>, typedLit(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">df.withColumn(<span class="string">"some_struct"</span>, typedLit((<span class="string">"foo"</span>, <span class="number">1</span>, <span class="number">0.3</span>)))</span><br><span class="line">df.withColumn(<span class="string">"some_map"</span>, typedLit(<span class="type">Map</span>(<span class="string">"key1"</span> -&gt; <span class="number">1</span>, <span class="string">"key2"</span> -&gt; <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">from pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df.withColumn(<span class="symbol">'new_colum</span>n', lit(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h2 id="select-latest-record"><a href="#select-latest-record" class="headerlink" title="select latest record"></a>select latest record</h2><p><a href="https://stackoverflow.com/questions/55615716/select-latest-record-from-spark-dataframe" target="_blank" rel="noopener">stackoverflow</a></p>
<h1 id="Hive-Hints"><a href="#Hive-Hints" class="headerlink" title="Hive Hints"></a>Hive Hints</h1><p><a href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#partitioning-hints" target="_blank" rel="noopener">hive hints</a></p>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><p><a href="https://www.youtube.com/watch?v=daXEp4HmS-E" target="_blank" rel="noopener">deep dive - spark optimization</a></p>
<p><a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html" target="_blank" rel="noopener">performance tuning</a></p>
<h2 id="Get-Baseline"><a href="#Get-Baseline" class="headerlink" title="Get Baseline"></a>Get Baseline</h2><ol>
<li>利用 spark-ui 观察任务运行情况（long stages，spill，laggard tasks, etc.）</li>
<li>利用 yarn 等观察资源利用情况（CPU 利用率 etc.）</li>
</ol>
<h2 id="Memory-spill"><a href="#Memory-spill" class="headerlink" title="Memory spill"></a>Memory spill<a name = "memory-spill" /></h2><p>Spark 运行时会分配一定的 memory（可以<a href="#specify-resources">指定资源需求</a>)， 分 storage 和 working memory。</p>
<ul>
<li>storage memory 是 persist 会用的 memory。当调用 persist（或 cache，一种使用 <code>StorageLevel.MemoryAndDisk</code> 的 persist）时，如果指定的 storage_level 有 memory，那么就会将数据存到 memory。</li>
<li>working memory 是 spark 运算所需要的 memory，这个大小是动态变化的。当 storage memory 占用过多内存时，working memory 就不够了。然后就会有 spill，就会慢。</li>
</ul>
<p>memory spill 表示 working memory 不够，spark 开始使用 disk。而 disk 的 I/O 效率是极低的。所以一旦出现 spill，性能就会大大降低。</p>
<p>working memory 不够有很多原因：</p>
<ol>
<li>Memory 资源申请的太少了，就是不够 ====》 增加 <code>spark.executor.memory</code><ol>
<li>数据在 memory/disk 的存储一般是 serialized，以节省空间。但数据 load 到 working memory 时，一般都是 deserialized 的，处理更快，但是更占空间。</li>
</ol>
</li>
<li>资源可以了，partition 太少，每个 partition 处理的数据太多，所以 spill 了 ====》 增加 <a href="shuffle-partition">shuffle partition</a></li>
<li>有不均衡出现，导致某些 task 处理的数据尤其多 ====》see <a href="#balance">balance</a></li>
<li>有太多 persist，持久化了太多东西，占用过多的 storage memory ====》see <a href="#persistence">persistence</a></li>
</ol>
<p><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-storage-hierachy.png" alt="image-20210319124829765"></p>
<h2 id="指定资源需求"><a href="#指定资源需求" class="headerlink" title="指定资源需求"></a>指定资源需求<a name="specify-resources" /></h2><p>Spark-submit 运行时，可通过指定以下参数来定义运行所需的资源：</p>
<ul>
<li><code>--conf spark.num.executors=xx</code> (或 <code>--num-executors xx</code>)：指定运行时需要几个 executor（也可以通过 <a href="#dynamic-allocation">dynamic allocation</a> 来根据运算动态分配 executors）</li>
<li><code>--conf spark.executor.memory=xxG</code>（或 <code>--executor-memory xxG</code>）：指定每个 executor 所需要的内存</li>
<li><code>--conf spark.executor.cores=xx</code>（或 <code>--executor-cores xx</code>）：指定每个 executor 所需要的 cores</li>
<li><code>--conf spark.driver.memory=xxG</code>（或 <code>--driver-memory xxG</code>）：指定每个 driver 所需要的内存。当执行 <code>df.collect()</code>时，会将数据 collect 到 driver，此时就需要 driver 有很多的 memory</li>
<li><code>--conf spark.driver.cores=xx</code>（或 <code>--driver-cores xx</code>）：指定每个 driver 所需要的 cores</li>
</ul>
<h2 id="Some-issues"><a href="#Some-issues" class="headerlink" title="Some issues"></a>Some issues</h2><h3 id="–executor-cores-settinng-not-working"><a href="#–executor-cores-settinng-not-working" class="headerlink" title="–executor-cores settinng not working"></a>–executor-cores settinng not working</h3><p>需要配置 <code>yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</code>，因为默认的使用的是 <a href="https://apache.googlesource.com/hadoop-common/+/e0c9f893b684246feb5b4adbb95a05a436cdb790/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/DefaultResourceCalculator.java" target="_blank" rel="noopener">DefaultResourceCalculator</a>，它只看 memory(–executor-memory)，DominantResourceCalculator 则同时考虑 cpu 和 memory</p>
<p>see <a href="https://stackoverflow.com/questions/33248108/spark-executor-on-yarn-client-does-not-take-executor-core-count-configuration" target="_blank" rel="noopener">stackoverflow</a></p>
<h3 id="–spark-dynamicAllocation-maxExecutors-not-working"><a href="#–spark-dynamicAllocation-maxExecutors-not-working" class="headerlink" title="–spark.dynamicAllocation.maxExecutors not working"></a>–spark.dynamicAllocation.maxExecutors not working<a name="dynamic-allocation" /></h3><p>这个需要和其他配置配合使用</p>
<blockquote>
<p>spark.dynamicAllocation.enabled = true<br>This requires <code>spark.shuffle.service.enabled</code> or <code>spark.dynamicAllocation.shuffleTracking.enabled</code> to be set. The following configurations are also relevant: <code>spark.dynamicAllocation.minExecutors</code>, <code>spark.dynamicAllocation.maxExecutors</code>, and <code>spark.dynamicAllocation.initialExecutors</code> <code>spark.dynamicAllocation.executorAllocationRatio</code></p>
</blockquote>
<p>如果还不工作，可能要按 [spark dynamic allocation not working](https://community.cloudera.com/t5/Support-Questions/Spark-dynamic-allocation-dont-work/td-p/140227 设置各 nodemanager 并重启</p>
<p>See <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation" target="_blank" rel="noopener">spark dynamic allocation</a></p>
<h2 id="Partitions"><a href="#Partitions" class="headerlink" title="Partitions"></a>Partitions</h2><p>接下来从输入、运行、输出三个阶段的 partition 优化来看</p>
<p>一般 1 partition -&gt; 1 task，分多少个 partition，就拆多少个 task 来运行。</p>
<ol>
<li><strong>Avoid the spills</strong></li>
<li><strong>Maximize parallelism</strong><ol>
<li>utilize all cores</li>
<li>provision only the cores you need</li>
</ol>
</li>
</ol>
<h3 id="输入（input）"><a href="#输入（input）" class="headerlink" title="输入（input）"></a>输入（input）</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.default.parallelism (don&#39;t use)</span><br><span class="line">spark.sql.files.maxPartitionBytes (mutable，控制每个 partition 读的文件大小)</span><br></pre></td></tr></table></figure>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle<a name="shuffle-partition" /></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.sql.shuffle.partitions（控制使用多少个 partition 来 shuffle）</span><br><span class="line">spark.default.parallelism（控制 rdd 的 partition 数目？？？？？？）</span><br></pre></td></tr></table></figure>
<p>如果配置了 <code>spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#39;true&#39;)</code> 或 <code>spark.sql.adaptive.coalescePartitions.enabled</code> ，它会动态控制 parition count （参见 <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions" target="_blank" rel="noopener">coalescing post shuffle partitions</a>），根据 shuffle 数据大小来动态设置 partition 数目。但是这个设置可能不合理，因为 shuffle 过程中，最终操作的数据可能远大于 shuffle read 的大小，这个过程中存在 deserialize 等。如果配置了动态控制，依然出现了 shuffle spill，那么可以先关掉这个配置，手动控制 shuffle partitions 大小。</p>
<h3 id="输出（output）"><a href="#输出（output）" class="headerlink" title="输出（output）"></a>输出（output）</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coalesce</span><br><span class="line">repartition</span><br><span class="line">repartition(range) &#x3D;&#x3D;&#x3D;&gt; range partitioner???</span><br><span class="line">df.localCheckPoint().repartition().... &#x3D;&#x3D;&gt; how to use tis</span><br></pre></td></tr></table></figure>
<h2 id="Balance"><a href="#Balance" class="headerlink" title="Balance"></a>Balance<a name="balance" /></h2><p>When some partitions are significantly larger than most, there is skew.</p>
<p>Balance 体现在很多方面：网络、GC、数据，当然最常见的问题是数据的不均匀。</p>
<p>通过查看 spark ui 可以看到不均匀的任务（这个时候需要停掉重跑）：</p>
<ol>
<li>查看 staggling tasks<ol>
<li>查看 stage 执行进度：stage 里剩余几个 task 执行特别慢，这个时候各个 task 处理的数据肯定存在不均匀，导致那几个 task 处理的尤其慢<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-straggling-task1.png" alt="image-20210317130715804"></li>
</ol>
</li>
<li>查看 stage 执行 metric：大部分时候没有 spill，但是 max 的时候有 spill；或者大部分的时候 read size 和 max read size 有很大差别<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-straggling-task2.png" alt="image-20210317130618356"></li>
</ol>
</li>
</ol>
</li>
<li>查看 stage 里各个节点的 GC time，GC time 分布不均匀，也是有问题的（什么问题？？）<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-gc-skew.png" alt="image-20210317130839506"></li>
</ol>
</li>
<li></li>
</ol>
<h2 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence<a name="persistence" /></h2><p>当 execution plan 中，有些 superset 被多个 subset 所使用，superset 计算复杂、耗时久，这个时候就可以选择将 superset persist，从而避免重复运算。</p>
<blockquote>
<p><a href="#action-job-stage-task">spark core</a> 中有几个概念，其中只有 action 会触发一次 dag 的运行。同一段代码，可能会生成不同的 dag，每次都需要执行。所以如果被多次使用的 superset，最好将它 cache，避免后续的重复运算。</p>
</blockquote>
<p>persist/cache 要慎用，因为：</p>
<ol>
<li>占资源。当 persist 消耗了太多的 storage memory 时，就会出现 <a href="#memory-spill">memory spill</a></li>
<li>也有时间损耗（serialize, deserialize, I/O)。persist 一般都以 serialized 的形式存储，节省空间，而 load 到 working memory 时，又需要 deserialiize</li>
</ol>
<blockquote>
<p>In Python, stored objects will always be serialized with the <a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> library, so it does not matter whether you choose a serialized level. The available storage levels in Python include <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>DISK_ONLY</code>, <code>DISK_ONLY_2</code>, and <code>DISK_ONLY_3</code>.*</p>
</blockquote>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>Cache 是选择 default 的 persist。persist 可以选择不同的 <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener">persistence storage level</a> </p>
<p>With <code>cache()</code>, you use only the default storage level :</p>
<ul>
<li><code>MEMORY_ONLY</code> for <strong>RDD</strong></li>
<li><code>MEMORY_AND_DISK</code> for <strong>Dataset</strong></li>
</ul>
<p>With <code>persist()</code>, you can specify which storage level you want for both <strong>RDD</strong> and <strong>Dataset</strong>.</p>
<p>From the official docs:</p>
<blockquote>
<ul>
<li>You can mark an <code>RDD</code> to be persisted using the <code>persist</code>() or <code>cache</code>() methods on it.</li>
<li>each persisted <code>RDD</code> can be stored using a different <code>storage level</code></li>
<li>The <code>cache</code>() method is a shorthand for using the default storage level, which is <code>StorageLevel.MEMORY_ONLY</code> (store deserialized objects in memory).</li>
</ul>
</blockquote>
<p>Use <code>persist()</code> if you want to assign a storage level other than :</p>
<ul>
<li><code>MEMORY_ONLY</code> to the <strong>RDD</strong></li>
<li>or <code>MEMORY_AND_DISK</code> for <strong>Dataset</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.catalog.cacheTable(<span class="string">"tableName"</span>)</span><br><span class="line">spark.catalog.uncacheTable(<span class="string">"tableName"</span>)</span><br><span class="line"></span><br><span class="line">dataFrame.cache()</span><br></pre></td></tr></table></figure>
<h2 id="broadcast-join"><a href="#broadcast-join" class="headerlink" title="broadcast join"></a>broadcast join</h2><p><a href="https://zhuanlan.zhihu.com/p/58765338" target="_blank" rel="noopener">spark 执行 map-join 优化</a></p>
<p><a href="https://www.jianshu.com/p/2c7689294a73" target="_blank" rel="noopener">spark broadcast join</a></p>
<p>几种方式：</p>
<h3 id="1-spark-自动识别小表-broadcast"><a href="#1-spark-自动识别小表-broadcast" class="headerlink" title="1. spark 自动识别小表 broadcast"></a>1. spark 自动识别小表 broadcast</h3><p><code>spark.sql.statistics.fallBackToHdfs=True</code>, 这样它会直接分析文件的大小，而不是 metastore 数据</p>
<h3 id="2-使用-hint"><a href="#2-使用-hint" class="headerlink" title="2. 使用 hint"></a>2. 使用 hint</h3><p><a href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#partitioning-hints" target="_blank" rel="noopener">hive hints</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ BROADCAST (b) */</span> * <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b)</span><br></pre></td></tr></table></figure>
<h3 id="3-使用-dataframe-api"><a href="#3-使用-dataframe-api" class="headerlink" title="3. 使用 dataframe api"></a>3. 使用 dataframe api</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> broadcast</span><br><span class="line">broadcast(spark.table(<span class="string">"b"</span>)).join(spark.table(<span class="string">"a"</span>), <span class="string">"id"</span>).show()</span><br></pre></td></tr></table></figure>
<h2 id="cache-vs-broadcast"><a href="#cache-vs-broadcast" class="headerlink" title="cache vs broadcast"></a>cache vs broadcast</h2><p><a href="https://stackoverflow.com/questions/38056774/spark-cache-vs-broadcast" target="_blank" rel="noopener">cache vs broadcast</a></p>
<blockquote>
<p>RDDs are divided into <em>partitions</em>. These partitions themselves act as an immutable subset of the entire RDD. When Spark executes each stage of the graph, each partition gets sent to a worker which operates on the subset of the data. In turn, each worker can <em>cache</em> the data if the RDD needs to be re-iterated.</p>
<p>Broadcast variables are used to send some immutable state <em>once</em> to each worker. You use them when you want a local copy of a variable.</p>
<p>These two operations are quite different from each other, and each one represents a solution to a different problem.</p>
</blockquote>
<h2 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h2><p><a href="https://blog.csdn.net/lhxsir/article/details/87882128" target="_blank" rel="noopener">spark-sql 优化小文件过多</a></p>
<p><a href="https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908" target="_blank" rel="noopener">On Spark, Hive, and Small Files: An In-Depth Look at Spark Partitioning Strategies</a></p>
<h4 id="为什么会有小文件？"><a href="#为什么会有小文件？" class="headerlink" title="为什么会有小文件？"></a>为什么会有小文件？</h4><p>当 spark 要 write 到 hive 表时，这实际也是一个 shuffle stage，就会分很多个 sPartition (spark partition)。每个 sPartition 在处理时，都会生成一个文件（如果是动态分区，则更严重，因为每个 sPartition 的数据分布式均匀的，每个 sPartition 可能包含很多个 hive paritition key，spark 每遇到一个 partition key 就生成一个文件），那么 sPartition 数目越多（动态分区的情况下，会更不可控），文件数就会越多。</p>
<p>简单来说，就是 spark 的一个 stage 分成了很多个 task（shuffle partitions 控制这个数量），即 sPartition，每个 sPartition 可能对应多个 hPartitiion（hive partition）key，多个 sPartition 也对应一个 hPartition key。而每个 sPartition 里对应的每个 hPartition key，都会生成一个文件。</p>
<p>那么，如果一个 sPartition 和 hPartition 只是一个 <strong>多（可控数目，对应最后每个 hPartitiion 的文件数）对一</strong> 的情况，那么文件数就是可控的。</p>
<blockquote>
<p>使用 hive 时，不会有小文件问题。hive 里只需要设置下边的这些参数，就</p>
<p>In pure Hive pipelines, there are configurations provided to automatically collect results into reasonably sized files, nearly transparently from the perspective of the developer, such as <em>hive.merge.smallfiles.avgsize</em>, or <em>hive.merge.size.per.task</em>.</p>
</blockquote>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ol>
<li>coalesce</li>
<li>repartition</li>
<li>Distribute by</li>
<li>adaptive execution</li>
</ol>
<p><a href="http://www.jasongj.com/spark/adaptive_execution/" target="_blank" rel="noopener">Adaptive Execution 让 Spark SQL 更高效更智能</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启用 Adaptive Execution ，从而启用自动设置 Shuffle Reducer 特性</span><br><span class="line">spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#39;true&#39;)</span><br><span class="line"># 设置每个 Reducer 读取的目标数据量，单位为字节。默认64M，一般改成集群块大小</span><br><span class="line">spark.conf.set(&quot;spark.sql.adaptive.shuffle.targetPostShuffleInputSize&quot;, &#39;134217728&#39;)</span><br></pre></td></tr></table></figure>
<h2 id="python-udf-vs-scala-udf"><a href="#python-udf-vs-scala-udf" class="headerlink" title="python udf vs scala udf"></a>python udf vs scala udf</h2><p><a href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62" target="_blank" rel="noopener">python udf vs scala udf</a></p>
<p><img src="https://miro.medium.com/max/1570/1*ddtDqMvoDGhxsw0CDEnfag.png" alt="img"></p>
<p><img src="https://miro.medium.com/max/1415/1*SMlxTZJBsAPKmpdRH5VFVw.png" alt="img"></p>
<p><img src="https://miro.medium.com/max/1500/1*FFi8Yk6mwSc6AvI-avWcYw.png" alt="img"></p>
<h1 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h1><h2 id="Null-aware-predicate-sub-queries-cannot-be-used-in-nested-conditions"><a href="#Null-aware-predicate-sub-queries-cannot-be-used-in-nested-conditions" class="headerlink" title="Null-aware predicate sub-queries cannot be used in nested conditions"></a>Null-aware predicate sub-queries cannot be used in nested conditions</h2><p><code>not in</code> 不能和 <code>or</code> 之类的 condition 一块用。现在好像还没有修复，参见：<a href="https://github.com/apache/spark/pull/22141" target="_blank" rel="noopener">SPARK-25154</a></p>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed computing</tag>
      </tags>
  </entry>
  <entry>
    <title>yarn</title>
    <url>/2019/01/09/yarn/</url>
    <content><![CDATA[<h1 id="yarn-architecture"><a href="#yarn-architecture" class="headerlink" title="yarn architecture"></a><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">yarn architecture</a></h1><p>Yarn is used to <strong>manage/allocate cluster resource</strong> &amp; <strong>schedule/moniter jobs</strong>. These parts – resource manager – are split up from hadoop framework.</p>
<p><img src="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="yarn architecture"></p>
<p>Yarn has two main components:</p>
<ul>
<li><strong>Schedular</strong>: manage resources (cpu, memory, network, disk, etc.) and allocate it the applications.<ul>
<li>node manager will tell Schedular the node resource info (node status)</li>
<li>application master will ask Schedular for resources.</li>
<li>When partitioning resources among various queues, applications, Schedular supports pluggable policies. For example:<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a> allocate resources by tenant request. It’s used especially for <strong>multi-tenant scenario</strong>, designed to allow sharing a large cluster while giving each <strong>organization</strong> capacity guarantees. Each client/tenant can request any resources that are not used by others. And there’s strict ACLs to ensure the <strong>security</strong> of resources between tenants. The primary abstraction is queue. Different tenant use different queue to utilize the resources. And <strong>hierachical queues</strong> are provided to support data separation in one tenant.</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler</a> assigning resources to appliations such that all apps get, <strong>on average</strong>, <strong>an equal shares of resources over time</strong>. It’s mainly designed to share cluster between <strong>a number of users</strong>. It lets short apps are completed in a reasonable time while not starving long-lived apps. (resources might free up when new apps are submitted).</li>
</ul>
</li>
</ul>
</li>
<li><strong>ApplicationManager</strong>: accept job-submisons, negotiate to exeuct application masters, and moniter reboot app master when failure.<ul>
<li>AppMaster are the one who <ul>
<li>apply to Schedular for resources</li>
<li>boot up job execution</li>
<li>moniter the job execution status</li>
<li>tell app manager if the job fails or succeeds.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Other components:</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html" target="_blank" rel="noopener">ReservationSystem</a>: reserve some resources to ensure the predictable execution of important jobs.</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/Federation.html" target="_blank" rel="noopener">YARN Federation</a>: join clusters to scale and allow multiple independent clusters.</li>
</ul>
<h2 id="an-example"><a href="#an-example" class="headerlink" title="an example"></a>an example</h2><p>Take hive as example:</p>
<h3 id="load-data-–-non-distributed-computing-jobs"><a href="#load-data-–-non-distributed-computing-jobs" class="headerlink" title="load data – non-distributed-computing jobs"></a>load data – non-distributed-computing jobs</h3><ol>
<li>user uses hive command to load data into hive table. (eg. <code>LOAD DATA LOCAL INPATH &#39;/path/to/datafile/&#39; OVERWRITE INTO TABLE table_name;</code>)</li>
<li>hive calls hdfs to write data.</li>
<li>node inform schedular the new node status.</li>
</ol>
<h3 id="query-data-–-distributed-computing-jobs"><a href="#query-data-–-distributed-computing-jobs" class="headerlink" title="query data – distributed computing jobs"></a>query data – distributed computing jobs</h3><ol>
<li>user uses hive command to query data (eg. <code>select count(*) from xxx</code>)</li>
<li>hive submits a map-reduce job to appliction manager</li>
<li>application manager applies to Schedular (??? not sure) for a container to execute application master and boots it.</li>
<li>application master applies to Schedular for resoures to excute map-reduce job and boots the job.</li>
<li>the map-reduce job get input data from hdfs, and write output data into hdfs</li>
<li>the map-reduce job informs the application master the status of the job.</li>
<li>application manager will restart application master on failure (application failure/hardware failure). (when application failed, the job informs the app master, and app manager knows it, and then reboot it)</li>
</ol>
<h1 id="JobHistoryServer"><a href="#JobHistoryServer" class="headerlink" title="JobHistoryServer"></a>JobHistoryServer</h1><p>On YARN all applications page, here’s a link to job history. However, you must config to make it take effect.</p>
<p>Follow the instructions <a href="https://blog.csdn.net/xiaoduan_/article/details/79689882" target="_blank" rel="noopener">config of johhistory in hadoop</a>. Also, see <a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="noopener">Hadoop Cluster Setup</a> to get info about starting log and jobhistory server. See <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html" target="_blank" rel="noopener">History Server Rest API</a>, <a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/apidocs/index.html?org/apache/hadoop/mapreduce/v2/hs/package-tree.html" target="_blank" rel="noopener">JobHistoryServer javadoc</a></p>
<blockquote>
<p>Notes:</p>
<p>The host of <code>mapreduce.jobhistory.webapp.address</code> and <code>mapreduce.jobhistory.address</code> may need to be set as the real ip (get from <code>ipconfig getifaddr en0</code>) or some other host (eg. cncherish.local) instead of <code>localhost</code>.</p>
<p>When start history server, you can see the start host in the log. It may look like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">STARTUP_MSG: Starting JobHistoryServer</span><br><span class="line">STARTUP_MSG:   host &#x3D; CNcherish.local&#x2F;192.168.xx.xxx</span><br><span class="line">STARTUP_MSG:   args &#x3D; []</span><br><span class="line">STARTUP_MSG:   version &#x3D; 3.1.1</span><br></pre></td></tr></table></figure>
<p>This might be because the JobHistoryServer told <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html#Configurations" target="_blank" rel="noopener">yarn web proxy</a> that its host is ‘cncherish.local/192.168.xx.xxx’ (mapping host ‘cncherish.local’ to the real ip ‘192.168.xx.xxx’), while yarn knows that history host for map-reduce job is ‘localhost’ from <code>mapred-site.xml</code> — the config for map-reduced jobs. The incompatible info reduce the jobhistory link is unreachable.</p>
</blockquote>
<p>1.add the following properties into the <code>mapred-site.xml</code> (config the map-reduce framework)</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- config to persist the jobhistory logs in hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.cleaner.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置jobhistoryserver 没有配置的话 history入口不可用 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.x.xxx:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置web端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.x.xxx:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置正在运行中的日志在hdfs上的存放路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done_intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置运行过的日志存放在hdfs上的存放路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>2.add the following properties into the <code>yarn-site.xml</code> (config the yarn — resource manager)</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3.start the historyserver</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The following command will run server as a background daemon</span></span><br><span class="line">$ mapred --daemon start historyserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># The following command will run server on the current terminal.</span></span><br><span class="line"><span class="comment"># In this way, you can know how the server is started, stopped and what it does.</span></span><br><span class="line"><span class="comment"># Also you can know the real server host from the log, which should be aligned by the mapred-site.xml</span></span><br><span class="line">$ mapred historyserver</span><br></pre></td></tr></table></figure>
<h1 id="rest-api"><a href="#rest-api" class="headerlink" title="rest api"></a>rest api</h1><ul>
<li>yarn rest api:  throught postman <a href="http://localhost:8088/ws/v1/cluster/apps" target="_blank" rel="noopener">http://localhost:8088/ws/v1/cluster/apps</a> (get all the apps)</li>
<li>history rest api: <a href="http://cnpzzheng.local:19888/ws/v1/history" target="_blank" rel="noopener">http://cnpzzheng.local:19888/ws/v1/history</a> (get server info)<ul>
<li>use 19888 instead of 10020</li>
</ul>
</li>
</ul>
<h1 id="CheatSheet"><a href="#CheatSheet" class="headerlink" title="CheatSheet"></a>CheatSheet</h1><p><a href="https://www.jianshu.com/p/f510a1f8e5f0" target="_blank" rel="noopener">link</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看正在运行的程序资源使用情况</span></span><br><span class="line">$ yarn top</span><br><span class="line">$ yarn node -all -list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定queue使用情况</span></span><br><span class="line">$ yarn queue -status root.users.xxx</span><br><span class="line">$ yarn application -movetoqueue application_1528080031923_0067 -queue root.users.xxx</span><br><span class="line"></span><br><span class="line">$ yarn application -list -appStates [ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING,FINISHED,FAILED,KILLED]</span><br><span class="line">$ yarn application -list -appTypes [SUBMITTED, ACCEPTED, RUNNING]</span><br><span class="line">$ yarn applicationattempt -list application_1528080031923_0064</span><br><span class="line"></span><br><span class="line">$ yarn application -<span class="built_in">kill</span> application_1528080031923_0067</span><br><span class="line"></span><br><span class="line">$ yarn logs -applicationId application_1528080031923_0064</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>hadoop</tag>
        <tag>big data</tag>
        <tag>distributed computing</tag>
        <tag>resource manager</tag>
      </tags>
  </entry>
  <entry>
    <title>c# contextual keywords: yield</title>
    <url>/2020/03/10/yield/</url>
    <content><![CDATA[<p><a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/yield" target="_blank" rel="noopener">yield</a> is a contextual keywords. When it shows in a statement, it means the method or get accessor in which it appears is an iterator. Thus it provides a simple way to define an iterator, rather than a class that implements <code>IEnumerable</code> or <code>IEnumerator</code>.</p>
<blockquote>
<p>When you use the <code>yield</code> <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/#contextual-keywords" target="_blank" rel="noopener">contextual keyword</a> in a statement, you indicate that the method, operator, or <code>get</code> accessor in which it appears is an iterator. Using <code>yield</code> to define an iterator removes the need for an explicit extra class (the class that holds the state for an enumeration, see <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ienumerator-1" target="_blank" rel="noopener">IEnumerator</a> for an example) when you implement the <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.ienumerable" target="_blank" rel="noopener">IEnumerable</a> and <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.ienumerator" target="_blank" rel="noopener">IEnumerator</a> pattern for a custom collection type.</p>
</blockquote>
<h2 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yield return expression;	&#x2F;&#x2F; return an element in the iterator</span><br><span class="line">yield break;	&#x2F;&#x2F; end the iterator</span><br></pre></td></tr></table></figure>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h2><h4 id="What’s-an-iterator"><a href="#What’s-an-iterator" class="headerlink" title="What’s an iterator?"></a>What’s an iterator?</h4><p>An iterator means that it can be looped in <code>foreach</code> or LINQ query. In this case, it means the method or get accessor containing <code>yield</code> can be consumed by <code>foreach</code> or LINQ query.</p>
<h4 id="what-does-yield-means-in-this-iterator"><a href="#what-does-yield-means-in-this-iterator" class="headerlink" title="what does yield means in this iterator?"></a>what does <code>yield</code> means in this iterator?</h4><p>The <code>yield return</code> will return an element in the iterator. During the loop, the iterator use <code>MoveNext</code> to get i (take <a href="#yield-in-method">power method</a> as an example), and the <code>MoveNext</code> stop at the next <code>yield return</code> expression, and the <code>Current</code> property of the iterator is updated as this value, too.</p>
<h4 id="when-does-the-iterator-stopped"><a href="#when-does-the-iterator-stopped" class="headerlink" title="when does the iterator stopped?"></a>when does the iterator stopped?</h4><ul>
<li>when there’s <code>yield break</code></li>
<li>when the method body is end</li>
</ul>
<h4 id="what’s-the-requirements-to-define-such-iterator"><a href="#what’s-the-requirements-to-define-such-iterator" class="headerlink" title="what’s the requirements to define such iterator?"></a>what’s the requirements to define such iterator?</h4><ul>
<li>The return type must be <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.ienumerable" target="_blank" rel="noopener">IEnumerable</a>, <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ienumerable-1" target="_blank" rel="noopener">IEnumerable</a>, <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.ienumerator" target="_blank" rel="noopener">IEnumerator</a>, or <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ienumerator-1" target="_blank" rel="noopener">IEnumerator</a>.</li>
<li>The declaration can’t have any <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/in-parameter-modifier" target="_blank" rel="noopener">in</a> <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/ref" target="_blank" rel="noopener">ref</a> or <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/out-parameter-modifier" target="_blank" rel="noopener">out</a> parameters.</li>
<li>Don’t use <code>yield</code> in <a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/statements-expressions-operators/lambda-expressions" target="_blank" rel="noopener">Lambda expressions</a> and <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/delegate-operator" target="_blank" rel="noopener">anonymous methods</a>.</li>
<li>Don’t use <code>yield</code> in methods that contain unsafe blocks. For more information, see <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/unsafe" target="_blank" rel="noopener">unsafe</a>.</li>
<li>Don’t use <code>yield return</code> in a try-catch block. A <code>yield return</code> statement can be located in the try block of a try-finally statement.</li>
<li><code>yield break</code> can be located in a try block or a catch block but not a finally block</li>
</ul>
<h1 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h1><h3 id="yield-in-method"><a href="#yield-in-method" class="headerlink" title="yield in method"></a>yield in method<a name = 'yield-in-method' /></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static void Main(string[] args)</span><br><span class="line">&#123;</span><br><span class="line">  	var powers &#x3D; Power(2, 10);	&#x2F;&#x2F; won&#39;t execute the body of Power</span><br><span class="line">    foreach (var i in powers)</span><br><span class="line">    &#123;</span><br><span class="line">        Console.WriteLine($&quot;&#123;i&#125; &quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; yield in method &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">public static IEnumerable&lt;int&gt; Power(int number, int exponent)</span><br><span class="line">&#123;</span><br><span class="line">    int result &#x3D; 1;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; exponent; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        result &#x3D; result * number;</span><br><span class="line">        yield return result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="yield-in-get-accessor"><a href="#yield-in-get-accessor" class="headerlink" title="yield in get accessor"></a>yield in get accessor<a name = 'yield-in-get-accessor' /></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static void Main(string[] args)</span><br><span class="line">&#123;</span><br><span class="line">    foreach (var i in new Galaxies().AllGalaxies)</span><br><span class="line">    &#123;</span><br><span class="line">        Console.WriteLine($&quot;&#123;i.Name&#125;, &#123;i.Age&#125;&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Galaxies</span><br><span class="line">&#123;</span><br><span class="line">  &#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; yield in get accessor &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    public IEnumerable&lt;Galaxy&gt; AllGalaxies</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            yield return new Galaxy(&quot;The milky way&quot;, 1000);</span><br><span class="line">            yield return new Galaxy(&quot;Tadpole&quot;, 1000);</span><br><span class="line">            yield return new Galaxy(&quot;Andromeda&quot;, 1000);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Galaxy</span><br><span class="line">&#123;</span><br><span class="line">    public string Name &#123; get; &#125;</span><br><span class="line">    public int Age &#123; get; &#125;</span><br><span class="line"></span><br><span class="line">    public Galaxy(string name, int age)</span><br><span class="line">    &#123;</span><br><span class="line">        Name &#x3D; name;</span><br><span class="line">        Age &#x3D; age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>c#</tag>
      </tags>
  </entry>
  <entry>
    <title>redux</title>
    <url>/2018/05/29/redux/</url>
    <content><![CDATA[<h1 id="what’s-redux"><a href="#what’s-redux" class="headerlink" title="what’s redux"></a>what’s redux</h1><p><a href="https://cn.redux.js.org/" target="_blank" rel="noopener">redux 中文文档</a> 中的几个关键特性：</p>
<ol>
<li><strong>状态容器</strong>，提供<strong>可预测化</strong>的状态管理</li>
<li>跨平台，客户端、服务端、原生应用都能用</li>
<li>易于测试</li>
<li>轻量，支持 react 等界面库</li>
</ol>
<p>其中第一点，讲明了 redux 的主要用途：<strong>状态容器</strong></p>
<p>以 react 为例，页面是渲染状态树得到，有静态状态（<code>props</code>）、动态状态（<code>state</code>）。通过在代码中 <code>setState</code> 来修改状态树，状态自上而下传递到各个子组件，最终触发组件树的重新渲染。</p>
<p>使用 redux，我们就将状态的定义和允许的迁移 function 挪出去，放到一个状态容器里，来有效管理组件的所有状态和状态迁移。此时 <code>component</code> 代码中就没有 <code>state</code>、<code>setState</code> 等定义了。</p>
<h2 id="counter-without-redux"><a href="#counter-without-redux" class="headerlink" title="counter without redux"></a>counter without redux</h2><p>以计数器为例，不使用 redux，即直接在本地定义 <code>state</code>，并通过 <code>setState</code> 修改状态，以实现重现渲染。(<a href="https://github.com/HFCherish/react-learning/blob/master/redux/counter/src/Counter.js" target="_blank" rel="noopener">源代码</a>)</p>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> React, &#123;Component&#125; <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">Counter</span> <span class="keyword">extends</span> <span class="title">Component</span> </span>&#123;</span><br><span class="line">  state = &#123;</span><br><span class="line">    value: <span class="number">0</span>,</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.state.value&#125;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.increment&#125;&gt;+&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.decrement&#125;&gt;-&lt;/</span>button&gt;</span><br><span class="line">      &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  decrement = () =&gt; &#123;</span></span><br><span class="line"><span class="regexp">    this.setState(&#123;</span></span><br><span class="line"><span class="regexp">      value: this.state.value - 1</span></span><br><span class="line"><span class="regexp">    &#125;);</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  increment = () =&gt; &#123;</span></span><br><span class="line"><span class="regexp">    this.setState(&#123;</span></span><br><span class="line"><span class="regexp">      value: this.state.value + 1</span></span><br><span class="line"><span class="regexp">    &#125;);</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="counter-with-redux"><a href="#counter-with-redux" class="headerlink" title="counter with redux"></a>counter with redux</h2><p>如果使用 redux，就是将原本的 <code>state</code> 存储到一个 <code>store</code> 中，通过 <code>dispatch</code> 一个 <code>action</code>(eg. <code>{type: &#39;INCREMENT&#39;}</code>)触发状态变化，<code>action</code> 如何改变 <code>state</code> 是由一个纯函数 <code>reducer</code> 定义的。（<a href="https://github.com/HFCherish/react-learning/blob/master/redux/counter/src/CounterWithRedux.js" target="_blank" rel="noopener">源代码</a>）</p>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="comment">// counter.js</span></span><br><span class="line"><span class="keyword">import</span> React, &#123;Component&#125; <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="class"><span class="keyword">class</span> <span class="title">Counter</span> <span class="keyword">extends</span> <span class="title">Component</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      &lt;div&gt;</span><br><span class="line">        &#123;<span class="keyword">this</span>.props.store.getState()&#125;</span><br><span class="line">        &lt;button onClick=&#123;<span class="keyword">this</span>.increment&#125;&gt;+&lt;<span class="regexp">/button&gt;</span></span><br><span class="line"><span class="regexp">        &lt;button onClick=&#123;this.decrement&#125;&gt;-&lt;/</span>button&gt;</span><br><span class="line">      &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    );</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  decrement = () =&gt; &#123;</span></span><br><span class="line"><span class="regexp">  /</span><span class="regexp">/ 改变内部 state 惟一方法是 dispatch 一个 action。</span></span><br><span class="line"><span class="regexp">    this.props.store.dispatch(&#123;type: 'DECREMENT'&#125;);</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">  increment = () =&gt; &#123;</span></span><br><span class="line"><span class="regexp">    this.props.store.dispatch(&#123;type: 'INCREMENT'&#125;);</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">/</span><span class="regexp">/ reducer.js</span></span><br><span class="line"><span class="regexp">/</span>**</span><br><span class="line"> * 这是一个 reducer，形式为 (state, action) =&gt; state 的纯函数。</span><br><span class="line"> * 描述了 action 如何把 state 转变成下一个 state。</span><br><span class="line"> *<span class="regexp">/</span></span><br><span class="line"><span class="regexp">export default function counter(state = 0, action) &#123;</span></span><br><span class="line"><span class="regexp">  switch (action.type) &#123;</span></span><br><span class="line"><span class="regexp">    case 'INCREMENT':</span></span><br><span class="line"><span class="regexp">      return state + 1;</span></span><br><span class="line"><span class="regexp">    case 'DECREMENT':</span></span><br><span class="line"><span class="regexp">      return state - 1;</span></span><br><span class="line"><span class="regexp">    default:</span></span><br><span class="line"><span class="regexp">      return state;</span></span><br><span class="line"><span class="regexp">  &#125;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">/</span><span class="regexp">/ index.js</span></span><br><span class="line"><span class="regexp">import React from 'react';</span></span><br><span class="line"><span class="regexp">import * as ReactDOM from "react-dom";</span></span><br><span class="line"><span class="regexp">import Counter from "./</span>Counter<span class="string">";</span></span><br><span class="line"><span class="string">import counter from "</span>./reducer<span class="string">";</span></span><br><span class="line"><span class="string">import &#123;createStore&#125; from 'redux';</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">const store = createStore(counter);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">const render = () =&gt; ReactDOM.render(</span></span><br><span class="line"><span class="string">  &lt;Counter store=&#123;store&#125;/&gt;,</span></span><br><span class="line"><span class="string">  document.getElementById('root')</span></span><br><span class="line"><span class="string">);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">render();</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// store 的订阅接口，注册 listener（这里即 render）来订阅 store 的状态更新。</span></span><br><span class="line"><span class="string">store.subscribe(render);</span></span><br></pre></td></tr></table></figure>
<h1 id="why-redux"><a href="#why-redux" class="headerlink" title="why redux"></a>why redux</h1><p>不使用 redux 时，</p>
<ol>
<li><strong>状态（model）、状态如何被改变（controller）、页面模板/html（view）是耦合在一块的。</strong><font color='gray'>（react 提供的就是一种界面库，render 函数其实写的就是页面模板，将 model 填充进去，渲染得到最后的 view）</font></li>
<li><strong>组件之间强耦合，难以确定状态的改变是在哪里被谁触发，bug 追踪难。</strong></li>
</ol>
<p>使用 redux 后，</p>
<ol>
<li><strong>model 被抽出来了</strong>（即当前视图所对应的 model 对象），而且这个 model 对象不是一个贫血模型，它提供基本的 action 来保证 model 的完整性。</li>
<li>由于状态的变更只能通过 <code>dispatch</code> 来触发，<strong>解耦了父子组件之间的状态变更传递</strong>，易于定位. （可以利用 <a href="http://cn.redux.js.org/docs/advanced/Middleware.html" target="_blank" rel="noopener">middleware</a> 记录 state 变更日志，即可实现 state 变化过程透明和可预测）</li>
</ol>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><p>redux 是一个状态容器，它解决了： </p>
<ul>
<li>状态在哪：<code>createStore()</code></li>
<li>状态是什么：<code>store.getState()</code></li>
<li>状态怎么变： reducer，即 <code>(preState, action) =&gt; nextState</code> 函数</li>
<li>触发状态变更：<code>store.dispatch(action)</code></li>
<li>谁关心状态变化：<code>store.subscribe(callback)</code></li>
</ul>
<h2 id="reducer"><a href="#reducer" class="headerlink" title="reducer"></a><a href="http://cn.redux.js.org/docs/basics/Reducers.html" target="_blank" rel="noopener">reducer</a></h2><p>Reducers 指定了应用状态的变化如何响应 actions 并发送到 store 的，记住 <strong>actions 只是描述了有事情发生了</strong>这一事实，并没有描述应用如何更新 state。reducer 函数形式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(preState, action) &#x3D;&gt; nextState</span><br></pre></td></tr></table></figure>
<p>因为 <code>state</code> 有很多属性，针对属性的处理也有很多，全放在一块就太大、难管理，所以可以拆 reducer（称为 slice reducer），再利用 <a href="http://cn.redux.js.org/docs/api/combineReducers.html" target="_blank" rel="noopener"><code>combineReducer</code></a> 组合。<font color='gray'>（redux 总是一个根，多个子：一个 store，多个子状态；一个根 reducer，多个 slice reducers）</font></p>
<p>使用 <code>combineReducer</code> 后有两个问题：</p>
<ol>
<li>store state 和 slice reducer state 的关系？</li>
<li>slice reducer 如何被调用执行？</li>
</ol>
<h3 id="store-state-和-slice-reducer-state-的关系？"><a href="#store-state-和-slice-reducer-state-的关系？" class="headerlink" title="store state 和 slice reducer state 的关系？"></a>store state 和 slice reducer state 的关系？</h3><p><a href="http://cn.redux.js.org/docs/api/combineReducers.html" target="_blank" rel="noopener">官方 API 描述</a></p>
<blockquote>
<p><strong><code>combineReducers()</code> 返回的 <code>state</code> 对象，会将传入的每个 reducer 返回的 <code>state</code> 按其传递给 <code>combineReducers()</code> 时对应的 key 进行命名，并将所有 slice reducer 返回的结果合并</strong> </p>
</blockquote>
<p>举例：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> reducer1  = <span class="function">(<span class="params">state=<span class="string">'initA'</span>, action</span>) =&gt;</span> &#123;...&#125;</span><br><span class="line"><span class="keyword">const</span> reducer2  = <span class="function">(<span class="params">state=<span class="string">'initB'</span>, action</span>) =&gt;</span> &#123;...&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里 `&#123;reducer1, reducer2&#125;` 使用的是 es2015 的 shorthand property names。</span></span><br><span class="line"><span class="comment">// 等价于 `&#123;reducer1: reducer1, reducer2: reducer2&#125;`</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(combineReducer(&#123;reducer1, reducer2&#125;));</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;reducer1: 'initA', reducer2: 'initB'&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以指定 key 名称</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(combineReducer(&#123;<span class="attr">a</span>:reducer1, <span class="attr">b</span>:reducer2&#125;));</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;a: 'initA', b: 'initB'&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="slice-reducer-如何被调用执行？"><a href="#slice-reducer-如何被调用执行？" class="headerlink" title="slice reducer 如何被调用执行？"></a>slice reducer 如何被调用执行？</h3><p><a href="http://cn.redux.js.org/docs/api/combineReducers.html" target="_blank" rel="noopener">官方 API 描述</a></p>
<blockquote>
<p>返回值：<br>(Function)：<strong>一个调用 reducers 对象里所有 reducer 的 reducer，并且构造一个与 reducers 对象结构相同的 state 对象。</strong></p>
</blockquote>
<p>即 combinedReducer 会调用执行所有的 slice reducer，以实现分层处理</p>
<h2 id="初始化-state"><a href="#初始化-state" class="headerlink" title="初始化 state"></a><a href="http://cn.redux.js.org/docs/recipes/reducers/InitializingState.html" target="_blank" rel="noopener">初始化 state</a></h2><p>有两种方式：</p>
<ol>
<li>使用 <code>createStore(reducers, [preloadedState], [enhancers])</code> 中的 <code>preloadedState</code></li>
<li>使用 reducer 中的默认属性（<code>function someReducer(state={defaultValue}, action){}</code>）</li>
</ol>
<p>这其中有两条规则：</p>
<ol>
<li><strong>preloadedState 先于 reducer 去填充 state</strong></li>
<li><strong>创建 store 后，redux 会 <code>dispatch</code> 一个虚拟的 <code>action</code> 到 reducer，以触发其中的默认值来填充 state。</strong></li>
</ol>
<h3 id="使用单一简单-reducer"><a href="#使用单一简单-reducer" class="headerlink" title="使用单一简单 reducer"></a>使用单一简单 reducer</h3><blockquote>
<p><em>使用 <code>preloadedState</code> 后，单一 reducer 的默认 state 赋值<strong>会</strong>失效，因为流程是这样的：</em></p>
</blockquote>
<ol>
<li>创建 store，并根据 <code>preloadedState</code> 填充 <code>state</code>；</li>
<li><code>dispatch</code> 虚拟 <code>action</code> 来使用 reducer 默认值填充 <code>state</code>。而此时 <code>state</code> 已经不是 <code>undefind</code>，因此，这个默认值填充是无效的</li>
</ol>
<p>举一个例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// reducer</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">counterReducer</span>(<span class="params">state=<span class="number">0</span>, action</span>) </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 仅使用 reducer 初始化</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(counterReducer);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 同时使用 preloadedState 和 reducer</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(counterReducer, <span class="number">42</span>);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//42</span></span><br></pre></td></tr></table></figure>
<h3 id="使用-combineReducer"><a href="#使用-combineReducer" class="headerlink" title="使用 combineReducer()"></a>使用 <code>combineReducer()</code></h3><blockquote>
<p><em>使用 <code>preloadedState</code> 后，使用了 <code>combineReducers</code> 的 reducer 默认 state 赋值<strong>可能会</strong>失效，因为流程是这样的：</em></p>
</blockquote>
<ol>
<li>创建 store，并根据 <code>preloadedState</code> 填充 <code>state</code>；</li>
<li><code>dispatch</code> 虚拟 <code>action</code> 来调用所有子 reducers，<ul>
<li>如果 <code>preloadedState</code> 中定义了当前 <code>reducer</code> 的属性，则对当前 reducer，<code>state</code> 已经不是 <code>undefind</code>，此时默认值填充是无效的</li>
<li>如果 <code>preloadedState</code> 中没有定义当前 <code>reducer</code> 的属性，则对当前 reducer，<code>state</code> 是 <code>undefind</code>，默认值填充有效</li>
</ul>
</li>
</ol>
<p>同样看一个例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// reducer</span></span><br><span class="line"><span class="keyword">const</span> reducer1  = <span class="function">(<span class="params">state=<span class="string">'initA'</span>, action</span>) =&gt;</span> &#123;...&#125;</span><br><span class="line"><span class="keyword">const</span> reducer2  = <span class="function">(<span class="params">state=<span class="string">'initB'</span>, action</span>) =&gt;</span> &#123;...&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> combinedReducer = combineReducers(&#123;<span class="attr">a</span>: reducer1, <span class="attr">b</span>: reducer2&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 仅使用 reducer 初始化</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(combinedReducer);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;a: 'initA', b: 'initB'&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 同时使用 preloadedState 和 reducer</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(counterReducer, &#123;<span class="attr">a</span>: <span class="string">'initInPreload'</span>&#125;);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;a: 'initInPreload', b: 'initB'&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><em>使用 <code>combineReducers</code> 时，<code>preloadedState</code> 必须包含 slice reducer 中的属性，与传入的 keys 保持同样的结构</em> (参见 <a href="http://cn.redux.js.org/docs/api/createStore.html" target="_blank" rel="noopener">createStore API 说明</a>)</p>
</blockquote>
<p>举例：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> reducer1 = <span class="function">(<span class="params">state=<span class="string">'initA'</span>, action</span>) =&gt;</span> &#123;...&#125;</span><br><span class="line"><span class="keyword">const</span> combinedReducer = combineReducers(&#123;<span class="attr">a</span>: reducer1, <span class="attr">b</span>: reducer2&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确用法</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(combinedReducer, &#123;<span class="attr">b</span>: <span class="string">'bala'</span>&#125;);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;a: 'initA', b: 'bala'&#125;;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误用法. 此时不会包含 c 属性。</span></span><br><span class="line"><span class="comment">// 因为所有的 state 都必须映射到 reducer 上，必须保持同样的结构，映射不到的就会被忽略</span></span><br><span class="line"><span class="keyword">const</span> store = createStore(combinedReducer, &#123;<span class="attr">c</span>: <span class="string">'bala'</span>&#125;);</span><br><span class="line"><span class="built_in">console</span>.log(store.getState()); <span class="comment">//&#123;a: 'initA', b: 'initB'&#125;;</span></span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="http://redux.js.org/docs/basics/UsageWithReact.html" target="_blank" rel="noopener">usage with react</a></li>
<li><a href="http://redux.js.org/docs/basics/UsageWithReact.html" target="_blank" rel="noopener">显示和容器组件</a></li>
<li><a href="https://www.zhihu.com/question/41312576" target="_blank" rel="noopener">为什么用redux？原理？</a></li>
<li><a href="https://github.com/jasonslyvia/a-cartoon-intro-to-redux-cn" target="_blank" rel="noopener">看漫画学redux</a></li>
</ul>
]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title>react setState 的坑</title>
    <url>/2018/06/05/react-set-state/</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p><code>setState</code> 是更新 <code>state</code> 的 API，进而会引发组件的重新渲染。然而在使用过程中发现有些坑(参见 <a href="https://doc.react-china.org/docs/state-and-lifecycle.html#%E6%AD%A3%E7%A1%AE%E5%9C%B0%E4%BD%BF%E7%94%A8%E7%8A%B6%E6%80%81" target="_blank" rel="noopener">react 正确使用状态</a>)：</p>
<blockquote>
<ol>
<li><strong><code>setState(obj)</code>一般情况下不会立即更新 <code>state</code> 的值；</strong></li>
<li><strong>同一 cycle 的多次 <code>setState</code> 调用可能会合并（性能考虑）</strong></li>
</ol>
</blockquote>
<p>对于第一点，引用下边的例子：</p>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">incrementMultiple</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.setState(&#123;<span class="attr">count</span>: <span class="keyword">this</span>.state.count + <span class="number">1</span>&#125;);</span><br><span class="line">  <span class="keyword">this</span>.setState(&#123;<span class="attr">count</span>: <span class="keyword">this</span>.state.count + <span class="number">1</span>&#125;);</span><br><span class="line">  <span class="keyword">this</span>.setState(&#123;<span class="attr">count</span>: <span class="keyword">this</span>.state.count + <span class="number">1</span>&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码运行时，虽然是对 <code>state</code> 加了三次，但是每次加操作都是针对初始的 <code>state</code>，所以最终相当于仅加了一次。即上述代码等同于下边的代码：</p>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Object</span>.assign(</span><br><span class="line">  previousState,</span><br><span class="line">  &#123;<span class="attr">quantity</span>: state.quantity + <span class="number">1</span>&#125;,</span><br><span class="line">  &#123;<span class="attr">quantity</span>: state.quantity + <span class="number">1</span>&#125;,</span><br><span class="line">  ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h1><h2 id="从-react-生命周期看-setState-生效时间"><a href="#从-react-生命周期看-setState-生效时间" class="headerlink" title="从 react 生命周期看 setState 生效时间"></a>从 react 生命周期看 setState 生效时间</h2><p>要理解其原因，我们要先看一下 <a href="https://github.com/superman66/Front-End-Blog/issues/2" target="_blank" rel="noopener">react 生命周期</a>：</p>
<p><img src="https://cloud.githubusercontent.com/assets/12592949/24903814/1b2ff98c-1ee1-11e7-9f5a-59eb84171b53.png" alt="react 生命周期"></p>
<p><code>setState</code> 引起状态更新涉及四个函数：</p>
<ul>
<li>shouldComponentUpdate（默认 true）</li>
<li>componentWillUpdate</li>
<li>render</li>
<li>componentDidUpdate</li>
</ul>
<p><strong>但直到 <code>render</code> 被调用时，<code>state</code> 才被更新</strong>。</p>
<p>（如果 <code>shouldComponentUpdate</code> 返回 <code>false</code>，则 <code>render</code> 不会被执行，但是 <code>state</code> 会被更新）</p>
<p>因此多次调用 <code>setState</code> 时，不能依靠 <code>this.state</code> 来计算下一个 <code>state</code> 的值，因为 <code>this.state</code> 一直没更新。</p>
<blockquote>
<p>setState() does not always immediately update the component. It may batch or defer the update until later. This makes reading this.state right after calling setState() a potential pitfall.</p>
</blockquote>
<p>但是可以使用 setState(func) api 来解决上述问题</p>
<h2 id="setState-api"><a href="#setState-api" class="headerlink" title="setState api"></a><a href="https://doc.react-china.org/docs/react-component.html#setstate" target="_blank" rel="noopener">setState api</a></h2><p><code>setState</code> api 如下：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">setState(updater[, callback])</span><br></pre></td></tr></table></figure>
<p>其中，<code>callback</code> 是在 <code>setState</code> 完成且组件 re-render 完成后执行（一般建议用 <code>componentDidUpdate</code> 来实现类似逻辑）。</p>
<p>而 <code>updater</code> 可以是：</p>
<ul>
<li>obj：异步将 obj <strong>shallow merge</strong> 到旧 <code>state</code>，构建新 <code>state</code></li>
<li>func：形式如下：</li>
</ul>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">(prevState, props) =&gt; stateChange</span><br></pre></td></tr></table></figure>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>上述例子做如下修改即可实现真正的累加：</p>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">increment</span>(<span class="params">state</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> &#123;<span class="attr">count</span>: state.count + <span class="number">1</span>&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">incrementMultiple</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.setState(increment);</span><br><span class="line">  <span class="keyword">this</span>.setState(increment);</span><br><span class="line">  <span class="keyword">this</span>.setState(increment);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://www.jianshu.com/p/9278c4835c55" target="_blank" rel="noopener">更详细请点击参考这篇文章</a></p>
]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title>多维数据模型</title>
    <url>/2020/12/21/%E5%A4%9A%E7%BB%B4%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/159061537" target="_blank" rel="noopener">数据仓库建模</a></p>
<p><a href="https://www.cnblogs.com/cyechina/p/5425842.html" target="_blank" rel="noopener">数据仓库的多维数据模型</a></p>
<p><a href="http://webdataanalysis.net/web-data-warehouse/multidimensional-data-model/" target="_blank" rel="noopener">数据仓库的多维数据模型 – 非常好的一系列文章</a></p>
<h1 id="Kimball-维度建模"><a href="#Kimball-维度建模" class="headerlink" title="Kimball 维度建模"></a>Kimball 维度建模</h1><p><strong>维度建模就是时刻考虑如何能够提供简单性，以业务为驱动，以用户理解性和查询性能为目标</strong></p>
<p><a href="https://segmentfault.com/a/1190000038938864" target="_blank" rel="noopener">kimball维度建模详解</a></p>
<p>维度建模分为两种表：事实表和维度表</p>
<ol>
<li><strong>事实表</strong>：必然存在的一些数据，像采集的日志文件，订单表，都可以作为事实表</li>
</ol>
<p>特征：<strong>是一堆主键的集合</strong>，每个主键对应维度表中的一条记录，客观存在的，根据主题确定出需要使用的数据</p>
<ol>
<li><strong>维度表</strong>：维度就是所分析的数据的一个量，维度表就是以合适的角度来创建的表，分析问题的一个角度：时间、地域、终端、用户等角度</li>
</ol>
<h1 id="多维数据模型的定义和作用"><a href="#多维数据模型的定义和作用" class="headerlink" title="多维数据模型的定义和作用"></a>多维数据模型的定义和作用</h1><p>　　多维数据模型是为了满足用户从多角度多层次进行数据查询和分析的需要而建立起来的基于事实和维的数据库模型，其基本的应用是为了实现OLAP（Online Analytical Processing）。</p>
<p>　　当然，通过多维数据模型的数据展示、查询和获取就是其作用的展现，但其真的作用的实现在于，通过数据仓库可以根据不同的数据需求建立起各类多维模型，并组成数据集市开放给不同的用户群体使用，也就是根据需求定制的各类数据商品摆放在数据集市中供不同的数据消费者进行采购。</p>
<p><strong>多维数据模型最大的优点就是其基于分析优化的数据组织和存储模式。</strong></p>
<h2 id="主题建模"><a href="#主题建模" class="headerlink" title="主题建模"></a>主题建模</h2><p><a href="https://zhuanlan.zhihu.com/p/113790356" target="_blank" rel="noopener">多维分析仓库构建-面向主题的建模</a></p>
<h3 id="构成"><a href="#构成" class="headerlink" title="构成"></a>构成</h3><p>主题建模是对原始数据、原始业务理解的基础上，将数据归类为多个主题（e.g. 销量主题、维修订单主题、线索转化主题…）。</p>
<p>一般，一个主题就是由一张事实表、多张维表、以及结果聚合表所组成。</p>
<ol>
<li>基于多维数据模型构建底层：事实表+维表</li>
<li>基于上述模型，聚合结果，生成聚合数据。</li>
</ol>
<p>事实表要尽可能宽，尽可能容纳此主题下所有指标，如果有新指标需求，则动态添加指标。但是事实表太宽可能导致后续计算资源不足，如果需要拆分事实，拆分事实表的过程即拆分子主题。对事实表的拆分不明确，即主题不明确，会导致后续资源的浪费或者维护成本的提高。因为后续可能出现衍生指标需要两个主题出的情况，那么需要再新出一个综合主题。</p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>主题建模是对数据的分类，这需要对领域或企业内数据特征有深刻理解。清晰的主题规划往往是数仓设计成败的关键。</p>
<p>与主题建模相对的，是按需输出数据，按照产品的需求出对应指标。</p>
<p>按需出指标，不必等待多维分析数仓建立完成即可开始，前期开发周期短。</p>
<p>主题建模是提前将维度和指标的全集定义好，聚合尽可能多的维度属性和指标的组合，与产品需求解耦，不会随产品需求增加而将数仓变得臃肿，可维护性好，长远来看性能上也更好。</p>
<h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h3><p>原子指标、衍生指标的增长：需要增加结果表的schema，所有数据库是兼容的，产生结果表的SQL修改其中读取事实表的查询，可以向后兼容</p>
<p>维度属性的增长：在维度表中增加具体的维度属性即可，不需要其他修改。</p>
<p>维度的增长：产生结果表的SQL增加新的维度表，结果表的schema也进行相应修改。</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>在看实例前，这里需要先了解两个概念：<strong>事实表和维表</strong>。事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；维表则是对事实表中事件的要素的描述信息。比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。这里不再展开了，解释概念真的很麻烦，而且基于我的理解的描述不一定所有人都能明白，还是直接上实例吧：</p>
<p><img src="http://webdataanalysis.net/wp-content/uploads/2010/08/Star-Schemas.png" alt="Star-Schemas"></p>
<p>事实表里面主要包含两方面的信息：<strong>维和度量</strong>，维的具体描述信息记录在维表，事实表中的维属性只是一个关联到维表的键，并不记录具体信息；度量一般都会记录事件的相应数值，比如这里的产品的销售数量、销售额等。维表中的信息一般是可以分层的，比如时间维的年月日、地域维的省市县等，这类分层的信息就是为了满足事实表中的度量可以在不同的粒度上完成聚合，比如2010年商品的销售额，来自上海市的销售额等。</p>
<h2 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h2><p>事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；如<strong>系统</strong>的<strong>日志</strong>、<strong>销售记录</strong>、<strong>用户访问日志</strong>等信息，<strong>事实表的记录是动态的增长的</strong>，所以<strong>体积是大于维度表</strong>。<strong>即：用户关心的业务数据，如销售数量，库存数量，销售金额</strong></p>
<h2 id="维表"><a href="#维表" class="headerlink" title="维表"></a>维表</h2><p>维表则是对事实表中事件的要素的描述信息。比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。</p>
<p><strong>维度表</strong>（Dimension Table）也称为<strong>查找表</strong>（Lookup Table）是<strong>与事实表相对应的表</strong>，这个表保存了<strong>维度的属性值</strong>，可以跟事实表做关联，<strong>相当于</strong>是将<strong>事实表</strong>中<strong>经常重复的数据抽取</strong>、<strong>规范出来用一张表管理</strong>，常见的有日期（日、周、月、季度等属性）、地区表等，所以<strong>维度表的变化通常不会太大</strong>。<strong>即：用来描述业务数据的数据，如日期、产品数据、地区、渠道</strong></p>
<p>基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。</p>
<h2 id="星型模型"><a href="#星型模型" class="headerlink" title="星型模型"></a>星型模型</h2><p>当所有维表都直接连接到“事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。<strong>数据有一定的冗余</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-1d39380d9238ca7c5876ac92d27750b2_1440w.jpg" alt="img"></p>
<h2 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h2><p>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。<strong>雪花模型是对星型模型的扩展</strong>。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是：<strong>通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-e7e1a7403be3ffb217f623d89771a573_1440w.jpg" alt="img"><em>**</em></p>
]]></content>
      <tags>
        <tag>big data</tag>
        <tag>data modeling</tag>
      </tags>
  </entry>
</search>
