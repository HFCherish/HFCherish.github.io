<!DOCTYPE html>
<html lang="Chinese">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0-rc1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hfcherish.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="从心所欲不逾矩">
<meta property="og:type" content="website">
<meta property="og:title" content="Cherish&#39;s Blog">
<meta property="og:url" content="http://hfcherish.github.io/page/2/index.html">
<meta property="og:site_name" content="Cherish&#39;s Blog">
<meta property="og:description" content="从心所欲不逾矩">
<meta property="og:locale">
<meta property="article:author" content="Cherish">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://hfcherish.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"Chinese","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Cherish's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Cherish's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cherish</p>
  <div class="site-description" itemprop="description">从心所欲不逾矩</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">68</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button animated">
    <button><i class="fa fa-comment"></i>
      Chat
    </button>
  </div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hfcherish" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hfcherish" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:pzcherishhf@gmail.com" title="E-Mail → mailto:pzcherishhf@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/12/18/airflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/18/airflow/" class="post-title-link" itemprop="url">airflow</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-18 15:33:14" itemprop="dateCreated datePublished" datetime="2020-12-18T15:33:14+08:00">2020-12-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-19 09:46:12" itemprop="dateModified" datetime="2023-05-19T09:46:12+08:00">2023-05-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><p><a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html">quickstart</a></p>
<blockquote>
<p>Airflow is published as <code>apache-airflow</code> package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open and applications usually pin them, but we should do neither and both at the same time. We decided to keep our dependencies as open as possible (in <code>setup.cfg</code> and <code>setup.py</code>) so users can install different version of libraries if needed. This means that from time to time plain <code>pip install apache-airflow</code> will not work or will produce unusable Airflow installation.</p>
<p>In order to have repeatable installation, however, starting from <strong>Airflow 1.10.10</strong> and updated in <strong>Airflow 1.10.13</strong> we also keep a set of “known-to-be-working” constraint files in the <code>constraints-master</code> and <code>constraints-1-10</code> orphan branches. Those “known-to-be-working” constraints are per major&#x2F;minor python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow version and python versions in the URL.</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --use-deprecated legacy-resolver <span class="string">&quot;apache-airflow==1.10.14&quot;</span> --constraint <span class="string">&quot;https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt&quot;</span> </span><br><span class="line"></span><br><span class="line">pip3 install <span class="string">&quot;apache-airflow==1.10.14&quot;</span> --constraint <span class="string">&quot;https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.8.txt&quot;</span> </span><br></pre></td></tr></table></figure>

<blockquote>
<p>On November 2020, new version of PIP (20.3) has been released with a new, 2020 resolver. This resolver does not yet work with Apache Airflow and might leads to errors in installation - depends on your choice of extras. In order to install Airflow you need to either downgrade pip to version 20.2.4 <code>pip upgrade --pip==20.2.4</code> or, in case you use Pip 20.3, you need to add option <code>--use-deprecated legacy-resolver</code> to your pip install command.</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># airflow needs a home, ~/airflow is the default,</span></span><br><span class="line"><span class="comment"># but you can lay foundation somewhere else if you prefer</span></span><br><span class="line"><span class="comment"># (optional)</span></span><br><span class="line"><span class="built_in">export</span> AIRFLOW_HOME=~/airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># install from pypi using pip</span></span><br><span class="line">pip install apache-airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the database</span></span><br><span class="line">airflow db init</span><br><span class="line"></span><br><span class="line">airflow <span class="built_in">users</span> create \</span><br><span class="line">    --username admin \</span><br><span class="line">    --firstname petrina \</span><br><span class="line">    --lastname zheng \</span><br><span class="line">    --role Admin \</span><br><span class="line">    --email spiderman@superhero.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the web server, default port is 8080</span></span><br><span class="line">airflow webserver --port 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the scheduler</span></span><br><span class="line"><span class="comment"># open a new terminal or else run webserver with ``-D`` option to run it as a daemon</span></span><br><span class="line">airflow scheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># visit localhost:8080 in the browser and use the admin account you just</span></span><br><span class="line"><span class="comment"># created to login. Enable the example_bash_operator dag in the home page</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/23/ambari-install-offline/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/23/ambari-install-offline/" class="post-title-link" itemprop="url">HDP install (offline using ambari)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-23 15:36:44" itemprop="dateCreated datePublished" datetime="2020-11-23T15:36:44+08:00">2020-11-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://wbaseurlww.cnblogs.com/shook/p/12409759.html">reference</a></p>
<p><a href="https://docs.cloudera.com/HDPDocuments/Ambari-latest/bk_ambari-installation/content/set_up_password-less_ssh.html">官方安装指导</a></p>
<h1 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h1><p>除非说明，默认以下操作都是在所有节点上执行</p>
<h2 id="修改-host"><a href="#修改-host" class="headerlink" title="修改 host"></a>修改 host</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># vi /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1             localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.105.137 master</span><br><span class="line">192.168.105.191 slave1</span><br><span class="line">192.168.105.13 slave2</span><br></pre></td></tr></table></figure>

<h2 id="修改-network-config"><a href="#修改-network-config" class="headerlink" title="修改 network config"></a>修改 network config</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># vi /etc/sysconfig/network</span></span><br><span class="line"><span class="comment"># Created by anaconda</span></span><br><span class="line">NETWORKING=<span class="built_in">yes</span></span><br><span class="line">HOSTNAME=master</span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># hostnamectl set-hostname master</span></span><br><span class="line">[root@master ~]<span class="comment"># hostname</span></span><br><span class="line">master</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping 各个节点，查看是否可连通</span></span><br><span class="line">[root@master ~]<span class="comment"># ping slave1</span></span><br><span class="line">PING slave1 (192.168.105.191) 56(84) bytes of data.</span><br></pre></td></tr></table></figure>

<h2 id="同步时间-ntp"><a href="#同步时间-ntp" class="headerlink" title="同步时间 ntp"></a>同步时间 ntp</h2><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><h2 id="关闭-Selinux-和-THP"><a href="#关闭-Selinux-和-THP" class="headerlink" title="关闭 Selinux 和 THP"></a>关闭 Selinux 和 THP</h2><h2 id="修改文件打开最大限制"><a href="#修改文件打开最大限制" class="headerlink" title="修改文件打开最大限制"></a><del>修改文件打开最大限制</del></h2><h2 id="SSH-无密码登录（主节点）"><a href="#SSH-无密码登录（主节点）" class="headerlink" title="SSH 无密码登录（主节点）"></a>SSH 无密码登录（主节点）</h2><h2 id="Reboot"><a href="#Reboot" class="headerlink" title="Reboot"></a>Reboot</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ shutdown -r now</span><br></pre></td></tr></table></figure>

<h1 id="制作本地源（离线安装）"><a href="#制作本地源（离线安装）" class="headerlink" title="制作本地源（离线安装）"></a>制作本地源（离线安装）</h1><h2 id="文件目录访问（http-服务方式）"><a href="#文件目录访问（http-服务方式）" class="headerlink" title="文件目录访问（http 服务方式）"></a>文件目录访问（http 服务方式）</h2><h2 id="制作本地源（主节点）"><a href="#制作本地源（主节点）" class="headerlink" title="制作本地源（主节点）"></a>制作本地源（主节点）</h2><h3 id="安装本地源制作相关工具"><a href="#安装本地源制作相关工具" class="headerlink" title="安装本地源制作相关工具"></a>安装本地源制作相关工具</h3><h3 id="修改文件里面的源地址"><a href="#修改文件里面的源地址" class="headerlink" title="修改文件里面的源地址"></a>修改文件里面的源地址</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@master ambari]<span class="comment"># vi ambari/centos7/2.7.4.0-118/ambari.repo</span></span><br><span class="line"><span class="comment">#VERSION_NUMBER=2.7.4.0-118</span></span><br><span class="line">[ambari-2.7.4.0]</span><br><span class="line"><span class="comment">#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json</span></span><br><span class="line">name=ambari Version - ambari-2.7.4.0</span><br><span class="line">baseurl=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/ambari/centos7/2.7.4.0-118/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">[root@master ambari]<span class="comment"># cp ambari/centos7/2.7.4.0-118/ambari.repo /etc/yum.repos.d/</span></span><br><span class="line">[root@master ambari]<span class="comment"># vi HDP/centos7/3.1.4.0-315/hdp.repo</span></span><br><span class="line"><span class="comment">#VERSION_NUMBER=3.1.4.0-315</span></span><br><span class="line">[HDP-3.1.4.0]</span><br><span class="line">name=HDP Version - HDP-3.1.4.0</span><br><span class="line">baseurl=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/HDP/centos7/3.1.4.0-315/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[HDP-UTILS-1.1.0.22]</span><br><span class="line">name=HDP-UTILS Version - HDP-UTILS-1.1.0.22</span><br><span class="line">baseurl=http://192.168.105.137/ambari/HDP-UTILS/centos7/1.1.0.22</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://192.168.105.137/ambari/HDP-UTILS/centowwws7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">[root@master ambari]<span class="comment"># cp HDP/centos7/3.1.4.0-315/hdp.repo /etc/yum.repos.d/</span></span><br></pre></td></tr></table></figure>



<h3 id="将创建好的源文件（-repo）拷贝到子节点"><a href="#将创建好的源文件（-repo）拷贝到子节点" class="headerlink" title="将创建好的源文件（.repo）拷贝到子节点"></a>将创建好的源文件（.repo）拷贝到子节点</h3><h1 id="安装-ambari-server"><a href="#安装-ambari-server" class="headerlink" title="安装 ambari-server"></a>安装 ambari-server</h1><h2 id="安装-ambari-server-1"><a href="#安装-ambari-server-1" class="headerlink" title="安装 ambari-server"></a>安装 ambari-server</h2><p>先安装，然后开始配置</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install ambari-server</span><br></pre></td></tr></table></figure>

<h2 id="设置并启动-ambari-server（主节点）"><a href="#设置并启动-ambari-server（主节点）" class="headerlink" title="设置并启动 ambari-server（主节点）"></a>设置并启动 ambari-server（主节点）</h2><p><a href="https://www.baeldung.com/find-java-home">how to find JAVA_HOME</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ java -XshowSettings:properties -version 2&gt;&amp;1 &gt; /dev/null | grep <span class="string">&#x27;java.home&#x27;</span> </span><br></pre></td></tr></table></figure>

<h3 id="使用默认-postgresql"><a href="#使用默认-postgresql" class="headerlink" title="使用默认 postgresql"></a>使用默认 postgresql</h3><p>Setup server 时，有几个交互式配置：</p>
<ol>
<li><p>是否自定义用户账户：</p>
<ol>
<li>选 n。即默认设置了 Ambari GUI 的登录用户为 admin&#x2F;admin。并且指定 Ambari Server 的运行用户为 root。</li>
</ol>
<blockquote>
<p>If you want to create a different user to run the Ambari Server, or to assign a previously created user, select <strong><code>y</code></strong> at the <code>Customize user account for ambari-server daemon</code> prompt, then provide a user name.</p>
</blockquote>
</li>
<li><p>JDK：</p>
<ol>
<li>选 2. 因为默认会安装并使用 oracle jdk，但是（1）不能联网下载（2）好像 oracle jdk 不会下载依赖包。所以自己安装好，在这指定 path 即可</li>
</ol>
</li>
<li><p>数据库：</p>
<ol>
<li>按默认配置创建 postgres 数据库</li>
</ol>
</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@master yum.repos.d]<span class="comment"># ambari-server setup</span></span><br><span class="line">Using python  /usr/bin/python</span><br><span class="line">Setup ambari-server</span><br><span class="line">Checking SELinux...</span><br><span class="line">SELinux status is <span class="string">&#x27;disabled&#x27;</span></span><br><span class="line">Customize user account <span class="keyword">for</span> ambari-server daemon [y/n] (n)? n</span><br><span class="line">Adjusting ambari-server permissions and ownership...</span><br><span class="line">Checking firewall status...</span><br><span class="line">Checking JDK...</span><br><span class="line">[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8</span><br><span class="line">[2] Custom JDK</span><br><span class="line">Enter choice (1): 2</span><br><span class="line">WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.</span><br><span class="line">WARNING: JCE Policy files are required <span class="keyword">for</span> configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.</span><br><span class="line">Path to JAVA_HOME: /usr/java/jdk1.8.0_202</span><br><span class="line">Validating JDK on Ambari Server...<span class="keyword">done</span>.</span><br><span class="line">Completing setup...</span><br><span class="line">Configuring database...</span><br><span class="line">Enter advanced database configuration [y/n] (n)? n</span><br><span class="line">Configuring database...</span><br><span class="line">Default properties detected. Using built-in database.</span><br><span class="line">Configuring ambari database...</span><br><span class="line">Checking PostgreSQL...</span><br><span class="line">Running initdb: This may take up to a minute.</span><br><span class="line">Initializing database ... OK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">About to start PostgreSQL</span><br><span class="line">Configuring <span class="built_in">local</span> database...</span><br><span class="line">Configuring PostgreSQL...</span><br><span class="line">Restarting PostgreSQL</span><br><span class="line">Creating schema and user...</span><br><span class="line"><span class="keyword">done</span>.</span><br><span class="line">Creating tables...</span><br><span class="line"><span class="keyword">done</span>.</span><br><span class="line">Extracting system views...</span><br><span class="line">ambari-admin-2.7.4.0-118.jar</span><br><span class="line">...........</span><br><span class="line">Adjusting ambari-server permissions and ownership...</span><br><span class="line">Ambari Server <span class="string">&#x27;setup&#x27;</span> completed successfully.</span><br></pre></td></tr></table></figure>

<h3 id="使用-mysql"><a href="#使用-mysql" class="headerlink" title="使用 mysql"></a>使用 mysql</h3><p><a href="https://programmer.group/linux-centos-7-mysql-5.7-offline-installation.html">离线安装 mysql</a></p>
<h4 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建用户组</span></span><br><span class="line">$ groupadd hdp</span><br><span class="line"><span class="comment"># 创建用户</span></span><br><span class="line">$ <span class="built_in">mkdir</span> /home/mysql</span><br><span class="line">$ useradd -g hdp hive -d /home/mysql/hive</span><br><span class="line">$ passwd hive</span><br><span class="line">yourPassword</span><br><span class="line"><span class="comment"># 创建临时目录</span></span><br><span class="line">$ <span class="built_in">mkdir</span> /home/mysql/hive/3306/data</span><br><span class="line">$ <span class="built_in">mkdir</span> /home/mysql/hive/3306/log</span><br><span class="line">$ <span class="built_in">mkdir</span> /home/mysql/hive/3306/tmp</span><br><span class="line">$ <span class="built_in">chown</span> -R hive:hdp /home/mysql/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">$ <span class="built_in">mv</span> mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz /usr/local</span><br><span class="line">$ <span class="built_in">cd</span> /usr/local</span><br><span class="line">$ tar -xzvf mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz</span><br><span class="line"><span class="comment"># Establish soft links for future upgrades</span></span><br><span class="line">$ <span class="built_in">ln</span> -s mysql-5.7.27-linux-glibc2.12-x86_64 mysql</span><br><span class="line"><span class="comment"># Modify users and user groups of all files under mysql folder</span></span><br><span class="line">$ <span class="built_in">chown</span> -R mysql:mysql mysql/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建配置文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc</span><br><span class="line">$ vi my.cnf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 mysql</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/local/mysql/bin</span><br><span class="line">$ ./mysqld --initialize --user=hive</span><br></pre></td></tr></table></figure>

<h4 id="安装-driver-并配置与-ambari-server-的-jdbc-连接"><a href="#安装-driver-并配置与-ambari-server-的-jdbc-连接" class="headerlink" title="安装 driver, 并配置与 ambari-server 的 jdbc 连接"></a>安装 driver, 并配置与 ambari-server 的 jdbc 连接</h4><p><a href="https://dev.mysql.com/downloads/connector/j/">mysql-connector-driver 下载</a> （选 RedHat 8 那个操作系统）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压</span></span><br><span class="line">$ rpm -ivh mysql80-community-release-el7-3.noarch.rpm</span><br><span class="line">$ <span class="built_in">cp</span> /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar</span><br><span class="line"><span class="comment"># 配置 driver path</span></span><br><span class="line">$ vi /etc/ambari-server/conf/ambari.properties</span><br><span class="line">添加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h4 id="配置-mysql"><a href="#配置-mysql" class="headerlink" title="配置 mysql"></a>配置 mysql</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置开机启动，并启动 mysql</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/local/mysql</span><br><span class="line"><span class="comment"># Copy the startup script to the resource directory and modify mysql.server. It&#x27;s better to modify mysqld as well. These two files are best synchronized.</span></span><br><span class="line">$ <span class="built_in">cp</span> ./support-files/mysql.server /etc/rc.d/init.d/mysqld</span><br><span class="line"><span class="comment"># Increase the execution privileges of mysqld service control scripts</span></span><br><span class="line">$ <span class="built_in">chmod</span> +x /etc/rc.d/init.d/mysqld</span><br><span class="line"><span class="comment"># Add mysqld service to system service</span></span><br><span class="line">$ chkconfig --add mysqld</span><br><span class="line"><span class="comment"># Check whether the mysqld service is in effect</span></span><br><span class="line">$ chkconfig --list mysqld </span><br><span class="line"><span class="comment"># mysql start</span></span><br><span class="line">$ service mysqld start</span><br><span class="line"><span class="comment"># View mysql status</span></span><br><span class="line">$ service mysqld status</span><br><span class="line"><span class="comment"># Check mysql related processes</span></span><br><span class="line">$ ps aux|grep mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">$ vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/local/mysql/bin</span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新 root 密码</span></span><br><span class="line">$ mysql -uroot -p</span><br><span class="line">$ mysql&gt; <span class="built_in">set</span> password <span class="keyword">for</span> root@localhost=password(<span class="string">&quot;root&quot;</span>);</span><br><span class="line"><span class="comment"># 配置 remote access to the mysql</span></span><br><span class="line">$ mysql&gt; use mysql</span><br><span class="line">$ mysql&gt; update user <span class="built_in">set</span> host=<span class="string">&#x27;%&#x27;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">$ mysql&gt; <span class="keyword">select</span> host,user from user;</span><br><span class="line">$ mysql&gt; grant all privileges on *.* to <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;yourPassword&#x27;</span>;</span><br><span class="line">$ mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>

<h4 id="创建-database"><a href="#创建-database" class="headerlink" title="创建 database"></a>创建 database</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE ambari;  </span><br><span class="line">use ambari;  </span><br><span class="line">CREATE USER &#x27;ambari&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;ambari&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;ambari&#x27;@&#x27;%&#x27;;  </span><br><span class="line">CREATE USER &#x27;ambari&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;ambar&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;ambari&#x27;@&#x27;localhost&#x27;;  </span><br><span class="line">CREATE USER &#x27;ambari&#x27;@&#x27;master&#x27; IDENTIFIED BY &#x27;ambari&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;ambari&#x27;@&#x27;master&#x27;;  </span><br><span class="line">FLUSH PRIVILEGES;  </span><br><span class="line">source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql  </span><br><span class="line"></span><br><span class="line">CREATE DATABASE hive;  </span><br><span class="line">use hive;  </span><br><span class="line">CREATE USER &#x27;hive&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;hive&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;hive&#x27;@&#x27;%&#x27;;  </span><br><span class="line">CREATE USER &#x27;hive&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;hive&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;hive&#x27;@&#x27;localhost&#x27;;  </span><br><span class="line">CREATE USER &#x27;hive&#x27;@&#x27;master&#x27; IDENTIFIED BY &#x27;hive&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;hive&#x27;@&#x27;master&#x27;;  </span><br><span class="line">FLUSH PRIVILEGES;  </span><br><span class="line">CREATE DATABASE oozie;  </span><br><span class="line">use oozie;  </span><br><span class="line">CREATE USER &#x27;oozie&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;oozie&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;oozie&#x27;@&#x27;%&#x27;;  </span><br><span class="line">CREATE USER &#x27;oozie&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;oozie&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;oozie&#x27;@&#x27;localhost&#x27;;  </span><br><span class="line">CREATE USER &#x27;oozie&#x27;@&#x27;master&#x27; IDENTIFIED BY &#x27;oozie&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;oozie&#x27;@&#x27;master&#x27;;  </span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>



<h4 id="Mysql-conf"><a href="#Mysql-conf" class="headerlink" title="Mysql conf"></a>Mysql conf</h4><p>上边 cnf 的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">[client]                                        # Client settings, the default connection parameters for the client</span><br><span class="line">port = 3306                                    # Default connection port</span><br><span class="line">socket = /home/mysql/hive/3306/tmp/mysql.sock                        # For socket sockets for local connections, the mysqld daemon generates this file</span><br><span class="line"></span><br><span class="line">[mysqld]                                        # Server Basic Settings</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Foundation setup</span></span><br><span class="line">user = hive</span><br><span class="line">bind-address = 0.0.0.0                         # Allow any ip host to access this database</span><br><span class="line">server-id = 1                                  # The unique number of Mysql service Each MySQL service Id needs to be unique</span><br><span class="line">port = 3306                                    # MySQL listening port</span><br><span class="line">basedir = /usr/local/mysql                      # MySQL installation root directory</span><br><span class="line">datadir = /home/mysql/hive/3306/data                      # MySQL Data File Location</span><br><span class="line">tmpdir  = /home/mysql/hive/3306/tmp                                  # Temporary directories, such as load data infile, will be used</span><br><span class="line">socket = /home/mysql/hive/3306/tmp/mysql.sock        # Specify a socket file for local communication between MySQL client program and server</span><br><span class="line">pid-file = /home/mysql/hive/3306/log/mysql.pid      # The directory where the pid file is located</span><br><span class="line">skip_name_resolve = 1                          # Only use IP address to check the client&#x27;s login, not the host name.</span><br><span class="line">character-set-server = utf8mb4                  # Database default character set, mainstream character set support some special emoticons (special emoticons occupy 4 bytes)</span><br><span class="line">transaction_isolation = READ-COMMITTED          # Transaction isolation level, which is repeatable by default. MySQL is repeatable by default.</span><br><span class="line">collation-server = utf8mb4_general_ci          # The character set of database corresponds to some sort rules, etc. Be careful to correspond to character-set-server.</span><br><span class="line">init_connect=&#x27;SET NAMES utf8mb4&#x27;                # Set up the character set when client connects mysql to prevent scrambling</span><br><span class="line">lower_case_table_names = 1                      # Is it case sensitive to sql statements, 1 means insensitive</span><br><span class="line">max_connections = 400                          # maximum connection</span><br><span class="line">max_connect_errors = 1000                      # Maximum number of false connections</span><br><span class="line">explicit_defaults_for_timestamp = true          # TIMESTAMP allows NULL values if no declaration NOT NULL is displayed</span><br><span class="line">max_allowed_packet = 128M                      # The size of the SQL packet sent, if there is a BLOB object suggested to be modified to 1G</span><br><span class="line">interactive_timeout = 1800                      # MySQL connection will be forcibly closed after it has been idle for more than a certain period of time (in seconds)</span><br><span class="line">wait_timeout = 1800                            # The default value of MySQL wait_timeout is 8 hours. The interactive_timeout parameter needs to be configured concurrently to take effect.</span><br><span class="line">tmp_table_size = 16M                            # The maximum value of interior memory temporary table is set to 128M; for example, group by, order by with large amount of data may be used as temporary table; if this value is exceeded, it will be written to disk, and the IO pressure of the system will increase.</span><br><span class="line">max_heap_table_size = 128M                      # Defines the size of memory tables that users can create</span><br><span class="line">query_cache_size = 0                            # Disable mysql&#x27;s cached query result set function; later test to determine whether to turn on or not based on business conditions; in most cases, close the following two items</span><br><span class="line">query_cache_type = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Memory settings allocated by user processes, and each session will allocate memory size <span class="keyword">for</span> parameter settings</span></span><br><span class="line">read_buffer_size = 2M                          # MySQL read buffer size. Requests for sequential table scans allocate a read buffer for which MySQL allocates a memory buffer.</span><br><span class="line">read_rnd_buffer_size = 8M                      # Random Read Buffer Size of MySQL</span><br><span class="line">sort_buffer_size = 8M                          # Buffer size used for MySQL execution sort</span><br><span class="line">binlog_cache_size = 1M                          # A transaction produces a log that is recorded in Cache when it is not committed, and persists the log to disk when it needs to be committed. Default binlog_cache_size 32K</span><br><span class="line"></span><br><span class="line">back_log = 130                                  # How many requests can be stored on the stack in a short time before MySQL temporarily stops responding to new requests; the official recommendation is back_log = 50 + (max_connections/5), with a cap of 900</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">log</span> setting</span></span><br><span class="line">log_error = /home/mysql/hive/3306/log/error.log                          # Database Error Log File</span><br><span class="line">slow_query_log = 1                              # Slow Query sql Log Settings</span><br><span class="line">long_query_time = 1                            # Slow query time; Slow query over 1 second</span><br><span class="line">slow_query_log_file = /home/mysql/hive/3306/log/slow.log                  # Slow Query Log Files</span><br><span class="line">log_queries_not_using_indexes = 1              # Check sql that is not used in the index</span><br><span class="line">log_throttle_queries_not_using_indexes = 5      # Represents the number of SQL statements per minute that are allowed to be logged to a slow log and are not indexed. The default value is 0, indicating that there is no limit.</span><br><span class="line">min_examined_row_limit = 100                    # The number of rows retrieved must reach this value before they can be recorded as slow queries. SQL that returns fewer than the rows specified by this parameter is not recorded in the slow query log.</span><br><span class="line">expire_logs_days = 5                            # MySQL binlog log log file saved expiration time, automatically deleted after expiration</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Master-slave replication settings</span></span><br><span class="line">log-bin = mysql-bin                            # Open mysql binlog function</span><br><span class="line">binlog_format = ROW                            # The way a binlog records content, recording each row being manipulated</span><br><span class="line">binlog_row_image = minimal                      # For binlog_format = ROW mode, reduce the content of the log and record only the affected columns</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Innodb settings</span></span><br><span class="line">innodb_open_files = 500                        # Restrict the data of tables Innodb can open. If there are too many tables in the library, add this. This value defaults to 300</span><br><span class="line">innodb_buffer_pool_size = 64M                  # InnoDB uses a buffer pool to store indexes and raw data, usually 60% to 70% of physical storage; the larger the settings here, the less disk I/O you need to access the data in the table.</span><br><span class="line">innodb_log_buffer_size = 2M                    # This parameter determines the size of memory used to write log files in M. Buffers are larger to improve performance, but unexpected failures can result in data loss. MySQL developers recommend settings between 1 and 8M</span><br><span class="line">innodb_flush_method = O_DIRECT                  # O_DIRECT reduces the conflict between the cache of the operating system level VFS and the buffer cache of Innodb itself.</span><br><span class="line">innodb_write_io_threads = 4                    # CPU multi-core processing capability settings are adjusted according to read-write ratio</span><br><span class="line">innodb_read_io_threads = 4</span><br><span class="line">innodb_lock_wait_timeout = 120                  # InnoDB transactions can wait for a locked timeout second before being rolled back. InnoDB automatically detects transaction deadlocks and rolls back transactions in its own lock table. InnoDB notices the lock settings with the LOCK TABLES statement. The default value is 50 seconds.</span><br><span class="line">innodb_log_file_size = 32M                      # This parameter determines the size of the data log file. Larger settings can improve performance, but also increase the time required to recover the failed database.</span><br></pre></td></tr></table></figure>



<h2 id="停止-ambari-server"><a href="#停止-ambari-server" class="headerlink" title="停止 ambari-server"></a>停止 ambari-server</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] ambari-server stop    <span class="comment">#停止命令</span></span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server reset   #重置命令</span></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server setup   #重新设置 </span></span><br><span class="line">[root@master ~]<span class="comment"># # ambari-server start   #启动命令</span></span><br></pre></td></tr></table></figure>

<h1 id="配置集群"><a href="#配置集群" class="headerlink" title="配置集群"></a>配置集群</h1><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>集群名字：ftms_hdp_qat</p>
<p>选择版本：</p>
<ul>
<li>hdp 3.1</li>
<li>redhat7</li>
</ul>
<h2 id="配置服务"><a href="#配置服务" class="headerlink" title="配置服务"></a>配置服务</h2><p>选择服务</p>
<p>选择 master&#x2F;slave for 各服务</p>
<p><img src="/images/ambari-20201125154733621.png" alt="image-20201125154733621"></p>
<h3 id="配置-hive-x2F-ozzie-x2F-ranger-database"><a href="#配置-hive-x2F-ozzie-x2F-ranger-database" class="headerlink" title="配置 hive&#x2F;ozzie&#x2F;ranger database"></a>配置 hive&#x2F;ozzie&#x2F;ranger database</h3><h3 id="使用-postgresql"><a href="#使用-postgresql" class="headerlink" title="使用 postgresql"></a>使用 postgresql</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 先执行下边这句话，再继续配置</span></span><br><span class="line">$ ambari-server setup --jdbc-db=postgres --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h3 id="使用-mysql-1"><a href="#使用-mysql-1" class="headerlink" title="使用 mysql"></a>使用 mysql</h3><p>使用 mysql（生产环境推荐使用），且 mysql 在其他地方也在用，而 postgresql 和 mysql 的语法是有区别的。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先执行下边这句话，再继续配置</span></span><br><span class="line">$ ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h3 id="配置-directories"><a href="#配置-directories" class="headerlink" title="配置 directories"></a>配置 directories</h3><p>采用默认的</p>
<h3 id="configurations"><a href="#configurations" class="headerlink" title="configurations"></a>configurations</h3><p>采用默认的</p>
<h1 id="异常调试"><a href="#异常调试" class="headerlink" title="异常调试"></a>异常调试</h1><h2 id="查看-ambari-的配置"><a href="#查看-ambari-的配置" class="headerlink" title="查看 ambari 的配置"></a>查看 ambari 的配置</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 ambari-server 的配置</span></span><br><span class="line">$ vi /etc/ambari-server/conf/ambari.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 hdp 各服务的配置</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/hdp/3.1.4.0-315/hbase/conf</span><br><span class="line"><span class="comment"># 运行服务</span></span><br><span class="line">$ /usr/hdp/3.1.4.0-315/hbase/bin/hbase shell</span><br></pre></td></tr></table></figure>

<h2 id="查看错误日志"><a href="#查看错误日志" class="headerlink" title="查看错误日志"></a>查看错误日志</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 ambari-server 启动的错误日志</span></span><br><span class="line">$ <span class="built_in">tail</span> /var/log/ambari-server/ambari-server.log -n 10 -f | less</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 hdp 各服务的日志</span></span><br><span class="line">$ <span class="built_in">cd</span> /usr/hdp/3.1.4.0-315/hbase/logs/hbase/hbase-hbase-regionserver-slave2.<span class="built_in">log</span></span><br><span class="line"><span class="comment"># 或者在 var 下看也一样，不知道是放了软 link 还是什么，日志好像是一样的</span></span><br><span class="line">$ <span class="built_in">cd</span> /var/log/hbase/hbase-hbase-regionserver-slave2.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 ranger service 的配置</span></span><br><span class="line">$ /etc/ranger/</span><br></pre></td></tr></table></figure>

<h2 id="重新安装"><a href="#重新安装" class="headerlink" title="重新安装"></a>重新安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ambari-server stop</span><br><span class="line">$ ambari-server reset</span><br><span class="line">$ ambari-server setup</span><br></pre></td></tr></table></figure>

<h2 id="安装找不到包"><a href="#安装找不到包" class="headerlink" title="安装找不到包"></a>安装找不到包</h2><h3 id="找不到-hdp-repo"><a href="#找不到-hdp-repo" class="headerlink" title="找不到 hdp.repo"></a>找不到 hdp.repo</h3><p>在实际安装时，ambari 会生成一个新的 ambari-hdp-1.repo，其中也定义了 hdp 的 baseurl 之类，这里对 hdp 定义的 name 可能是 <code>[HDP-3.1-repo-1]</code> , 而前边准备本地库时，定义的 hdp 的 name 是 <code>[HDP-3.1.4.0]</code>，这两个名字必须保持一致，否则 ambari 就找不到包（这是 ambari 的一个 bug）。</p>
<blockquote>
<p>解决方案：</p>
<p>将 <code>hdp.repo</code> 中的 [HDP-3.1.4.0] 改为 [HDP-3.1-repo-1]，并重新 scp 到各个节点</p>
</blockquote>
<h3 id="postfix找不到libmysqlclient-so-18"><a href="#postfix找不到libmysqlclient-so-18" class="headerlink" title="postfix找不到libmysqlclient.so.18"></a>postfix找不到libmysqlclient.so.18</h3><blockquote>
<p>还有一种简单的方法（没试过）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;$ yum reinstall mysql-libs -y</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先删除现在安装的 postfix</span></span><br><span class="line">$ systemctl <span class="built_in">disable</span> postfix</span><br><span class="line">$ systemctl stop postfix</span><br><span class="line">$ yum remove postfix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给 libmysqlclient.so.18 加上软链</span></span><br><span class="line">$ find / -name <span class="string">&#x27;*libmysqlclient.so.18*&#x27;</span></span><br><span class="line">/usr/lib64/mysql/libmysqlclient.so.18</span><br><span class="line">$ <span class="built_in">ln</span> -s /usr/lib64/mysql/libmysqlclient.so.18 /usr/lib/libmysqlclient.so.18</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新安装 postfix 并启动</span></span><br><span class="line">$ yum install postfix</span><br><span class="line">$ systemctl <span class="built_in">enable</span> postfix</span><br><span class="line">$ systemctl start postfix</span><br><span class="line">$ systemctl status postfix.service</span><br></pre></td></tr></table></figure>

<h3 id="找不到-libtirpc-devel"><a href="#找不到-libtirpc-devel" class="headerlink" title="找不到 libtirpc-devel"></a>找不到 libtirpc-devel</h3><p>如果出错，可能各个节点都需要做这件事</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不能联网</span></span><br><span class="line"><span class="comment"># 先下载 https://centos.pkgs.org/7/centos-x86_64/libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm.html 包</span></span><br><span class="line">$ yum-config-manager --<span class="built_in">enable</span> rhui-REGION-rhel-server-optional</span><br><span class="line">$ yum install libtirpc-devel-0.2.4-0.16.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果可以联网。这个可以从 CentOs-Base.repo 里下，但不能联网就没办法了</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc/yum.repos.d</span><br><span class="line">$ <span class="built_in">cp</span> backup/CentOs-Base.repo .</span><br></pre></td></tr></table></figure>

<h2 id="ranger-admin-start-fail"><a href="#ranger-admin-start-fail" class="headerlink" title="ranger-admin start fail"></a>ranger-admin start fail</h2><p>start ranger-admin fail.</p>
<blockquote>
<p>error detail:<br>This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you <em>might</em> want to use the less safe log_bin_trust_function_creators variable)</p>
</blockquote>
<p>Solution (<a href="https://stackoverflow.com/questions/26015160/deterministic-no-sql-or-reads-sql-data-in-its-declaration-and-binary-logging-i">stackflow</a>):</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Execute the following in the MySQL console:</span></span><br><span class="line">SET GLOBAL log_bin_trust_function_creators = 1;</span><br></pre></td></tr></table></figure>

<h2 id="atlas-server-启动失败"><a href="#atlas-server-启动失败" class="headerlink" title="atlas server 启动失败"></a>atlas server 启动失败</h2><h3 id="Ranger-authorization-失败"><a href="#Ranger-authorization-失败" class="headerlink" title="Ranger authorization 失败"></a>Ranger authorization 失败</h3><blockquote>
<p>执行 <code>cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n</code> 命令时，失败报 404</p>
<p>Error detail:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.coprocessor.CoprocessorException: HTTP 404 Error: HTTP 404</span><br><span class="line">	at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1261)</span><br><span class="line">	at org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.grant(RangerAuthorizationCoprocessor.java:1072)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos$AccessControlService<span class="variable">$1</span>.grant(AccessControlProtos.java:10023)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos<span class="variable">$AccessControlService</span>.callMethod(AccessControlProtos.java:10187)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:8135)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:2426)</span><br><span class="line">	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:2408)</span><br><span class="line">	at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService<span class="variable">$2</span>.callBlockingMethod(ClientProtos.java:42010)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:131)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor<span class="variable">$Handler</span>.run(RpcExecutor.java:324)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.RpcExecutor<span class="variable">$Handler</span>.run(RpcExecutor.java:304)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
<p>这个原因是 atlas 通过 ranger 访问 hbase 的时候没有权限。可能是由于安装先后顺序的原因，也有 atlas + ranger 本身需要手动配置的原因，导致 ranger 中没有配置 atlas 对 hbase、kafka 的访问权限。因此需要做做几件事：</p>
<ol>
<li><p>在 ambari 的 ranger config 中，启动 hbase ranger plugin，并重启相关服务</p>
</li>
<li><p>在 ambari 的 ranger config 中，启动 kafka ranger  plugin，并重启相关服务 &#x3D;&#x3D;&#x3D;&gt; 暂时没做</p>
</li>
<li><p>在 ranger 添加 hbase 的 service（参照 <a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/authorization-ranger/content/resource_service_configure_an_hbase_service.html">Configure a Resource-based Service: HBase</a>)</p>
<ol>
<li>这里 service 的名称必须和 <code>/usr/hdp/3.1.4.0-315/ranger-hbase-plugin/install.properties</code> 里配置 <code>REPOSITORY_NAME</code> 保持一致（但是似乎并不需要？）</li>
<li>username，password 说是 The end system username that can be used for connection.（目前来看，我现在是随便写的 admin 的账号）</li>
<li>Zookeeper 和 hbase.authentication 的配置是和  <code>/usr/hdp/3.1.4.0-315/hbase/conf/hbase-site.xml</code> 中的配置保持一致</li>
</ol>
<p><img src="/images/ambari-20201130112641591.png" alt="image-20201130112641591"></p>
<p><img src="/images/ambari-20201130112704671.png" alt="image-20201130112704671"></p>
</li>
<li><p>添加 atlas 对 hbase tables、kafka topic 的访问 policies（可参看：<a href="https://github.com/emaxwell-hw/Atlas-Ranger-Tag-Security/blob/master/README.md">tag-based security with atlas + ranger</a>）&#x3D;&#x3D;&#x3D;》 暂时没做 kafka</p>
<ol>
<li><strong>创建 hbase 的 policies 时，必须给 all - table, column-family, column 加上 <code>hbase</code> 这个 user</strong>，否则可能会遇到 403。这个原因是启动 metadata server 时，会执行 <code>cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n</code> 这么一条命令，执行时，会切换到 <code>hbase</code> 这个用户，如果这里不加权限，这条命令就会执行失败</li>
</ol>
<p><img src="/images/ambari-20201130112445208.png" alt="image-20201130112445208"></p>
</li>
</ol>
<p>ranger 的访问链接: <a href="http://ranger-server-host:6080/index.html">http://ranger-server-host:6080/index.html</a> (可以从 ambari ranger config 中看到)</p>
<h3 id="生成-jar-包失败（报-no-such-file-or-directory"><a href="#生成-jar-包失败（报-no-such-file-or-directory" class="headerlink" title="生成 jar 包失败（报 no such file or directory)"></a>生成 jar 包失败（报 no such file or directory)</h3><blockquote>
<p>在执行 <code>source /usr/hdp/current/atlas-server/conf/atlas-env.sh ; /usr/hdp/current/atlas-server/bin/atlas_start.py</code> 时报错，</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  File <span class="string">&quot;/usr/hdp/current/atlas-server/bin/atlas_start.py&quot;</span>, line 163, <span class="keyword">in</span> </span><br><span class="line">    returncode = main()</span><br><span class="line">  File <span class="string">&quot;/usr/hdp/current/atlas-server/bin/atlas_start.py&quot;</span>, line 73, <span class="keyword">in</span> main</span><br><span class="line">    mc.expandWebApp(atlas_home)</span><br><span class="line">  File <span class="string">&quot;/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py&quot;</span>, line 160, <span class="keyword">in</span> expandWebApp</span><br><span class="line">    jar(atlasWarPath)</span><br><span class="line">  File <span class="string">&quot;/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py&quot;</span>, line 213, <span class="keyword">in</span> jar</span><br><span class="line">    process = runProcess(commandline)</span><br><span class="line">  File <span class="string">&quot;/usr/hdp/3.1.4.0-315/atlas/bin/atlas_config.py&quot;</span>, line 249, <span class="keyword">in</span> runProcess</span><br><span class="line">    p = subprocess.Popen(commandline, stdout=stdoutFile, stderr=stderrFile, shell=shell)</span><br><span class="line">  File <span class="string">&quot;/usr/lib64/python2.7/subprocess.py&quot;</span>, line 711, <span class="keyword">in</span> __init__</span><br><span class="line">    errread, errwrite)</span><br><span class="line">  File <span class="string">&quot;/usr/lib64/python2.7/subprocess.py&quot;</span>, line 1327, <span class="keyword">in</span> _execute_child</span><br><span class="line">    raise child_exception</span><br><span class="line">OSError: [Errno 2] No such file or directory</span><br></pre></td></tr></table></figure>
</blockquote>
<p>通过在 <code>atlas_config.py</code> 中添加 log，发现最后是在执行 <code>/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/jar -xf /usr/hdp/3.1.4.0-315/atlas/server/webapp/atlas.war</code> 时报错，找不到的是 jar 命令. 原因是 jar 是 jdk 中的命令，而使用的默认 openjdk 其实只安装了 jre.</p>
<blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有的 openjdk 列表</span></span><br><span class="line">$ yum list | grep jdk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 jdk</span></span><br><span class="line">$ yum install java-1.8.0-openjdk.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># copy jar 到指定目录</span></span><br><span class="line">$ <span class="built_in">cp</span> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/bin/jar /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b03-1.el7.x86_64/jre/bin/</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Host-Disk-Usage-alert"><a href="#Host-Disk-Usage-alert" class="headerlink" title="Host Disk Usage alert"></a>Host Disk Usage alert</h2><p>安装过程下载了挺多乱七八糟的东西，导致硬盘报警了</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看目前各文件系统的硬盘使用情况，如果设置的 80% 报警，则只要有某个文件系统的使用哪个超了，就会报警</span></span><br><span class="line">$ <span class="built_in">df</span> -h</span><br><span class="line">文件系统                       容量  已用  可用 已用% 挂载点</span><br><span class="line">devtmpfs                        32G     0   32G    0% /dev</span><br><span class="line">tmpfs                           32G     0   32G    0% /dev/shm</span><br><span class="line">tmpfs                           32G  824M   31G    3% /run</span><br><span class="line">tmpfs                           32G     0   32G    0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/centos-root         50G   31G   20G   61% /</span><br><span class="line">/dev/sda1                     1014M  154M  861M   16% /boot</span><br><span class="line">/dev/mapper/vg_data2-lv_data2  200G   55M  200G    1% /data02</span><br><span class="line">/dev/mapper/centos-home         47G  444M   47G    1% /home</span><br><span class="line">/dev/mapper/vg_data1-lv_data1  200G  4.9G  196G    3% /data01</span><br><span class="line">tmpfs                          6.3G     0  6.3G    0% /run/user/0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看现在系统中的文件占用情况，找大文件去清</span></span><br><span class="line">$ <span class="built_in">du</span> -hs /*</span><br><span class="line">$ <span class="built_in">du</span> -hs /root/*</span><br><span class="line">$ <span class="built_in">du</span> -hs /var/log/*</span><br><span class="line">$ <span class="built_in">du</span> -h /var/* -d 1 | <span class="built_in">sort</span> -n -r</span><br></pre></td></tr></table></figure>



<h1 id="验证各服务可用"><a href="#验证各服务可用" class="headerlink" title="验证各服务可用"></a>验证各服务可用</h1><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="创建文件夹（可选？？？）"><a href="#创建文件夹（可选？？？）" class="headerlink" title="创建文件夹（可选？？？）"></a>创建文件夹（可选？？？）</h3><p><a href="https://www.tutorialspoint.com/hive/hive_quick_guide.htm">hive 验证可用</a></p>
<h3 id="配置-ranger-service-amp-policies"><a href="#配置-ranger-service-amp-policies" class="headerlink" title="配置 ranger service &amp; policies"></a>配置 ranger service &amp; policies</h3><p><a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/authorization-ranger/content/resource_service_configure_a_hive_service.html">ranger-hive-service</a></p>
<p>service 命名</p>
<p>url: get from ambari-hive, or when you connect hive, it will show the whole connect string</p>
<p>policy 中 <code>all-database,table,column</code> 加上 <code>hive</code> user </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 重启 spark 服务</span><br><span class="line">Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br></pre></td></tr></table></figure>

<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ManagedandExternalTables">hive managed &amp; external tables</a></p>
<p>acid table not supported now （acid new feature in hive，many others does not support it)</p>
<p><a href="https://github.com/Gowthamsb12/BigData-Blogs/blob/master/Spark_ACID">spark-acids thoughts</a></p>
<p><img src="/images/ambari-table-create-in-hive.png" alt="image-20201211154708672"></p>
<p><img src="/images/ambari-table-create-in-spark.png" alt="image-20201211154728273"></p>
<p>I figured it out. Just set: <code>mapred.input.dir.recursive</code> and <code>hive.mapred.supports.subdirectories</code> to <code>true</code>. (Hive-site.xml)</p>
<p> &#x2F;warehouse&#x2F;tablespace&#x2F;managed&#x2F;hive&#x2F;ftms_ods.db&#x2F;test_user6&#x2F;delta_0000001_0000001_0000</p>
<p>&#x2F;warehouse&#x2F;tablespace&#x2F;managed&#x2F;hive&#x2F;ftms_ods.db&#x2F;test_user7&#x2F;part-00000-2a86feec-be9a-413d-a696-8ff115d14075-c000.snappy.orc</p>
<h2 id="包冲突"><a href="#包冲突" class="headerlink" title="包冲突"></a>包冲突</h2><p>xbean-asm5-shaded-4.4.jar</p>
<p>xmlbeans-3.1.0.jar</p>
<p>xercesImpl-2.9.1.jar</p>
<p>xerces2-xsd11-2.11.1.jar</p>
<p>Xmlapis.jar</p>
<h2 id="Xml"><a href="#Xml" class="headerlink" title="Xml"></a>Xml</h2><p>删除 &#x2F;user&#x2F;ftms&#x2F;lib&#x2F;xercesImpl-2.9.1.jar 和 &#x2F;user&#x2F;ftms&#x2F;lib&#x2F;xml-apis-1.3.04.jar</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: javax.xml.parsers.FactoryConfigurationError: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:311)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267)</span><br><span class="line">	at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:120)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.asXmlDocument(Configuration.java:3442)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3417)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3388)</span><br><span class="line">	at org.apache.hadoop.conf.Configuration.writeXml(Configuration.java:3384)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.getConfVarInputStream(HiveConf.java:2410)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:2703)</span><br><span class="line">	at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:2657)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.hiveConf(HiveMetaStoreUtil.scala:18)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.createHiveMetaStoreClient(HiveMetaStoreUtil.scala:23)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveMetaStoreClient(HiveMetaStoreUtil.scala:34)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionCols(HiveMetaStoreUtil.scala:78)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveMetaStoreUtil$.getHiveTablePartitionColNames(HiveMetaStoreUtil.scala:73)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveDataSource$.buildInsertSql(HiveDataSource.scala:7)</span><br><span class="line">	at com.ftms.datapipeline.common.HiveDataSource$.save(HiveDataSource.scala:42)</span><br><span class="line">	at com.ftms.datapipeline.tasks.dwd.DCompany$.main(DCompany.scala:197)</span><br><span class="line">	at com.ftms.datapipeline.tasks.dwd.DCompany.main(DCompany.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)</span><br><span class="line">Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)</span><br><span class="line">	... 23 more</span><br><span class="line">Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Provider org.apache.xerces.jaxp.DocumentBuilderFactoryImpl not found</span><br><span class="line">	at java.util.ServiceLoader.fail(ServiceLoader.java:239)</span><br><span class="line">	at java.util.ServiceLoader.access$300(ServiceLoader.java:185)</span><br><span class="line">	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372)</span><br><span class="line">	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)</span><br><span class="line">	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:294)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)</span><br><span class="line">	... 23 more</span><br></pre></td></tr></table></figure>



<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="在-yarn-client-模式下运行-spark"><a href="#在-yarn-client-模式下运行-spark" class="headerlink" title="在 yarn-client 模式下运行 spark"></a>在 yarn-client 模式下运行 spark</h3><p>会出现 <code>Library directory &#39;...\assembly\target\scala-2.11\jars&#39; does not exist; make sure Spark is built.</code>，这个大致原因是 yarn-client 模式下</p>
<p><a href="https://www.jianshu.com/p/016fbd2a421b"><code>spark.yarn.jar</code>和<code>spark.yarn.archive</code>的使用</a></p>
<blockquote>
<p>Running Spark on YARN requires a binary distribution of Spark which is built with YARN support. Binary distributions can be downloaded from the <a href="https://spark.apache.org/downloads.html">downloads page</a> of the project website. To build Spark yourself, refer to <a href="https://spark.apache.org/docs/latest/building-spark.html">Building Spark</a>.<br> To make Spark runtime jars accessible from YARN side, you can specify <code>spark.yarn.archive</code> or <code>spark.yarn.jars</code>. For details please refer to <a href="https://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties">Spark Properties</a>. If neither <code>spark.yarn.archive</code> nor <code>spark.yarn.jars</code> is specified, Spark will create a zip file with all jars under <code>$SPARK_HOME/jars</code> and upload it to the distributed cache</p>
</blockquote>
<h1 id="Install-ClickHouse"><a href="#Install-ClickHouse" class="headerlink" title="Install ClickHouse"></a>Install ClickHouse</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 clickhouse</span></span><br><span class="line">$ rpm -ivh clickhouse-server-common-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-common-static-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-server-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line">$ rpm -ivh clickhouse-client-19.4.3.11-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">$ service clickhouse-server start</span><br><span class="line">Start clickhouse-server service: Path to data directory <span class="keyword">in</span> /etc/clickhouse-server/config.xml: /var/lib/clickhouse/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 client 验证运行成功</span></span><br><span class="line">$ clickhouse-client</span><br><span class="line">ClickHouse client version 19.4.3.11.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 19.4.3 revision 54416.</span><br><span class="line"></span><br><span class="line">master :) <span class="keyword">select</span> 1</span><br><span class="line"></span><br><span class="line">SELECT 1</span><br><span class="line"></span><br><span class="line">┌─1─┐</span><br><span class="line">│ 1 │</span><br><span class="line">└───┘</span><br><span class="line"></span><br><span class="line">1 rows <span class="keyword">in</span> <span class="built_in">set</span>. Elapsed: 0.001 sec.</span><br></pre></td></tr></table></figure>

<h2 id="client-启动失败"><a href="#client-启动失败" class="headerlink" title="client 启动失败"></a>client 启动失败</h2><blockquote>
<p>error detail:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ClickHouse client version 20.8.3.18.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Code: 102. DB::NetException: Unexpected packet from server localhost:9000 (expected Hello or Exception, got Unknown packet)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>这个错表示，clickhouse-client 收到返回了，但是返回的结果是非预期错误。这个错一般是由于端口占用。可以通过 <code>netstat -antp|grep LIST|grep 9000</code> 查询。</p>
<h3 id="Solution："><a href="#Solution：" class="headerlink" title="Solution："></a>Solution：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新 clickHouse 的端口为 9011</span></span><br><span class="line">$ vi /etc/clickhouse-server/config.xml</span><br><span class="line">:%s/9000/9011</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动时带上端口号</span></span><br><span class="line">$ clickhouse-client --port 9011</span><br></pre></td></tr></table></figure>

<h1 id="安装-python3"><a href="#安装-python3" class="headerlink" title="安装 python3"></a>安装 python3</h1><p><a href="https://www.python.org/downloads/release/python-361/">https://www.python.org/downloads/release/python-361/</a></p>
<p><a href="https://www.jianshu.com/p/758b592387d1">reference doc</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压并安装 python3</span></span><br><span class="line">$ tar -xf Python-3.?.?.tar.xz</span><br><span class="line">$ <span class="built_in">cd</span> Python-3.?.?</span><br><span class="line">$ ./configure</span><br><span class="line">$ make altinstall</span><br><span class="line">$ python3.x -V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建软链</span></span><br><span class="line">$ <span class="built_in">which</span> python3.6</span><br><span class="line">$ <span class="built_in">ln</span> -s /usr/local/bin/python3.6.1 /usr/bin/python3</span><br><span class="line">$ python3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 pip</span></span><br><span class="line">$ python3 -m ensurepip</span><br><span class="line">$ pip3</span><br></pre></td></tr></table></figure>

<blockquote>
<p>如果安装 pip 时报错 zlib not found：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 zlib 相关依赖包</span></span><br><span class="line">$ yum -y install zlib*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 python安装包,修改Module路径的setup文件，Modules/Setup.dist （或者 Modules/Setup） 文件</span></span><br><span class="line">$ vi Module/Setup</span><br><span class="line"><span class="comment">#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz</span></span><br><span class="line">去掉注释</span><br><span class="line">     zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz</span><br></pre></td></tr></table></figure>
</blockquote>
<h1 id="安装-airflow"><a href="#安装-airflow" class="headerlink" title="安装 airflow"></a>安装 airflow</h1><p>为了方便管理，可以安装个 mpack，然后就可以从 ambari 安装、管理、监控 airflow</p>
<ul>
<li><a href="https://miho120.medium.com/integrating-apache-airflow-with-apache-ambari-ccab2c90173">install airflow from ambari</a></li>
<li><a href="https://github.com/miho120/ambari-airflow-mpack">git</a></li>
</ul>
<p>这个插件在安装&#x2F;启动时，其实就是执行了 <code>/var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py</code> ，脚本中提供了安装、启动、停止。安装时，本质是执行了以下内容：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install apache-airflow[all]==1.9.0 apache-airflow[celery]==1.9.0</span><br></pre></td></tr></table></figure>

<blockquote>
<p>相关的安装、启动等脚本都在 <code>/var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts</code> 目录下</p>
</blockquote>
<p>但上述过程只能在有线环境执行，离线环境还是得自己下。</p>
<h2 id="install-airflow-offline"><a href="#install-airflow-offline" class="headerlink" title="install airflow offline"></a>install airflow offline</h2><p><a href="https://airflow.apache.org/docs/stable/installation.html">airflow installation 官方</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先下载相关包</span></span><br><span class="line">$ <span class="built_in">mkdir</span> airflow-install</span><br><span class="line">$ <span class="built_in">cd</span> airflow-install</span><br><span class="line">$ pip download <span class="string">&#x27;apache-airflow[all]==1.10.12&#x27;</span> \</span><br><span class="line">--constraint  <span class="string">&#x27;https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.6.txt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来更新上述 `airflow_scheduler_control.py` 中的安装脚本，选择从本地文件安装即可</span></span><br><span class="line">$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_scheduler_control.py</span><br><span class="line">$ vi /var/lib/ambari-agent/cache/common-services/AIRFLOW/1.10.0/package/scripts/airflow_webserver_control.py</span><br><span class="line"><span class="comment"># 下边这个命令是上述脚本的内容，这里只是介绍一下执行的命令</span></span><br><span class="line">$ pip install apache-airflow==1.10.12 --no-index -f ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过程中可能会有些包缺，例如 docutils、pytest-runner 等，从 https://pypi.org/ 下载相应的 .whl 文件，放到该目录下</span></span><br><span class="line"><span class="comment"># 然后通过 --no-index -f ./ 或者 --no-index -f ./xxx.whl 来安装即可</span></span><br><span class="line">$ pip install pytest-runner --no-index -f ./</span><br></pre></td></tr></table></figure>

<p>实际操作时，下载完 airflow 后，就更新 airflow_scheduler_control.py 和 airflow_webserver_control.py （两个文件的 install method 是一模一样的，按同样的方式修改就行）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">################# 源文件</span></span><br><span class="line">     <span class="number">11</span>         <span class="keyword">def</span> <span class="title function_">install</span>(<span class="params">self, env</span>):</span><br><span class="line">     <span class="number">12</span>                 <span class="keyword">import</span> params</span><br><span class="line">     <span class="number">13</span>                 env.set_params(params)</span><br><span class="line">     <span class="number">14</span>                 <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">     <span class="number">15</span>                 <span class="built_in">print</span>(env)</span><br><span class="line">     <span class="number">16</span>                 <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">     <span class="number">17</span>                 self.install_packages(env)</span><br><span class="line">     <span class="number">18</span>                 Logger.info(<span class="built_in">format</span>(<span class="string">&quot;Installing Airflow Service&quot;</span>))</span><br><span class="line">     <span class="number">19</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; pip&quot;</span>))</span><br><span class="line">     <span class="number">20</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; setuptools&quot;</span>))</span><br><span class="line">     <span class="number">21</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython==0.28&quot;</span>))</span><br><span class="line">     <span class="number">22</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-        installed apache-airflow[all]==1.10.0&quot;</span>))</span><br><span class="line">     <span class="number">23</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-        installed apache-airflow[celery]==1.10.0&quot;</span>))</span><br><span class="line">     <span class="number">24</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;chmod 755 /bin/airflow /usr/bin/airflow&quot;</span>))</span><br><span class="line">     <span class="number">25</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;useradd &#123;airflow_user&#125;&quot;</span>), ignore_failures=<span class="literal">True</span>)</span><br><span class="line">     <span class="number">26</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;mkdir -p &#123;airflow_home&#125;&quot;</span>))</span><br><span class="line">     <span class="number">27</span>                 airflow_make_startup_script(env)</span><br><span class="line">     <span class="number">28</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;&quot;</span>))</span><br><span class="line">     <span class="number">29</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb&quot;</span>),</span><br><span class="line">     <span class="number">30</span>                         user=params.airflow_user</span><br><span class="line">     <span class="number">31</span>                 )</span><br><span class="line"></span><br><span class="line"><span class="comment">################### 修改后的</span></span><br><span class="line">     <span class="number">11</span>         <span class="keyword">def</span> <span class="title function_">install</span>(<span class="params">self, env</span>):</span><br><span class="line">     <span class="number">12</span>                 <span class="keyword">import</span> params</span><br><span class="line">     <span class="number">13</span>                 env.set_params(params)</span><br><span class="line">    <span class="comment"># 这里是去安装 pip，已经装好了，就不装了</span></span><br><span class="line">     <span class="number">14</span> <span class="comment">#               self.install_packages(env)</span></span><br><span class="line">     <span class="number">15</span>                 Logger.info(<span class="built_in">format</span>(<span class="string">&quot;Installing Airflow Service&quot;</span>))</span><br><span class="line">     <span class="number">16</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; pip&quot;</span>))</span><br><span class="line">     <span class="number">17</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; setuptools&quot;</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx</span></span><br><span class="line">     <span class="number">18</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;pip install --upgrade &#123;airflow_pip_params&#125; docutils pytest-runner Cython --no-index -f /install/python_install/airflow-install/&quot;</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12，且仅安装 minimal packages</span></span><br><span class="line">     <span class="number">19</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow==1.10.12 --no-index -f /install/python_install/airflow-install/&quot;</span>))</span><br><span class="line">    <span class="comment"># 从指定目录找安装包 --no-index -f ./xxx，并更新版本为 1.10.12</span></span><br><span class="line">     <span class="number">20</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export SLUGIFY_USES_TEXT_UNIDECODE=yes &amp;&amp; pip install --upgrade &#123;airflow_pip_params&#125; --ignore-installed apache-airflow[celery]==1.10.12 --no-index -f /install/python_install/airflow-install/&quot;</span>))</span><br><span class="line">    <span class="comment"># 目前系统中默认是安装在 /usr/local/bin/airflow</span></span><br><span class="line">     <span class="number">21</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;chmod 755 /usr/local/bin/airflow&quot;</span>))</span><br><span class="line">     <span class="number">22</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;useradd &#123;airflow_user&#125;&quot;</span>), ignore_failures=<span class="literal">True</span>)</span><br><span class="line">     <span class="number">23</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;mkdir -p &#123;airflow_home&#125;&quot;</span>))</span><br><span class="line">     <span class="number">24</span>                 airflow_make_startup_script(env)</span><br><span class="line">     <span class="number">25</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;chown -R &#123;airflow_user&#125;:&#123;airflow_group&#125; &#123;airflow_home&#125;&quot;</span>))</span><br><span class="line">     <span class="number">26</span>                 Execute(<span class="built_in">format</span>(<span class="string">&quot;export AIRFLOW_HOME=&#123;airflow_home&#125; &amp;&amp; airflow initdb&quot;</span>),</span><br><span class="line">     <span class="number">27</span>                         user=params.airflow_user</span><br><span class="line">     <span class="number">28</span>                 )</span><br></pre></td></tr></table></figure>

<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p><strong>Admin Name</strong> : admin </p>
<p><strong>Cluster Name</strong> : ftms_hdp_qat </p>
<p><strong>Total Hosts</strong> : 3 (3 new) </p>
<p><strong>Repositories</strong>:</p>
<p>redhat7 (HDP-3.1):<br> <a href="http://10.66.18.11/ambari/HDP/centos7/3.1.4.0-315/">http://10.66.18.11/ambari/HDP/centos7/3.1.4.0-315/</a></p>
<p>redhat7 (HDP-3.1-GPL):<br> <a href="http://10.66.18.11/ambari/HDP-GPL/centos7/3.1.4.0-315/">http://10.66.18.11/ambari/HDP-GPL/centos7/3.1.4.0-315/</a></p>
<p>redhat7 (HDP-UTILS-1.1.0.22):<br> <a href="http://10.66.18.11/ambari/HDP-UTILS/centos7/1.1.0.22/">http://10.66.18.11/ambari/HDP-UTILS/centos7/1.1.0.22/</a></p>
<p><strong>Services:</strong></p>
<p><em>HDFS</em> </p>
<p>DataNode : 2 hosts </p>
<p>NameNode : master </p>
<p>NFSGateway : 0 host </p>
<p>SNameNode : slave1 </p>
<p><em>YARN + MapReduce2</em> </p>
<p>Timeline Service V1.5 : slave1 </p>
<p>NodeManager : 1 host </p>
<p>ResourceManager : master </p>
<p>Timeline Service V2.0 Reader : master </p>
<p>Registry DNS : master </p>
<p><em>Tez</em> </p>
<p>Clients : 1 host </p>
<p><em>Hive</em> </p>
<p>Metastore : slave1 </p>
<p>HiveServer2 : slave1 </p>
<p>Database : Existing MySQL &#x2F; MariaDB Database </p>
<p><em>HBase</em> </p>
<p>Master : master </p>
<p>RegionServer : 1 host </p>
<p>Phoenix Query Server : 0 host </p>
<p><em>Sqoop</em> </p>
<p>Clients : 1 host </p>
<p><em>Oozie</em> </p>
<p>Server : master </p>
<p>Database : Existing MySQL &#x2F; MariaDB Database </p>
<p><em>ZooKeeper</em> </p>
<p>Server : 3 hosts </p>
<p><em>Infra Solr</em> </p>
<p>Infra Solr Instance : master </p>
<p><em>Ambari Metrics</em> </p>
<p>Metrics Collector : slave2 </p>
<p>Grafana : master </p>
<p><em>Atlas</em> </p>
<p>Metadata Server : slave1 </p>
<p><em>Kafka</em> </p>
<p>Broker : master </p>
<p><em>Ranger</em> </p>
<p>Admin : slave1 </p>
<p>Tagsync : 1 host </p>
<p>Usersync : slave1 </p>
<p><em>SmartSense</em> </p>
<p>Activity Analyzer : master </p>
<p>Activity Explorer : master </p>
<p>HST Server : master </p>
<p><em>Spark2</em> </p>
<p>Livy for Spark2 Server : 0 host </p>
<p>History Server : master </p>
<p>Thrift Server : 0 host </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/13/atlas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/13/atlas/" class="post-title-link" itemprop="url">atlas</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-13 10:04:33" itemprop="dateCreated datePublished" datetime="2020-11-13T10:04:33+08:00">2020-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="http://atlas.apache.org/public/images/twiki/architecture.png" alt="img"></p>
<h1 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h1><p><a href="https://atlas.apache.org/2.0.0/InstallationSteps.html">install steps</a></p>
<p>Access Apache Atlas UI using a browser: <a href="http://localhost:21000/">http://localhost:21000</a> </p>
<p>You can also access the rest api <code>http://localhost:21000/api/atlas/v2</code></p>
<p>默认的用户名密码是 (admin, admin)</p>
<h1 id="Atlas-Features"><a href="#Atlas-Features" class="headerlink" title="Atlas Features"></a>Atlas Features</h1><h2 id="定义元模型，规范元数据"><a href="#定义元模型，规范元数据" class="headerlink" title="定义元模型，规范元数据"></a>定义元模型，规范元数据</h2><p>atlas 可以维护（增删改查） metadata types，支持</p>
<ul>
<li>创建多种类型的 metadata types<ul>
<li>businessmetadatadef：业务元数据的元模型</li>
<li>classificationdef：标签数据的元模型</li>
<li>entitydef：一般元数据的元模型</li>
<li>enumdef</li>
<li>relationshipdef：关系元数据的元模型</li>
<li>structdef</li>
</ul>
</li>
<li>元模型支持定义属性约束、索引、唯一性等</li>
<li>按 id&#x2F;typename&#x2F;query 来检索</li>
</ul>
<blockquote>
<p><a href="http://atlas.apache.org/api/v2/resource_TypesREST.html#resource_TypesREST_createAtlasTypeDefs_POST">相关 API 定义</a></p>
<p><a href="http://atlas.apache.org/api/v2/json_AtlasTypesDef.html">typedef request schema object</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># DELETE/GET/POST/PUT</span><br><span class="line">/v2/types/typedef</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="约束"><a href="#约束" class="headerlink" title="约束"></a>约束</h3><ul>
<li>typename 全局唯一</li>
</ul>
<h2 id="可以维护元数据"><a href="#可以维护元数据" class="headerlink" title="可以维护元数据"></a>可以维护元数据</h2><h3 id="import-metadata"><a href="#import-metadata" class="headerlink" title="import metadata"></a>import metadata</h3><p>atlas 提供以下途径将元数据引入系统：</p>
<ol>
<li>REST API：atlas 提供 api 可以 bulk saveOrUpdate 某个 type 的元数据</li>
<li>文件：atlas 可以上传文件，并 saveOrUpdate 文件中所定义的元模型、元数据等</li>
<li>atlas hook：atlas 通过监听 kafka topic <code>ATLAS_HOOK</code> ，来实时引入数据源中的元数据。目前已提供 Apache Hive&#x2F;Apache HBase&#x2F;Apache Storm&#x2F;Apache Sqoop 的 hook<ol>
<li>hive hook<ol>
<li>可以 import hive databases &amp; tables 元数据</li>
<li>可以监听以下类型的 hive 操作，capture 其中的元数据：<ol>
<li>create database</li>
<li>create table&#x2F;view, create table as select</li>
<li>load, import, export</li>
<li>DMLs (insert)</li>
<li>alter database</li>
<li>alter table (skewed table information, stored as, protection is not supported)</li>
<li>alter view</li>
</ol>
</li>
</ol>
</li>
<li>sqoop<ol>
<li>目前仅支持监听 sqoop 的 hiveimport operation 完成后，capture 其中的元数据。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="业务元数据"><a href="#业务元数据" class="headerlink" title="业务元数据"></a>业务元数据</h2><p>atlas 可以:</p>
<ol>
<li>定义业务元数据元模型规范业务元数据</li>
<li>在技术元数据上，添加业务元数据，来实现关联</li>
<li>支持按业务元数据检索</li>
</ol>
<h2 id="血缘"><a href="#血缘" class="headerlink" title="血缘"></a>血缘</h2><p>atlas 可以查询元数据血缘关系。应该是基于关系图实现。</p>
<p>目前 hive 表可以支撑到 column 级别的血缘分析。</p>
<h2 id="可以维护标签"><a href="#可以维护标签" class="headerlink" title="可以维护标签"></a>可以维护标签</h2><p>atlas 中的 classification 即标签，可以打在元数据、术语等地方。</p>
<p>可以基于 classification 检索元数据。</p>
<p>可以做 classification 的传播：</p>
<ul>
<li>基于继承关系链的传播</li>
<li>基于血缘关系链的传播</li>
</ul>
<h3 id="Classification-vs-label"><a href="#Classification-vs-label" class="headerlink" title="Classification vs label"></a>Classification vs label</h3><p>Atlas 中的 classification 和 label 都是标签的概念，label 是轻量级、谁都可以加的简单标签，classification 则有更多的支持。</p>
<p><a href="https://docs.cloudera.com/runtime/7.2.1/atlas-working-with-classifications/topics/atlas-working-with-classifications.html">atlas classification vs label</a></p>
<h2 id="可以维护企业术语表"><a href="#可以维护企业术语表" class="headerlink" title="可以维护企业术语表"></a>可以维护企业术语表</h2><p>atlas 可以维护企业的术语，并将术语与元数据关联，支持按术语检索元数据，通过以下三个概念来实现细粒度的术语管理：</p>
<ol>
<li>glossary：术语表，最高级别，所有的术语都必须属于某个术语表</li>
<li>category：术语分类，必须挂在某个 glossary。可以拥有 childCategories</li>
<li>term：术语。必须挂在某个 glossary。可以挂在某个 category</li>
</ol>
<h2 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h2><p>atlas 通过 kafka 实现 hook，引入元数据；也通过 kafka 广播元数据修改，供消费者使用。</p>
<ul>
<li><p>notifications to atlas：<code>ATLAS_HOOK</code>. 目前已提供 Apache Hive&#x2F;Apache HBase&#x2F;Apache Storm&#x2F;Apache Sqoop 的 hook，来监听这些数据源的元数据</p>
</li>
<li><p>notifications from atlas: <code>ATLAS_ENTITIES</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 监听并发布以下事件的通知</span><br><span class="line">ENTITY_CREATE:         sent when an entity instance is created</span><br><span class="line">ENTITY_UPDATE:         sent when an entity instance is updated</span><br><span class="line">ENTITY_DELETE:         sent when an entity instance is deleted</span><br><span class="line">CLASSIFICATION_ADD:    sent when classifications are added to an entity instance</span><br><span class="line">CLASSIFICATION_UPDATE: sent when classifications of an entity instance are updated</span><br><span class="line">CLASSIFICATION_DELETE: sent when classifications are removed from an entity instance</span><br><span class="line"></span><br><span class="line"># notification data</span><br><span class="line">AtlasEntity  entity;</span><br><span class="line">OperationType operationType;</span><br><span class="line">List&lt;AtlasClassification&gt;  classifications</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h2><p>atlas 所有的元数据存储在图数据库 JanusGraph，而索引数据则存储在 index store（solr &#x2F; elasticsearch）来做全文检索</p>
<p>atlas 支持以下检索方式：</p>
<ol>
<li>唯一定位元数据：通过 id</li>
<li>basic 检索：基于 type、attributes、classifciation、terms 等 query parameter 做全文检索</li>
<li>advance 检索：可以使用 dsl 语言做全文检索</li>
</ol>
<h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><ol>
<li>基础设施高可用。<ol>
<li>atlas 使用 JanusGraph 存储元数据，并将 HBase（默认，可采用其他数据库）作为 backing store。HBase 本身的高可用特性支撑了 metadata store 的高可用</li>
<li>atlas 使用 solr &#x2F; elasticsearch 存储元数据索引。这些组件同样支持高可用</li>
</ol>
</li>
<li>Web Service 高可用。<ol>
<li>目前 atlas 的 web service 同一时间只能有一个 active instance 响应，以实现元数据维护、缓存等的一致性问题。高可用模式即有多个备用（passive）instances，当 active instance down 后，可以自动切换某个 passive instance，作为新的 active instance。</li>
</ol>
</li>
</ol>
<h2 id="访问控制"><a href="#访问控制" class="headerlink" title="访问控制"></a>访问控制</h2><p>atlas 支持非常细粒度的访问控制：</p>
<ul>
<li><p>元模型：基于某个元模型或某类元模型的访问控制。典型 example：</p>
<blockquote>
<ul>
<li>Admin users can create&#x2F;update&#x2F;delete types of all categories</li>
<li>Data stewards can create&#x2F;update&#x2F;delete classification types</li>
<li>Healthcare data stewards can create&#x2F;update&#x2F;delete types having names start with “hc”</li>
</ul>
</blockquote>
</li>
<li><p>元数据：基于元模型、标签、元数据 id 的元数据访问控制。典型 example：</p>
<blockquote>
<ul>
<li>Admin users can perform all entity operations on entities of all types</li>
<li>Data stewards can perform all entity operations, except delete, on entities of all types</li>
<li>Data quality admins can add&#x2F;update&#x2F;remove DATA_QUALITY classification</li>
<li>Users in specific groups can read&#x2F;update entities with PII classification or its sub-classification</li>
<li>Finance users can read&#x2F;update entities whose ID start with ‘finance’</li>
</ul>
</blockquote>
</li>
<li><p>admin 操作：可以控制 user&#x2F;group 来 import&#x2F;export entities</p>
</li>
</ul>
<h2 id="UI"><a href="#UI" class="headerlink" title="UI"></a>UI</h2><p>有个 ui 可以管理元数据、标签、术语</p>
<h1 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h1><ol>
<li>web service 只有一个 active instance</li>
<li>Typename 全局唯一</li>
<li>ui 挺慢的</li>
</ol>
<h1 id="基于-atlas-我们可以做什么"><a href="#基于-atlas-我们可以做什么" class="headerlink" title="基于 atlas 我们可以做什么"></a>基于 atlas 我们可以做什么</h1><ol>
<li>数据集发现<ol>
<li>数据字典浏览和检索</li>
</ol>
</li>
<li>数据集导入和导出<ol>
<li>导出数据集，供系统之间交互使用</li>
<li>导入数据集（数据标准、指标口径等）</li>
</ol>
</li>
<li>标签管理：<ol>
<li>维护用户画像标签 (增删改查)</li>
<li>基于标签检索数据集</li>
</ol>
</li>
<li>数据标准维护和浏览：（可以通过术语做？）<ol>
<li>维护数据标准（增删改查）</li>
<li>可以为数据标准加标签</li>
</ol>
</li>
<li>指标和口径维护和浏览<ol>
<li>维护指标口径（增删改查）</li>
<li>可以为指标口径加标签</li>
</ol>
</li>
<li>数据集 staticstics<ol>
<li>数据接入和使用情况统计</li>
</ol>
</li>
<li>辅助数据分析师生成分析：<ol>
<li>通过关联技术元数据和业务元数据，当数据分析师按业务语言定义分析后，可以快速查找相关的表、字段等</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/11/import-data-to-hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/11/import-data-to-hive/" class="post-title-link" itemprop="url">数据导入 hive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-11 11:58:27" itemprop="dateCreated datePublished" datetime="2020-11-11T11:58:27+08:00">2020-11-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ftp-csv-文件导入"><a href="#ftp-csv-文件导入" class="headerlink" title="ftp .csv 文件导入"></a>ftp .csv 文件导入</h1><p>可以先将文件弄到 HDFS，然后创建&#x2F;更新 hive 表来关联到 HDFS 文件。</p>
<p>将文件弄到 HDFS有以下一些方法：</p>
<ol>
<li><strong>ftp -&gt; local -&gt; hdfs:</strong> 将文件先下载到本地，再通过 hdfs 命令拷贝到 hdfs 中</li>
<li><strong>ftp -&gt; hdfs</strong>: 直接连接 FTP，将文件拷到 hdfs 中，省却本地拷贝</li>
<li><strong>已有的数据采集工具</strong>：使用实时数据流处理系统，来实现不同系统之间的流通</li>
</ol>
<h2 id="一、ftp-gt-local-gt-hdfs"><a href="#一、ftp-gt-local-gt-hdfs" class="headerlink" title="一、ftp -&gt; local -&gt;hdfs"></a>一、ftp -&gt; local -&gt;hdfs</h2><p>几种方案：</p>
<ol>
<li><p><code>hadoop fs -get ftp://uid:password@server_url/file_path temp_file | hadoop fs -moveFromLocal tmp_file hadoop_path/dest_file</code> </p>
</li>
<li><p>参照<a href="https://community.cloudera.com/t5/Support-Questions/How-read-ftp-server-files-and-load-into-hdfs-in-incremental/m-p/223519/highlight/true#M185384">这个实现</a>用 python 包从 ftp 中读，然后用 hdfs 命令写到 hdfs</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> hdfs <span class="keyword">import</span> InsecureClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also use KerberosClient or custom client</span></span><br><span class="line">namenode_address = <span class="string">&#x27;your namenode address&#x27;</span></span><br><span class="line">webhdfs_port = <span class="string">&#x27;your webhdfs port&#x27;</span> <span class="comment"># default for Hadoop 2: 50070, Hadoop 3: 9870</span></span><br><span class="line">user = <span class="string">&#x27;your user name&#x27;</span></span><br><span class="line">client = InsecureClient(<span class="string">&#x27;http://&#x27;</span> + namenode_address + <span class="string">&#x27;:&#x27;</span> + webhdfs_port, user=user)</span><br><span class="line"></span><br><span class="line">ftp_address = <span class="string">&#x27;your ftp address&#x27;</span></span><br><span class="line">hdfs_path = <span class="string">&#x27;where you want to write&#x27;</span></span><br><span class="line"><span class="keyword">with</span> urlopen(ftp_address) <span class="keyword">as</span> response:</span><br><span class="line">    content = response.read()</span><br><span class="line">    <span class="comment"># You can also use append=True</span></span><br><span class="line">    <span class="comment"># Further reference: https://hdfscli.readthedocs.io/en/latest/api.html#hdfs.client.Client.write</span></span><br><span class="line">    <span class="keyword">with</span> client.write(hdfs_path) <span class="keyword">as</span> writer:</span><br><span class="line">        writer.write(content</span><br></pre></td></tr></table></figure>
</li>
<li><p>参考 <a href="https://blog.csdn.net/yiluohan0307/article/details/79364525">ftp 提取文件到 hdfs</a></p>
</li>
</ol>
<h2 id="二、ftp-gt-hdfs"><a href="#二、ftp-gt-hdfs" class="headerlink" title="二、ftp -&gt; hdfs"></a>二、ftp -&gt; hdfs</h2><p>几种方案：(参考 <a href="https://blog.csdn.net/yiluohan0307/article/details/79364525">ftp 提取文件到 hdfs</a>)</p>
<ol>
<li><p>用 <a href="http://hadoop101.blogspot.com/?view=classic">FTP To HDFS</a> 连接 ftp，把文件直接放到 hdfs</p>
</li>
<li><p>HDFS dfs -cp: 简单快速，但不显示进度，适用于小文件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs –<span class="built_in">cp</span> [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hadoop distcp: 分布式提取，快，能显示拷贝进度，不支持流式写入（即拷贝的文件不能有其他程序在写入），适合大量文件或大文件的拷贝</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop distcp [ftp://username:password@hostname/ftp_path] [hdfs:///hdfs_path]</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="三、已有的数据采集工具"><a href="#三、已有的数据采集工具" class="headerlink" title="三、已有的数据采集工具"></a>三、已有的数据采集工具</h2><h3 id="文件导入"><a href="#文件导入" class="headerlink" title="文件导入"></a>文件导入</h3><ol>
<li><p><a href="https://nifichina.github.io/general/GettingStarted.html#%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%88%AB%E7%9A%84%E5%A4%84%E7%90%86%E5%99%A8">apache NiFi</a> 来实现不同系统之间的流通，似乎拷贝完，会直接删除 ftp 上的文件</p>
</li>
<li><p>Apache Flume是一个分布式、可靠、高可用的日志收集系统，支持各种各样的数据来源。基于流式数据，适用于日志和事件类型的数据收集，重构后的Flume-NG版本中一个agent（数据传输流程）中的source（源）和sink（目标）之间通过channel进行链接，同一个源可以配置多个channel。多个agent还可以进行链接组合共同完成数据收集任务，使用起来非常灵活。</p>
<p><a href="https://blog.csdn.net/qq_39160721/article/details/80255588">flume 采集 ftp 文件 上传到 hadoop</a> 使用 <a href="https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#spooling-directory-source">spooldir source</a>（不确定是不是能用）, 也可以使用第三方 source 组件 <a href="https://github.com/keedio/flume-ftp-source">flume-ftp-source</a></p>
<blockquote>
<p>Flume 也支持 sql source 的流式导入（使用 <a href="https://github.com/keedio/flume-ng-sql-source">flume-ng-sql-source</a> 插件），并提供对数据进行简单处理，并写到各数据接收方的能力。因此它的实时性更好。</p>
</blockquote>
</li>
<li><p>DataX：阿里的开源框架，本身社区不太活跃，但有很多 fork 再改的，似乎架构不错</p>
</li>
<li><p>Gobllin: Gobblin是用来整合各种数据源的通用型ETL框架，Gobblin的接口封装和概念抽象做的很好，作为一个ETL框架使用者，我们只需要实现我们自己的Source，Extractor，Conventer类，再加上一些数据源和目的地址之类的配置文件提交给Gobblin就行了。Gobblin相对于其他解决方案具有普遍性、高度可扩展性、可操作性。</p>
</li>
<li><p>kettle：一款开源的ETL工具</p>
</li>
</ol>
<h3 id="其他数据源（非-FTP-文件）"><a href="#其他数据源（非-FTP-文件）" class="headerlink" title="其他数据源（非 FTP 文件）"></a>其他数据源（非 FTP 文件）</h3><ol>
<li>Apache Sqoop：RDBMS &lt;–&gt; HDFS</li>
<li>Aegisthus：针对 Cassandra 数据源</li>
<li>mongo-hadoop：针对 mongodb 数据源</li>
</ol>
<h1 id="数据导入需要关注的问题"><a href="#数据导入需要关注的问题" class="headerlink" title="数据导入需要关注的问题"></a>数据导入需要关注的问题</h1><ol>
<li><strong>数据源都有哪些？</strong><ol>
<li>结构化（sql）、半结构化（json, xml…)、非结构化（video、image、file…)</li>
<li>日志数据（csv)、业务数据</li>
</ol>
</li>
<li><strong>是否可以直接连接数据库？</strong><ol>
<li>针对关系型数据，如果可连接数据库，可以通过 sqoop 导入数据到 hive<ol>
<li>增量式导入？？</li>
</ol>
</li>
<li>针对关系型数据，如果不能连接数据库：<ol>
<li><strong>是否可以默认周期性导出符合特定标准的 .csv 文件？</strong><ol>
<li>如果数据库导出 dump 文件，再将 dump 文件导入到 hadoop，则比较麻烦，以 oracle 为例，可能需要使用 COPYToBDA 来创建 hive table <a href="https://weidongzhou.wordpress.com/2016/11/12/data-query-between-bda-and-exadata-part-4-query-oracle-dump-file-on-bda-using-copy2bda/">Query Oracle Dump File on BDA Using Copy2BDA</a> ，或者将 dump 文件先导入到一个 temp oracle 数据库中，再用 sqoop 导入到 hive</li>
<li>如果数据库周期性导出 .csv 文件，将这些 .csv 文件使用上述工具（flume 等）导入到 hive，需要关注增量式导出和导入<ol>
<li>增量式导出：文件的组织结构、命名规范 ，.csv 内 record 要求包含 modified date, delete date（在增量式导入时，需要基于这些时间来合并表）</li>
<li>增量式导入：将新增的 .csv 文件作为 hive external table，然后通过中间 view 来合并基表和incremental 表，并更新基表、清空 incremental 表。<a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html">Incrementally Updating a Table</a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>导入周期和实时性需求<ol>
<li><strong>哪些需要每天批量导入、哪些需要流式实时导入</strong></li>
<li><strong>哪些需要全量导入、哪些需要增量式导入？</strong></li>
</ol>
</li>
<li><strong>如何实现增量式导入？删除的数据是否有删除标识（软删除）？</strong><ol>
<li>如果用 sqoop，参考 <a href="https://hfcherish.github.io/2020/11/10/sqoop/">sqoop 增量导入</a>，不支持对删除数据的处理</li>
<li>如果用 flume<ol>
<li>如果是 sql source，使用 <a href="https://github.com/keedio/flume-ng-sql-source">flume-ng-sql-source</a>, 对于 mysql 可以通过 query <code>agent.sources.sqlSource.custom.query</code> 来获取增量 source</li>
<li>如果是文件导入，则需要通过 <a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html">Incrementally Updating a Table</a> 来合并表</li>
</ol>
</li>
<li>Spark SQL</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/10/sqoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/10/sqoop/" class="post-title-link" itemprop="url">sqoop</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-10 13:39:30" itemprop="dateCreated datePublished" datetime="2020-11-10T13:39:30+08:00">2020-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p>Sqoop: sq are the first two of “sql”, oop are the last three of “hadoop”. It <strong>transfers bulk data between hdfs and relational database servers</strong>. It supports:</p>
<ul>
<li>Full Load</li>
<li>Incremental Load</li>
<li>Parallel Import&#x2F;Export (throught mapper jobs)</li>
<li>Compression</li>
<li>Kerberos Security Integration</li>
<li>Data  loading directly to HIVE</li>
</ul>
<blockquote>
<p>Sqoop cannot import .csv files into hdfs&#x2F;hive. It only support databases &#x2F; mainframe datasets import.</p>
</blockquote>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>Sqoop provides CLI, thus you can use a simple command to conduct import&#x2F;export.</p>
<p>The import&#x2F;export are executes in fact through map tasks.</p>
<p><img src="/images/sqoop-20201110134912159.png" alt="sqoop-20201110134912159.png"></p>
<p>When Import from DB:</p>
<ul>
<li>it split to some map tasks. And each map task will connect to DB, and fetch some rows&#x2F;tables, and write it to a file into HDFS</li>
</ul>
<p>When export to DB:</p>
<ul>
<li>it also split to some map tasks. And each map task will fetch a HDFS file, and write each record in the file as a row by specified delimiter to some table.</li>
</ul>
<p><img src="/images/sqoop-20201110135220097.png" alt="sqoop-20201110135220097.png"></p>
<h1 id="Sqoop-v-s-Spark-jdbc"><a href="#Sqoop-v-s-Spark-jdbc" class="headerlink" title="Sqoop v.s. Spark jdbc"></a>Sqoop v.s. Spark jdbc</h1><p>sqoop uses mapreduce technique, while spark is a revolutionary engine to replace mapreduce technique with its in-memory execution and DAG smartness. Thus Spark jdbc is way faster than sqoop.</p>
<ol>
<li>You can combine all the read, transform and write operations into one script&#x2F;program instead of reading it separately through SQOOP in one script and then doing transformation and write in another.</li>
<li>You can define a new split column on the fly (using functions like ORA_HASH) if you want the data to be partitioned in a proper way.</li>
<li>You can control the number of connection to the database. Increasing the number of connection will surely speed up your data import.</li>
</ol>
<h1 id="Common-Commands"><a href="#Common-Commands" class="headerlink" title="Common Commands"></a>Common Commands</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$  sqoop import \</span><br><span class="line">	--connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt;</span><br><span class="line">	--table &lt;my_table name&gt;</span><br><span class="line">	--username &lt;username_for_my_sql&gt; --password &lt;password&gt;</span><br><span class="line">  --target-dir &lt;target <span class="built_in">dir</span> <span class="keyword">in</span> HDFS <span class="built_in">where</span> data needs to be imported&gt;</span><br><span class="line">  </span><br><span class="line">$  sqoop <span class="built_in">export</span> \</span><br><span class="line">	--connect jdbc:mysql://&lt;ipaddress&gt;/&lt;database name&gt;</span><br><span class="line">	--table &lt;my_table name&gt;</span><br><span class="line">	--username &lt;username_for_my_sql&gt; --password &lt;password&gt;</span><br><span class="line">  --export-dir &lt;target <span class="built_in">dir</span> <span class="keyword">in</span> HDFS <span class="built_in">where</span> data needs to be exported&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Incremental-Import"><a href="#Incremental-Import" class="headerlink" title="Incremental Import"></a>Incremental Import</h1><p>增量导入时，sqoop 需要识别到增量数据，有三种方法：</p>
<ol>
<li>根据自增字段识别新数据（append 模式）：可以直接识别新数据并导入到 hive 中</li>
<li>根据修改时间识别新数据（lastmodified 模式）：要求这个字段会随数据的改变而改变，但是似乎只能导入到 hdfs 中，不能直接导入到 hive 中。导入时，可以通过制定<code>--merge-key id</code> 来按 id 字段进行合并，或者之后通过 <code>sqoop merge</code> 功能单独运行</li>
<li>根据 where 或 query 识别新数据：可能之后只能通过 <code>sqoop merge</code> 来 merge 数据</li>
</ol>
<blockquote>
<p>目前 <strong>sqoop 导入时不能识别删除数据</strong>，都需要通过其他方式来解决（对比数据，或者数据上有 delete 标识符时，通过 <a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_data-access/content/incrementally-updating-hive-table-with-sqoop-and-ext-table.html">Incrementally Updating a Table</a> 来实现）</p>
</blockquote>
<h2 id="append-模式"><a href="#append-模式" class="headerlink" title="append 模式"></a>append 模式</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table oms_order \</span><br><span class="line">--target-dir  /tmp/query  \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--null-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--null-non-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--incremental append \	<span class="comment"># append 模式</span></span><br><span class="line">--check-column <span class="built_in">id</span> \			<span class="comment"># 自增字段</span></span><br><span class="line">--last-value 22 \				<span class="comment"># 自增字段的 last value</span></span><br><span class="line">-m 2 </span><br></pre></td></tr></table></figure>

<h2 id="lastmodified-模式"><a href="#lastmodified-模式" class="headerlink" title="lastmodified 模式"></a>lastmodified 模式</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/dicts \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table doit_stu \</span><br><span class="line">--target-dir  /doit_stu/2020-02-09  \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--null-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--null-non-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--incremental lastmodified \		<span class="comment"># lastmodified 模式</span></span><br><span class="line">--check-column update_time \		<span class="comment"># 时间字段</span></span><br><span class="line">--last-value <span class="string">&#x27;2020-02-09 23:59:59&#x27;</span> \	<span class="comment"># 上一次获取的数据时间</span></span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span> \</span><br><span class="line">--merge-key <span class="built_in">id</span> \					<span class="comment">#按照id字段进行合并</span></span><br><span class="line">-m 1 </span><br></pre></td></tr></table></figure>

<h2 id="条件查询"><a href="#条件查询" class="headerlink" title="条件查询"></a>条件查询</h2><p>这里写的都是全量导入 hive。如果要增量，只能先导入到 hdfs，然后再做 merge</p>
<h3 id="–where"><a href="#–where" class="headerlink" title="–where"></a>–where</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table oms_order \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">-m 2   \</span><br><span class="line">--null-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--null-non-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--<span class="built_in">where</span> <span class="string">&quot;delivery_company is null&quot;</span> \	<span class="comment"># filter condition</span></span><br><span class="line">--hive-overwrite</span><br></pre></td></tr></table></figure>

<h3 id="–query"><a href="#–query" class="headerlink" title="–query"></a>–query</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.33.2:3306/doit_mall \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir  /tmp/query  \		<span class="comment"># sqoop 导入数据到 hive，本质就是先将数据导入到 hdfs，然后再去 hive 数据库创建元数据。这里需要手动指定中间临时目标目录（不太清楚为啥）</span></span><br><span class="line">--hive-import \</span><br><span class="line">--hive-table doit12.oms_order \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-textfile \</span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span> \</span><br><span class="line">--compress   \</span><br><span class="line">--compression-codec gzip \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--null-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--null-non-string <span class="string">&#x27;\\N&#x27;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--query <span class="string">&quot;select id,member_id,order_sn,total_amount,pay_amount,status from oms_order where status=4 and \$CONDITIONS&quot;</span>  \			<span class="comment"># 查询语句，也支持复杂查询</span></span><br><span class="line">-m 2</span><br></pre></td></tr></table></figure>

<h1 id="运行-sqoop-action"><a href="#运行-sqoop-action" class="headerlink" title="运行 sqoop action"></a>运行 sqoop action</h1><p>在数据接入时，特别是连接数据库增量导入数据时，这种周期性任务的执行，有很多种方式：</p>
<ol>
<li>写一个 long running 脚本，不断执行增量 import</li>
<li><a href="https://www.cnblogs.com/xing901022/p/6091306.html">采用 Oozie 等调度工具来运行</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/10/MPP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/10/MPP/" class="post-title-link" itemprop="url">MPP (Massively Parallel Processing)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-10 10:12:56" itemprop="dateCreated datePublished" datetime="2020-11-10T10:12:56+08:00">2020-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://zhuanlan.zhihu.com/p/148621151">5分钟了解MPP数据库</a></p>
<p>MPP (Massively Parallel Processing)，即大规模并行处理。简单来说，MPP是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果(与Hadoop相似，但主要针对大规模关系型数据的分析计算)。</p>
<h2 id="MPP架构特征"><a href="#MPP架构特征" class="headerlink" title="MPP架构特征"></a>MPP架构特征</h2><ul>
<li>任务并行执行;</li>
<li>数据分布式存储(本地化);</li>
<li>分布式计算;</li>
<li>私有资源;</li>
<li>横向扩展;</li>
<li><a href="#share-nothing">Shared Nothing</a>架构。</li>
</ul>
<h1 id="MPPDB-v-s-Hadoop"><a href="#MPPDB-v-s-Hadoop" class="headerlink" title="MPPDB v.s. Hadoop"></a>MPPDB v.s. Hadoop</h1><p><a href="https://www.zhihu.com/question/22799482/answer/81615602">知乎-为什么说HADOOP扩展性优于MPP架构的关系型数据库？</a></p>
<p>hadoop 和 MPPDB <strong>最大的区别在于：对数据管理理念的不同。</strong> </p>
<ol>
<li>HDFS&#x2F;Hadoop 对于数据管理是<strong>粗放型管理</strong>，以一个文件系统的模式，让用户根据文件夹层级，把文件直接塞到池子里。处理也<strong>以批处理为主，就是拼命 scan</strong>。如果想在一大堆数据里找符合条件的数据，hadoop 就是粗暴的把所有文件从头到尾 scan 一遍，因为对于这些文件他没有索引、分类等，他管的少，知道的也少，用的时候每次就要全 scan。</li>
<li>数据库的本质在于数据管理，对外提供在线访问、增删改查等一系列操作。数据库的<strong>内存管理比较精细</strong>，有一套很完善的数据管理和分布体系。如果想在一大堆数据里找符合条件的数据，他可以根据分区信息先落到某个机器，再根据多维分区落到某个文件，再在文件里通过索引数据页的树形结构查询，可以直接定位到记录。</li>
<li>因为这样的基本理念不同，使得 hadoop 的扩展只需要简单的增加机器，内部平衡和迁移 data block；而数据库的扩充则涉及到数据拓扑结构的变更、或者不同机器间数据的迁移，当变化迁移的时候，依然需要维护分区、索引等，这种复杂度就比粗放的 HDFS 要高很多了。目前两者的存储模型不同。hadoop 用的是 HDFS，而 MPP 需要自己切分扩展。HDFS 扩展是通过元数据做的，name node 存元数据，增加一个节点，修改元数据就行，所以 HDFS 的扩展能力受到管理元数据的机器（name node） 的性能限制，一般来说可以到 10k 的规模。但 MPP 采用没有中心节点的存储模型，比如 hash，每次增加节点，都需要 rehash，规模增加到几百台的时候，扩展能力就有下降下来了。</li>
</ol>
<p>通常来讲，MPP数据库有对SQL的完整兼容和一些事务的处理能力，Hadoop在处理非结构化和半结构化数据上具备优势，所以MPP适合多维度数据自助分析、数据集市等；Hadoop适合海量数据存储查询、批量数据ETL、非结构化数据分析(日志分析、文本分析)等海量<strong>数据批处理</strong>应用要求。但通过 sql 还是 map-reduce 来查，其实只是一种查询形式。目前也有很多 sql on hadoop 的方案，例如 impala （sql on hadoop，其实是一个 MPP engine，所以它的查询性能会更好，提供更低的延迟和更少的处理时间）、spark Sql 等。现在Spark的重点都在Spark SQL，因为它已经不仅仅是SQL了，而是新的 “spark core”。（详见最后链接中Reynold Xin对此的解释）</p>
<blockquote>
<p>Spark SQL is not just about SQL. It turns out the primitives required for general data processing (eg ETL) are not that different from the relational operators, and that is what Spark SQL is. Spark SQL is the new Spark core with the Catalyst optimizer and the Tungsten execution engine, which powers the DataFrame, Dataset, and last but not least SQL.</p>
</blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-2195887a063e35952ffb9c94d3e18755_720w.jpg" alt="img"></p>
<h1 id="常用的MPP数据库有哪些"><a href="#常用的MPP数据库有哪些" class="headerlink" title="常用的MPP数据库有哪些"></a>常用的MPP数据库有哪些</h1><ul>
<li>自我管理的数据仓库<ul>
<li>HPE vertica</li>
<li>MemSql</li>
<li>Teradata</li>
</ul>
</li>
<li>按需 MPP 数据库<ul>
<li>aws redshift</li>
<li>azure sql 数据仓库</li>
<li>google bigQuery</li>
</ul>
</li>
<li><strong>GreenPlum</strong></li>
<li>Sybase IQ</li>
<li>TD Aster Data</li>
</ul>
<h1 id="Share-Nothing"><a href="#Share-Nothing" class="headerlink" title="Share Nothing"></a>Share Nothing<a name="share-nothing" /></h1><p><a href="https://www.cnblogs.com/kzwrcom/p/6397709.html">数据库架构设计的三种模式：share nothing , share everythong , share disk</a></p>
<p>数据库构架设计中主要有Shared Everthting、Shared Nothing、和Shared Disk：</p>
<ol>
<li><p>Shared Everthting:一般是针对单个主机，完全透明共享CPU&#x2F;MEMORY&#x2F;IO，并行处理能力是最差的，典型的代表SQLServer</p>
</li>
<li><p>Shared Disk：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。</p>
</li>
<li><p>Shared Nothing：各个处理单元都有自己私有的CPU&#x2F;内存&#x2F;硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表DB2 DPF和Hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。<br>我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。</p>
</li>
</ol>
<h1 id="GreenPlum"><a href="#GreenPlum" class="headerlink" title="GreenPlum"></a>GreenPlum</h1><p><a href="https://gpdb.docs.pivotal.io/5280/admin_guide/intro/arch_overview.html#arch_segments">greenplum</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/11/09/data-lake/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/09/data-lake/" class="post-title-link" itemprop="url">data lake</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-09 13:54:11" itemprop="dateCreated datePublished" datetime="2020-11-09T13:54:11+08:00">2020-11-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://zhuanlan.zhihu.com/p/91165577">数据湖</a></p>
<p>数据湖是：</p>
<ol>
<li>装有一些便于提取、分析、搜索、挖掘的设备（本身不具备分析能力，是其他分析工具可以方便的在湖上运行，而不需要把湖的数据挪出去再分析）</li>
<li>存放各种数据（格式不统一，原始数据）：结构、半结构、非结构化</li>
<li>来源各种各样，能很方便的导入到数据湖</li>
</ol>
<p><strong>数据湖就是原始数据保存区</strong>. 虽然这个概念国内谈的少，但<strong>绝大部分互联网公司都已经有了</strong>。国内一般把整个HDFS叫做数据仓库（广义），<strong>即存放所有数据的地方</strong>，而国外一般叫数据湖（data lake）。把需要的数据导入到数据湖，如果你想结合来自数据湖的信息和客户关系管理系统（CRM）里面的信息，我们就进行连接，<strong>只有需要时才执行这番数据结合</strong>。</p>
<p>数据湖是多结构数据的系统或存储库，它们以原始格式和模式存储，通常作为对象“blob”或文件存储。数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。数据湖中的数据包括结构化数据（关系数据库数据），半结构化数据（CSV、XML、JSON等），非结构化数据（电子邮件，文档，PDF）和二进制数据（图像、音频、视频），从而形成一个容纳所有形式数据的集中式数据存储。</p>
<p>数据湖从本质上来讲，是一种企业数据架构方法，物理实现上则是一个数据存储平台，用来集中化存储企业内海量的、多来源，多种类的数据，<strong>并支持对数据进行快速加工和分析</strong> (支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统）。从实现方式来看，目前Hadoop是最常用的部署数据湖的技术，但并不意味着数据湖就是指Hadoop集群。为了应对不同业务需求的特点，MPP数据库+Hadoop集群+传统数据仓库这种“混搭”架构的数据湖也越来越多出现在企业信息化建设规划中。</p>
<blockquote>
<p><code>Data Lake</code>： 数据湖<br><code>Data Swamp</code>： 数据沼泽<br><code>Data Mart</code>： 数据集市<br><code>Data Warehouse</code>： 数据仓库<br><code>Data Cube</code>：数据立方体<br><code>Data Stream</code>：数据流<br><code>Data Virtualization</code>：数据虚拟化</p>
</blockquote>
<h2 id="错误认知"><a href="#错误认知" class="headerlink" title="错误认知"></a>错误认知</h2><ul>
<li>错误认知1： 数据湖仅用于“存储”数据<ul>
<li>支持对数据进行快速加工和分析。支持直接在数据湖上运行分析，而无需将数据移至单独的分析系统</li>
</ul>
</li>
<li>错误认知2：数据湖仅存储“原始”数据<ul>
<li>需要有定义的机制来编目和保护数据。这些元素并非原始数据，而是对数据湖的管理数据。</li>
</ul>
</li>
</ul>
<h2 id="数据和和分析解决方案的基本要素"><a href="#数据和和分析解决方案的基本要素" class="headerlink" title="数据和和分析解决方案的基本要素"></a>数据和和分析解决方案的基本要素</h2><p>组织构建数据湖和分析平台时，他们需要考虑许多关键功能，包括：</p>
<h4 id="数据移动（支持大规模的数据以原始形式导入）"><a href="#数据移动（支持大规模的数据以原始形式导入）" class="headerlink" title="数据移动（支持大规模的数据以原始形式导入）"></a>数据移动（支持大规模的数据以原始形式导入）</h4><p>数据湖允许您导入任何数量的实时获得的数据。您可以从多个来源收集数据，并以其原始形式将其移入到数据湖中。此过程允许您扩展到任何规模的数据，同时节省定义数据结构、Schema 和转换的时间。</p>
<h4 id="安全地存储和编目数据（编目使得数据是被监督的，可用的）"><a href="#安全地存储和编目数据（编目使得数据是被监督的，可用的）" class="headerlink" title="安全地存储和编目数据（编目使得数据是被监督的，可用的）"></a>安全地存储和编目数据（编目使得数据是被监督的，可用的）</h4><p>数据湖允许您存储关系数据（例如，来自业务线应用程序的运营数据库和数据）和非关系数据（例如，来自移动应用程序、IoT 设备和社交媒体的运营数据库和数据）。它们还使您能够通过<strong>对数据进行爬网、编目和建立索引</strong>来了解湖中的数据。最后，必须保护数据以确保您的数据资产受到保护。</p>
<blockquote>
<p>是数据湖里的数据本身有索引吗？还是基于数据湖做catalog、元数据管理等？catalog 即是对数据湖数据的索引？？？</p>
</blockquote>
<h4 id="分析（可以直接在数据湖上，运行快速加工和分析）"><a href="#分析（可以直接在数据湖上，运行快速加工和分析）" class="headerlink" title="分析（可以直接在数据湖上，运行快速加工和分析）"></a>分析（可以直接在数据湖上，运行快速加工和分析）</h4><p>数据湖允许组织中的各种角色（如数据科学家、数据开发人员和业务分析师）通过各自选择的分析工具和框架来访问数据。这包括 Apache Hadoop、Presto 和 Apache Spark 等开源框架，以及数据仓库和商业智能供应商提供的商业产品。<strong>数据湖允许您运行分析，而无需将数据移至单独的分析系统</strong>。</p>
<h4 id="机器学习（在数据湖上进行机器学习）"><a href="#机器学习（在数据湖上进行机器学习）" class="headerlink" title="机器学习（在数据湖上进行机器学习）"></a>机器学习（在数据湖上进行机器学习）</h4><p>数据湖将允许组织生成不同类型的见解，包括报告历史数据以及进行机器学习（构建模型以预测可能的结果），并建议一系列规定的行动以实现最佳结果。</p>
<h1 id="Data-swamp"><a href="#Data-swamp" class="headerlink" title="Data swamp"></a>Data swamp</h1><p>搭建数据湖容易，但是让数据湖发挥价值是很难。如果只是一直往里面灌数据，而应用场景极少，没有输出或者极少输出，形成<strong>单向湖</strong>。</p>
<p>企业的业务是实时在变化的，这代表着沉积在数据湖中的数据定义、数据格式实时都在发生的转变，企业的大型数据湖对企业数据治理（Data Governance）提升了更高的要求。大部分使用数据湖的企业在数据真的需要使用的时候，往往因为数据湖中的数据质量太差而无法最终使用。数据湖，被企业当成一个大数据的垃圾桶，最终数据湖成为臭气熏天，存储在Hadoop当中的数据成为无人可以清理的数据沼泽.</p>
<p>数据湖架构的主要挑战是存储原始数据而不监督内容。对于使数据可用的数据湖，它需要有定义的机制来编目和保护数据。没有这些元素，就无法找到或信任数据，从而导致出现“<a href="https://www.gartner.com/newsroom/id/2809117">数据沼泽</a>”。 满足更广泛受众的需求需要数据湖具有管理、语义一致性和访问控制。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7f3ec7c14c46b4e7328f74e44529ad21_720w.jpg" alt="img"></p>
<h1 id="Data-Lake-v-s-Data-Warehouse"><a href="#Data-Lake-v-s-Data-Warehouse" class="headerlink" title="Data Lake v.s. Data Warehouse"></a>Data Lake v.s. Data Warehouse</h1><p><a href="https://aws.amazon.com/cn/big-data/datalakes-and-analytics/what-is-a-data-lake/">aws 什么是数据湖</a></p>
<p><a href="https://dbaplus.cn/news-141-2924-1.html">数据仓库、数据湖 -&gt; 数据中台</a></p>
<p>数据仓库里的数据都满足特定的 schema，而数据湖则没有。仓库里的数据是简单整理过的，湖里的则是原始的（但不全是原始的）。仓库里的来源也都是规范的关系数据，而湖里则什么都有。</p>
<p>数据仓库是一个优化的数据库，用于分析来自事务系统和业务线应用程序的关系数据。事先定义数据结构和 Schema 以优化快速 SQL 查询，其中结果通常用于操作报告和分析。数据经过了清理、丰富和转换，因此可以充当用户可信任的“单一信息源”。</p>
<p>数据湖有所不同，因为它存储来自业务线应用程序的关系数据，以及来自移动应用程序、IoT 设备和社交媒体的非关系数据。捕获数据时，未定义数据结构或 Schema。这意味着您可以存储所有数据，而不需要精心设计也无需知道将来您可能需要哪些问题的答案。您可以对数据使用不同类型的分析（如 SQL 查询、大数据分析、全文搜索、实时分析和机器学习）来获得见解。</p>
<p>随着使用数据仓库的组织看到数据湖的优势，他们正在改进其仓库以包括数据湖，并启用各种查询功能、数据科学使用案例和用于发现新信息模型的高级功能。Gartner 将此演变称为“分析型数据管理解决方案”或“<a href="https://www.gartner.com/doc/3614317/magic-quadrant-data-management-solutions">DMSA</a>”。</p>
<table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">数据仓库</th>
<th align="center">数据湖</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>数据</strong></td>
<td align="center">来自事务系统、运营数据库和业务线应用程序的<strong>关系数据</strong></td>
<td align="center">来自 IoT 设备、网站、移动应用程序、社交媒体和企业应用程序的<strong>非关系和关系数据</strong></td>
</tr>
<tr>
<td align="center"><strong>Schema</strong></td>
<td align="center">设计在数据仓库实施之前（写入型 Schema）</td>
<td align="center">写入在分析时（读取型 Schema）</td>
</tr>
<tr>
<td align="center"><strong>性价比</strong></td>
<td align="center">更快查询结果会带来较高存储成本</td>
<td align="center">更快查询结果只需较低存储成本</td>
</tr>
<tr>
<td align="center">**数据质量 **</td>
<td align="center">可作为重要事实依据的高度监管数据</td>
<td align="center">任何可以或无法进行监管的数据（例如原始数据）</td>
</tr>
<tr>
<td align="center"><strong>用户</strong></td>
<td align="center">业务分析师</td>
<td align="center">数据科学家、数据开发人员和业务分析师（使用监管数据）</td>
</tr>
<tr>
<td align="center"><strong>分析</strong></td>
<td align="center">批处理报告、BI 和可视化</td>
<td align="center">机器学习、预测分析、数据发现和分析</td>
</tr>
</tbody></table>
<p><img src="https://pic2.zhimg.com/80/v2-9eb32acb557b22322749f775c17b0079_720w.jpg" alt="img"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/10/10/az-data-engineer-certificate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/10/az-data-engineer-certificate/" class="post-title-link" itemprop="url">az data engineer certificate</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-10-10 17:03:32" itemprop="dateCreated datePublished" datetime="2020-10-10T17:03:32+08:00">2020-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-19 09:46:54" itemprop="dateModified" datetime="2023-05-19T09:46:54+08:00">2023-05-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://docs.microsoft.com/zh-cn/learn/certifications/azure-data-engineer?tab=tab-learning-paths">learning paths</a></p>
<h2 id="On-premises-Env-vs-Cloud"><a href="#On-premises-Env-vs-Cloud" class="headerlink" title="On-premises Env vs Cloud"></a>On-premises Env vs Cloud</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/evolving-world-of-data/3-systems-on-premise-vs-cloud">link</a></p>
<p>The term <em>total cost of ownership</em> (TCO) describes the final cost of owning a given technology. In <strong>on-premises systems</strong>, TCO includes the following costs:</p>
<ul>
<li>Hardware</li>
<li>Software licensing</li>
<li>Labor (installation, upgrades, maintenance)</li>
<li>Datacenter overhead (power, telecommunications, building, heating and cooling)</li>
</ul>
<p><strong>Cloud systems</strong> like Azure track costs by subscriptions. A subscription can be based on usage that’s measured in compute units, hours, or transactions. The cost includes hardware, software, disk storage, and labor. Because of economies of scale, an on-premises system can rarely compete with the cloud in terms of the measurement of the service usage.</p>
<p>The cost of operating an on-premises server system rarely aligns with the actual usage of the system. In cloud systems, the cost usually aligns more closely with the actual usage.</p>
<h4 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h4><blockquote>
<p>However, many companies use the cloud still in a wasting way. The cost in cloud systems in fact rarely aligns with the actual usage, too.</p>
<p>The main advantage of Cloud is that it can <strong>be charged on usage</strong>. Thus, this advantage only works when using the cloud by need. Otherwise, the cloud advantage evaporates, especially from the aspect of cost.</p>
<p>The advantages of cloud:</p>
<ol>
<li>charge on usage</li>
<li>enjoy the high quality and compresensive services of big company.</li>
</ol>
</blockquote>
<h2 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/survey-the-azure-data-platform/2-structured-vs-non-structured">link</a></p>
<p>For nonstructured Data, the data <strong>structure is defined only when the data is read</strong>. The difference in the definition point gives you flexibility to use the same source data for different outputs.</p>
<blockquote>
<p>JSON is in fact semistructured data.</p>
</blockquote>
<p>Examples of nonstructured data include binary, audio, and image files. </p>
<p>NoSQL is in fact semistructured data. The open-source world offers four types of NoSQL databases:</p>
<ol>
<li><strong>Key-value store</strong>: Stores key-value pairs of data in a table structure.</li>
<li><strong>Document database</strong>: Stores documents that are <strong>tagged with metadata</strong> to aid document searches.</li>
<li><strong>Graph database</strong>: Finds relationships between data points by using a structure that’s composed of vertices and edges.</li>
<li><strong>Column database</strong>: Stores data based on columns rather than rows. Columns can be defined at the query’s runtime, allowing flexibility in the data that’s returned performantly.</li>
</ol>
<h3 id="Azure-Storage"><a href="#Azure-Storage" class="headerlink" title="Azure Storage"></a>Azure Storage</h3><p><a href="https://docs.microsoft.com/en-us/learn/modules/survey-the-azure-data-platform/3-store-data-using-azure-store-account">Azure Storage account</a> is the base storage type. It’s mainly used to store data but with poor or no query ability.</p>
<p>Azure Storage offers four configuration options:</p>
<ul>
<li><strong>Azure Blob</strong>: A scalable object store for text and binary data. The cheapst choice to store bot not query data.</li>
<li><strong>Azure Files</strong>: Managed file shares for cloud or on-premises deployments</li>
<li><strong>Azure Queue</strong>: A messaging store for reliable messaging between application components</li>
<li><strong>Azure Table</strong>: A NoSQL store for no-schema storage of structured data</li>
</ul>
<h2 id="Tasks-of-an-Azure-data-engineer"><a href="#Tasks-of-an-Azure-data-engineer" class="headerlink" title="Tasks of an Azure data engineer:"></a>Tasks of an Azure data engineer:</h2><p> <a href="https://docs.microsoft.com/en-us/learn/modules/data-engineering-processes/3-data-engineering-practices">link</a></p>
<p><a href="https://docs.microsoft.com/zh-cn/learn/modules/data-engineering-processes/4-architecturing-project">example</a></p>
<p><a href="https://docs.microsoft.com/en-us/learn/modules/data-engineering-processes/2-roles-and-responsibilities">Data Engineer vs Data Scientist vs AI engineer</a>: Data engineer decide how to organized the data and pre-process the data. Data scientist use the result of Data engineer to create analysis model, and extract value. AI engineer use the existed model &amp; tools to process the data. AI engineer may need the help of Data engineer to store the result, and the help of Data analysis to generate new model.</p>
<p>Here are some of the tasks of an Azure data engineer:</p>
<ul>
<li>Design and develop data storage and data processing solutions for the enterprise.</li>
<li>Set up and deploy cloud-based data services such as blob services, databases, and analytics.</li>
<li>Secure the platform and the stored data. Make sure only the necessary users can access the data.</li>
<li>Ensure business continuity in uncommon conditions by using techniques for high availability and disaster recovery.</li>
<li>Monitor to ensure that the systems run properly and are cost-effective.</li>
</ul>
<h2 id="Plan-the-data-storage-solution"><a href="#Plan-the-data-storage-solution" class="headerlink" title="Plan the data storage solution"></a>Plan the data storage solution</h2><p><a href="https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/3-operations-and-latency">Determine operational needs</a></p>
<p>What are the main operations you’ll be completing on each data type, and what are the performance requirements?</p>
<p>Ask yourself these questions:</p>
<ul>
<li>Will you be doing simple lookups using an ID?</li>
<li>Do you need to query the database for one or more fields?</li>
<li>How many create, update, and delete operations do you expect?</li>
<li>Do you need to run complex analytical queries?</li>
<li>How quickly do these operations need to complete?</li>
</ul>
<p><a href="https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/5-choose-the-right-azure-service-for-your-data">a storage solution example on the e-commerce system</a>: </p>
<ul>
<li><p>Product catalog data: cosmosDB</p>
<ul>
<li><p><strong>Data classification:</strong> Semi-structured because of the need to extend or modify the schema for new products</p>
<p><strong>Operations:</strong></p>
<ul>
<li>Customers require a high number of read operations, with the ability to query on many fields within the database.</li>
<li>The business requires a high number of write operations to track the constantly changing inventory.</li>
</ul>
<p><strong>Latency &amp; throughput:</strong> High throughput and low latency</p>
<p><strong>Transactional support:</strong> Required</p>
</li>
</ul>
</li>
<li><p>Photos &amp; videos: azure blob (with azure CDN)</p>
<ul>
<li><p><strong>Data classification:</strong> Unstructured</p>
<p><strong>Operations:</strong></p>
<ul>
<li>Only need to be retrieved by ID.</li>
<li>Customers require a high number of read operations with low latency.</li>
<li>Creates and updates will be somewhat infrequent and can have higher latency than read operations.</li>
</ul>
<p><strong>Latency &amp; throughput:</strong> Retrievals by ID need to support low latency and high throughput. Creates and updates can have higher latency than read operations.</p>
<p><strong>Transactional support:</strong> Not required</p>
</li>
</ul>
</li>
<li><p>Buisiness Data: azure sql (with azure analysis services)</p>
<ul>
<li><p><strong>Data classification:</strong> Structured</p>
<p><strong>Operations:</strong> Read-only, complex analytical queries across multiple databases</p>
<p><strong>Latency &amp; throughput:</strong> Some latency in the results is expected based on the complex nature of the queries.</p>
<p><strong>Transactional support:</strong> Required</p>
</li>
</ul>
</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><h2 id="Private-cloud-vs-Public-Cloud-vs-Specific-Cloud"><a href="#Private-cloud-vs-Public-Cloud-vs-Specific-Cloud" class="headerlink" title="Private cloud vs Public Cloud vs Specific Cloud"></a>Private cloud vs Public Cloud vs Specific Cloud</h2><ul>
<li><p><input checked="" disabled="" type="checkbox"> 
Are there still private cloud and public cloud??    &#x3D;&#x3D;&#x3D;&gt; yes</p>
</li>
<li><p><input disabled="" type="checkbox"> 
What’s the difference? The private cloud will have higher quality cloud? The so-called SLA of public cloud is in fact not assured??</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
Maybe there’re also specific cloud, like financial cloud (maybe more secure and fast) &#x3D;&#x3D;&#x3D;&gt; yes</p>
</li>
</ul>
<h4 id="Points"><a href="#Points" class="headerlink" title="Points:"></a>Points:</h4><blockquote>
<ol>
<li>There is specific cloud. Mayi has financial cloud.</li>
<li>There are both public cloud &amp; private cloud, but they’re closely related. That is, they use the same cloud technology (images), but with different clusters &amp; differnt tariff.</li>
</ol>
</blockquote>
<h2 id="Azure-storage-account"><a href="#Azure-storage-account" class="headerlink" title="Azure storage account"></a>Azure storage account</h2><ol>
<li>how to configure to use different type?</li>
<li>Why they’re all in storage account instead of as independant service??</li>
<li>azure blob vs azure file storage</li>
</ol>
<h2 id="Column-database"><a href="#Column-database" class="headerlink" title="Column database"></a>Column database</h2><ol>
<li>How the data is organized in the hardware?</li>
<li>How should we save the data into column database?</li>
<li>Wide Column Stores</li>
<li>BigTable</li>
</ol>
<p>Cosmosdb(Cassendra) vs HBase: infrastructure, load-balance???</p>
<p>Cassendra: Wide Column Stores</p>
<h2 id="ELT-vs-ELTL"><a href="#ELT-vs-ELTL" class="headerlink" title="ELT vs ELTL"></a>ELT vs ELTL</h2><ul>
<li><input disabled="" type="checkbox"> In fact, often ELTL???</li>
<li><input disabled="" type="checkbox"> How is the data stored in T(transform)???</li>
</ul>
<h2 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h2><ul>
<li><input disabled="" type="checkbox"> what’s markup language???</li>
<li><input disabled="" type="checkbox"> why yaml is not markup language but json is???</li>
</ul>
<h2 id="Azure-SQL"><a href="#Azure-SQL" class="headerlink" title="Azure SQL"></a>Azure SQL</h2><ul>
<li><input disabled="" type="checkbox"> how azure sql support queries across multiple databases??</li>
<li><input disabled="" type="checkbox"> Why does azure sql data warehouse not support???</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/07/23/network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/23/network/" class="post-title-link" itemprop="url">network</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-07-23 14:10:25" itemprop="dateCreated datePublished" datetime="2020-07-23T14:10:25+08:00">2020-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-05 11:04:28" itemprop="dateModified" datetime="2023-05-05T11:04:28+08:00">2023-05-05</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://www.youtube.com/watch?v=rL8RSFQG8do&list=PLF360ED1082F6F2A5">Eli the computer guy</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>the whole picture </p>
<p>Speed &amp; storage unit</p>
<p>Physical &amp; logical</p>
<p>modem</p>
<ul>
<li>t1</li>
<li>dsl:<ul>
<li>no faster than 12Mb&#x2F;s</li>
<li>Asynchronous: download faster than upload</li>
</ul>
</li>
<li>cabel</li>
<li>satellite</li>
</ul>
<p>Router</p>
<p>firewall</p>
<ul>
<li>block the internet to get into your network</li>
</ul>
<p>VPN</p>
<ul>
<li>enable the internet to get into your network</li>
<li>client-server</li>
</ul>
<p>Switch</p>
<ul>
<li>Connect everything together</li>
</ul>
<h1 id="TCP-x2F-IP"><a href="#TCP-x2F-IP" class="headerlink" title="TCP&#x2F;IP"></a>TCP&#x2F;IP</h1><p>ip: internet protocol. How to find a computer.</p>
<p>tcp: transmission control protocol. How to communicate between 2 computers</p>
<p>suite: tcp protocol, ip protocol, etc</p>
<p>tcp windowing</p>
<p>components:</p>
<ul>
<li><p>ip address</p>
</li>
<li><p>subnet</p>
</li>
<li><p>default gateway</p>
</li>
<li><p>dns</p>
</li>
<li><p>dhcp</p>
</li>
<li><p>nat</p>
</li>
</ul>
<p>subnet mask: network identifier, device identifier.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> windows, first release and renew to ensure to get the latest configurations</span></span><br><span class="line">ipconfig /release</span><br><span class="line">ipconfig /renew</span><br><span class="line">ipconfig /all</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> linux and mac</span></span><br><span class="line">ifconfig [-a]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ping 10.1.10.11 10 <span class="built_in">times</span> with 60ms ttl</span></span><br><span class="line">ping www.baidu.com -c 10 -m 60</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">in</span> windows, it<span class="string">&#x27;s tracert. For linux and mac, it&#x27;</span>s traceroute</span></span><br><span class="line">traceroute www.baidu.com</span><br></pre></td></tr></table></figure>

<h2 id="OpenDNS-for-network-security"><a href="#OpenDNS-for-network-security" class="headerlink" title="OpenDNS for network security"></a>OpenDNS for network security</h2><p>dns can be hacked so that you’re redirected to some unwanted websites. Or a virus on your computer tries to pull more viruses  from some virus website. </p>
<p>OpenDNS is a product to prevent such nasty issues.</p>
<ol>
<li><p>With a router, and dhcp on that router (allocating ip&#x2F;subset mask,etc to all your network devices), you config the dns as the opendns</p>
</li>
<li><p>On opendns control panel, add your network (the external ip of your network, i.e. the router ip address) and config the filter for your network.</p>
</li>
<li><p>When accessing unallowed websites, you will be redirected to a block opendns page by opendns.</p>
</li>
</ol>
<p>If you don’t have static ip, install the dynamic ip address from opendns, which will tell the opendns your current network address.</p>
<h1 id="Servers"><a href="#Servers" class="headerlink" title="Servers"></a>Servers</h1><p>server operating system: more robust and expensive, compared to desktop operating system.</p>
<p>specific hardware: for data center. xeon processor, redundant power supply, raid (redundant hard drive).</p>
<h1 id="Voice-over-ip-VOIP"><a href="#Voice-over-ip-VOIP" class="headerlink" title="Voice over ip (VOIP)"></a>Voice over ip (VOIP)</h1><p>At the beginning, telephone system communicate using wires. It’s completely separate to comupter system.</p>
<p>VOIP make the audio transimit through the TCP&#x2F;IP protocol.</p>
<p>Client-Server infrastructure: hard phones &#x2F; soft phones have to install the VOIP client to communicate to VOIP server, which routes all audio communications.</p>
<p>Iphone in fact installs VOIP client and phone through VOIP service. IPhone is the above soft phones.</p>
<p>Gateways. It can connect different communication system, e.g. the old telephone system and VOIP system, so that you can call through VOIP server -&gt; Gateway -&gt; the wireline telephone.</p>
<p>Codec. It encode the audio transmitted on VOIP service and decide what the packet should be like. So it decides the quality and bandwidth it needs.</p>
<p>QOS. quality of service. On switches and routers, config the VOIP transmision as high priority, so that it won’t be influenced by other bandwidth sharer.</p>
<p>Unifed communication. I think it’s the whole idea that make the telephone and computer stay in the same system, so that we can do a lot by this.</p>
<h1 id="Cloud-Computing"><a href="#Cloud-Computing" class="headerlink" title="Cloud Computing"></a>Cloud Computing</h1><p>Web application. If we want some application, but it’s in fact not on our own hardware, and it’s outside somewhere, then it’s in fact cloud computing. It’s just maybe it’s private cloud.</p>
<p>Cluster. And to be a cloud, there also needs to be a cluster, so that when some bad things happen, it can recover itself.</p>
<p>Cluster make the services more robust. It’s a normal way for cloud to provide robustness.</p>
<p>Terminal Services. There’s terminal service server and thin clients. You can access a remote operating system desktop through thin clients. This seems to be the remote connection. Furthermore, the remote system can just be an specific application instead of a desktop. In such case, there’s application server.</p>
<p>This is an old technique. In old days, it’s called mainframe and dumb terminals. Anyway, the thin clients capture all the strokes you made and send them to the terminal service server, and the terminal service server sends the result image back. The teminal service server is the one with processing abilities, and all thin clients share the cpu time, each allocated with one slice of CPU time.</p>
<p>Terminal service is also a cloud related technique to access cloud services.</p>
<p>Virtualization. A tenichque that separate the operating system from the hardware, so that you can transfer the operating system with the applications in it easily. There’re 2 flavours to implement it:</p>
<ol>
<li><p>client installer. This is the normal virtualbox or vmware fusion. You installed this installer on your current operating system, and then you installed another os box using the installer.</p>
</li>
<li><p>hypervision. Hypervision is in fact an operating system. Install hypervision on hardware, and then using the management client on your computer to install anything you want on that hypervision. The hypervision will then allocate a piece of hard drive, ram, etc to the os you asked. In the vmware world, the hypervision is esxi which is always free, the management software is the vsphere which is in charged.</p>
</li>
</ol>
<p>Current cloud computing should be using some techniques like the hypervision. Anyway, they try to separate the os from the hardware, so that you can copy-paste the whole environment easily to another hardware like a file.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2020/06/28/Cache-Memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/28/Cache-Memory/" class="post-title-link" itemprop="url">Cache Memory</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-06-28 15:29:22" itemprop="dateCreated datePublished" datetime="2020-06-28T15:29:22+08:00">2020-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-10 10:46:52" itemprop="dateModified" datetime="2023-05-10T10:46:52+08:00">2023-05-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="General-Concept"><a href="#General-Concept" class="headerlink" title="General Concept"></a>General Concept</h1><p><img src="/images/cache-memory-simple.003.jpeg" alt="cache-memory-simple.003"></p>
<p><img src="/images/cache-memory-simple.004.jpeg" alt="cache-memory-simple.004"></p>
<p><img src="/images/cache-memory-simple.005.jpeg" alt="cache-memory-simple.005"></p>
<p><img src="/images/cache-memory-simple.006.jpeg" alt="cache-memory-simple.006"></p>
<h1 id="CPU-Core-Caching"><a href="#CPU-Core-Caching" class="headerlink" title="CPU Core Caching"></a>CPU Core Caching</h1><p><img src="/images/cache-memory-simple.008.jpeg" alt="cache-memory-simple.008"></p>
<p><img src="/images/cache-memory-simple.009.jpeg" alt="cache-memory-simple.009"></p>
<p><img src="/images/cache-memory-simple.010.jpeg" alt="cache-memory-simple.010"></p>
<p><img src="/images/cache-memory-simple.011.jpeg" alt="cache-memory-simple.011"></p>
<p><img src="/images/cache-memory-simple.012.jpeg" alt="cache-memory-simple.012"></p>
<p><img src="/images/cache-memory-simple.013.jpeg" alt="cache-memory-simple.013"></p>
<h1 id="Cache-Lines"><a href="#Cache-Lines" class="headerlink" title="Cache Lines"></a>Cache Lines</h1><p><img src="/images/cache-memory-simple.015.jpeg" alt="cache-memory-simple.015"></p>
<p><img src="/images/cache-memory-simple.016.jpeg" alt="cache-memory-simple.016"></p>
<h1 id="Cache-Memory"><a href="#Cache-Memory" class="headerlink" title="Cache Memory"></a>Cache Memory</h1><h2 id="Associative-Memory"><a href="#Associative-Memory" class="headerlink" title="Associative Memory"></a>Associative Memory</h2><p><img src="/images/cache-memory-simple.019.jpeg" alt="cache-memory-simple.019"></p>
<p><img src="/images/cache-memory-simple.020.jpeg" alt="cache-memory-simple.020"></p>
<p><img src="/images/cache-memory-simple.021.jpeg" alt="cache-memory-simple.021"></p>
<p><img src="/images/cache-memory-simple.022.jpeg" alt="cache-memory-simple.022"></p>
<h2 id="Direct-Mapped-Memory"><a href="#Direct-Mapped-Memory" class="headerlink" title="Direct-Mapped Memory"></a>Direct-Mapped Memory</h2><p><img src="/images/cache-memory-simple.024.jpeg" alt="cache-memory-simple.024"></p>
<p><img src="/images/cache-memory-simple.025.jpeg" alt="cache-memory-simple.025"></p>
<p><img src="/images/cache-memory-simple.026.jpeg" alt="cache-memory-simple.026"></p>
<p><img src="/images/cache-memory-simple.027.jpeg" alt="cache-memory-simple.027"></p>
<h2 id="Set-Associative-Memory"><a href="#Set-Associative-Memory" class="headerlink" title="Set Associative Memory"></a>Set Associative Memory</h2><p><img src="/images/cache-memory-simple.029.jpeg" alt="cache-memory-simple.029"></p>
<p><img src="/images/cache-memory-simple.030.jpeg" alt="cache-memory-simple.030"></p>
<p><img src="/images/cache-memory-simple.031.jpeg" alt="cache-memory-simple.031"></p>
<h1 id="Cache-Read-x2F-Write-Policies"><a href="#Cache-Read-x2F-Write-Policies" class="headerlink" title="Cache Read&#x2F;Write Policies"></a>Cache Read&#x2F;Write Policies</h1><p><img src="/images/cache-memory-simple.033.jpeg" alt="cache-memory-simple.033"></p>
<p><img src="/images/cache-memory-simple.034.jpeg" alt="cache-memory-simple.034"></p>
<p><img src="/images/cache-memory-simple.035.jpeg" alt="cache-memory-simple.035"></p>
<p><img src="/images/cache-memory-simple.036.jpeg" alt="cache-memory-simple.036"></p>
<p><a href="https://www.infoq.cn/article/cache-coherency-primer">cache coherency</a></p>
<p>MESI protocol: (Modified, Exclusive, Shared, Invalid)</p>
<blockquote>
<ul>
<li><strong>I</strong>nvalid lines are cache lines that are either not present in the cache, or whose contents are known to be stale. For the purposes of caching, these are ignored. Once a cache line is invalidated, it’s as if it wasn’t in the cache in the first place.</li>
<li><strong>S</strong>hared lines are clean copies of the contents of main memory. Cache lines in the shared state can be used to serve reads but they can’t be written to. Multiple caches are allowed to have a copy of the same memory location in “shared” state at the same time, hence the name.</li>
<li><strong>E</strong>xclusive lines are also clean copies of the contents of main memory, just like the S state. The difference is that when one core holds a line in E state, no other core may hold it at the same time, hence “exclusive”. That is, the same line must be in the I state in the caches of all other cores.</li>
<li><strong>M</strong>odified lines are dirty; they have been locally modified. If a line is in the M state, it must be in the I state for all other cores, same as E. In addition, modified cache lines need to be written back to memory when they get evicted or invalidated – same as the regular dirty state in a write-back cache.</li>
</ul>
</blockquote>
<h1 id="Principle-Of-Locality"><a href="#Principle-Of-Locality" class="headerlink" title="Principle Of Locality"></a>Principle Of Locality</h1><p><img src="/images/cache-memory-simple.038.jpeg" alt="cache-memory-simple.038"></p>
<p><img src="/images/cache-memory-simple.039.jpeg" alt="cache-memory-simple.039"></p>
<p><img src="/images/cache-memory-simple.040.jpeg" alt="cache-memory-simple.040"></p>
<p><img src="/images/cache-memory-simple.041.jpeg" alt="cache-memory-simple.041"></p>
<p><img src="/images/cache-memory-simple.042.jpeg" alt="cache-memory-simple.042"></p>
<p><img src="/images/cache-memory-simple.043.jpeg" alt="cache-memory-simple.043"></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="/images/cache-memory-simple.045.jpeg" alt="cache-memory-simple.045"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cherish</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">429k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">6:30</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>
<script class="next-config" data-name="chatra" type="application/json">{"enable":true,"async":true,"id":null}</script>
<script src="/js/third-party/chat/chatra.js"></script>
<script async src="https://call.chatra.io/chatra.js"></script>






  





</body>
</html>
