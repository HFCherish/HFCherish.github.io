<!DOCTYPE html>
<html lang="Chinese">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0-rc1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hfcherish.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="从心所欲不逾矩">
<meta property="og:type" content="website">
<meta property="og:title" content="Cherish&#39;s Blog">
<meta property="og:url" content="http://hfcherish.github.io/page/4/index.html">
<meta property="og:site_name" content="Cherish&#39;s Blog">
<meta property="og:description" content="从心所欲不逾矩">
<meta property="og:locale">
<meta property="article:author" content="Cherish">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://hfcherish.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"Chinese","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Cherish's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Cherish's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cherish</p>
  <div class="site-description" itemprop="description">从心所欲不逾矩</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">68</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button animated">
    <button><i class="fa fa-comment"></i>
      Chat
    </button>
  </div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hfcherish" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hfcherish" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:pzcherishhf@gmail.com" title="E-Mail → mailto:pzcherishhf@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/03/01/obs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/01/obs/" class="post-title-link" itemprop="url">obs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-03-01 10:50:15" itemprop="dateCreated datePublished" datetime="2019-03-01T10:50:15+08:00">2019-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://support.huaweicloud.com/productdesc-obs/zh-cn_topic_0045829060.html">Huawei Obs</a> is an object storage service on cloud.</p>
<h1 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><ol>
<li>The real complete file or byte stream to save</li>
<li><strong>object name is the unique id</strong> in a bucket<ol>
<li>it’s used as <strong>part of url path</strong>. The naming restrictions are fit to url path naming restrictions.</li>
</ol>
</li>
<li>Access(based on version in fact)<ol>
<li>Object ACL:<ol>
<li>general control to object: <strong>read object, read&#x2F;write object ACL, only users in the same account</strong></li>
</ol>
</li>
<li>Object policy<ol>
<li>fine-grained control to object: <strong>fine-grained actions(put,delete…) on object, all users</strong></li>
</ol>
</li>
</ol>
</li>
<li><strong>multi-versions</strong><ol>
<li>an object can has multiple versions, each of which has an unique id.</li>
<li>Whether there’s multi-version, it’s a policy set on a bucket.</li>
</ol>
</li>
<li>directory:<ol>
<li><strong>directory is just a view</strong>. Essentially, it’s an empty object end with “&#x2F;“.</li>
<li><strong>all objects in a bucket are on the same level</strong>. There’s no multi-level directory in fact.</li>
<li>to create the directory view, you need to create an object with name ending with “&#x2F;“ explicility, eg. “sub1&#x2F;sub2&#x2F; . It will create a two-level dir in console. There’s no need to create “sub1&#x2F;“ first then “sub1&#x2F;sub2”.</li>
</ol>
</li>
<li>object actions:<ol>
<li>For writing, there’s <strong>only write&#x2F;restricted-append&#x2F;delete, no put</strong></li>
<li><strong>basically-write-once-read-many</strong></li>
</ol>
</li>
<li>upload modes:<ol>
<li>stream</li>
<li>file</li>
<li>multi-part (support breakpoint resume)</li>
<li>append</li>
</ol>
</li>
</ol>
<h2 id="Bucket"><a href="#Bucket" class="headerlink" title="Bucket"></a>Bucket</h2><ol>
<li>The place to save objects</li>
<li><strong>bucket name is the unique id</strong> for one account(a tenant).<ol>
<li>it’s used as <strong>part of domain name</strong> on url. The naming restrictions are fit to domain naming restrictions.</li>
</ol>
</li>
<li>Access<ol>
<li>Bucket ACL: <ol>
<li>general control to bucket and all objects in bucket: <strong>read&#x2F;put buckets, read&#x2F;write bucket ACL, only users in the same account</strong></li>
</ol>
</li>
<li>Bucket policy<ol>
<li>fine-grained control to specific objects in bucket: <strong>fine-grained actions on bucket or specific objects in bucket, all users</strong></li>
</ol>
</li>
</ol>
</li>
<li>storage type<ol>
<li>standard: <ol>
<li><strong>quick access &amp; high throughput</strong>. It’s used for high access requests and not so big files.</li>
</ol>
</li>
<li>warm:<ol>
<li><strong>low access.</strong></li>
</ol>
</li>
<li>cold:<ol>
<li><strong>very very low access</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="region"><a href="#region" class="headerlink" title="region"></a>region</h2><p>The region of nodes where the storage really happens.</p>
<h2 id="signature"><a href="#signature" class="headerlink" title="signature"></a>signature</h2><p>The signature to identify a user when accessing buckets&#x2F;objects.</p>
<ol>
<li>ak(access key): represent a user. one user can have multi aks. It’s kind of like an user role</li>
<li>sk(secret key): one-to-one corresponding with ak. The secret key used for RSA authentication &amp; authorization.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/30/pipeline-process-beam/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/30/pipeline-process-beam/" class="post-title-link" itemprop="url">pipeline process: beam</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-30 17:02:49" itemprop="dateCreated datePublished" datetime="2019-01-30T17:02:49+08:00">2019-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-19 09:47:46" itemprop="dateModified" datetime="2023-05-19T09:47:46+08:00">2023-05-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="What’s-beam"><a href="#What’s-beam" class="headerlink" title="What’s beam"></a>What’s beam</h1><p><a href="https://beam.apache.org/get-started/beam-overview/">beam</a> is a open-source, unified model for defining both batched &amp; streaming data-parallel processing pipelines.</p>
<ul>
<li>open-source (apache v2 license)</li>
<li>to define data-parallel processing pipelines</li>
<li>an unified model to define pipelines. The real processing is run by the underlying runner (eg. spark, apache apex, etc.). <a href="https://beam.apache.org/get-started/beam-overview/">all available runners</a></li>
<li>can process both batched  (bounded datasets) &amp; streaming (unbounded datasets) datasets</li>
</ul>
<h1 id="Use-it"><a href="#Use-it" class="headerlink" title="Use it"></a>Use it</h1><p>See the <a href="https://beam.apache.org/get-started/beam-overview/">wordcount examples</a>, <a href="https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java">wordcount src</a></p>
<p>Now we define a simple pipeline and run it.</p>
<p><code>Transform</code>, <code>Count</code> are all built-in atom operations to define the pipeline scripts.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.beam.examples;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.Pipeline;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.io.TextIO;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.options.PipelineOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.options.PipelineOptionsFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.Count;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.Filter;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.FlatMapElements;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.transforms.MapElements;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.values.KV;</span><br><span class="line"><span class="keyword">import</span> org.apache.beam.sdk.values.TypeDescriptors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MinimalWordCount</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a PipelineOptions object. This object lets us set various execution</span></span><br><span class="line">    <span class="comment">// options for our pipeline, such as the runner you wish to use.</span></span><br><span class="line">    <span class="type">PipelineOptions</span> <span class="variable">options</span> <span class="operator">=</span> PipelineOptionsFactory.create();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the Pipeline object with the options we defined above</span></span><br><span class="line">    <span class="type">Pipeline</span> <span class="variable">p</span> <span class="operator">=</span> Pipeline.create(options);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set</span></span><br><span class="line">    p.apply(TextIO.read().from(<span class="string">&quot;gs://apache-beam-samples/shakespeare/*&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Concept #2: Apply a FlatMapElements transform the PCollection of text lines.</span></span><br><span class="line">        .apply(</span><br><span class="line">            FlatMapElements.into(TypeDescriptors.strings())</span><br><span class="line">                .via((String word) -&gt; Arrays.asList(word.split(<span class="string">&quot;[^\\p&#123;L&#125;]+&quot;</span>))))</span><br><span class="line">        .apply(Filter.by((String word) -&gt; !word.isEmpty()))</span><br><span class="line">        <span class="comment">// Concept #3: Apply the Count transform to our PCollection of individual words. </span></span><br><span class="line">        .apply(Count.perElement())</span><br><span class="line">        .apply(</span><br><span class="line">            MapElements.into(TypeDescriptors.strings())</span><br><span class="line">                .via(</span><br><span class="line">                    (KV&lt;String, Long&gt; wordCount) -&gt;</span><br><span class="line">                        wordCount.getKey() + <span class="string">&quot;: &quot;</span> + wordCount.getValue()))</span><br><span class="line">        <span class="comment">// Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline.</span></span><br><span class="line">        .apply(TextIO.write().to(<span class="string">&quot;wordcounts&quot;</span>));</span><br><span class="line"></span><br><span class="line">    p.run().waitUntilFinish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Some-conceptions"><a href="#Some-conceptions" class="headerlink" title="Some conceptions"></a>Some conceptions</h1><h2 id="I-x2F-O-data-source-x2F-target"><a href="#I-x2F-O-data-source-x2F-target" class="headerlink" title="I&#x2F;O (data source&#x2F;target)"></a>I&#x2F;O (data source&#x2F;target)</h2><p>Beam can process both batched  (bounded datasets) &amp; streaming (unbounded datasets) datasets. <a href="https://beam.apache.org/documentation/io/built-in/">built-in io transforms</a></p>
<p>Take reading as example, you specify the file location (the location must be accessable for the runner), and then the reader pull from datasource. You may also define the trigger to collect input window. When trigger is satisfied, window elements are emitted.</p>
<p>For unbounded datasets, they are split into windows. And each window is again a bounded datasets. In each window, there’re some elements. You can define how the elements are grouped as a window and when to emit the window elements for processing. <a href="https://beam.apache.org/documentation/programming-guide/#windowing">window concept</a></p>
<h2 id="Runner"><a href="#Runner" class="headerlink" title="Runner"></a>Runner</h2><p>Beam is an unified model. It abstracts the conception to define and run a pipeline. The real execution is conducted by the underlying runners.</p>
<p><a href="https://beam.apache.org/get-started/beam-overview/">all available runners</a></p>
<p>For unbounded datasets, the underlying runner must support stream processing.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/23/linux-command/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/23/linux-command/" class="post-title-link" itemprop="url">linux commands</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-23 15:32:13" itemprop="dateCreated datePublished" datetime="2019-01-23T15:32:13+08:00">2019-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-02-14 17:05:07" itemprop="dateModified" datetime="2023-02-14T17:05:07+08:00">2023-02-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="chmod-chown"><a href="#chmod-chown" class="headerlink" title="chmod, chown"></a>chmod, chown</h1><p><a href="https://www.linux.com/learn/understanding-linux-file-permissions">understanding linux file permissions</a></p>
<p>File permissions are defined by <strong>permission group</strong> and <strong>permission type</strong></p>
<ol>
<li>permission group<ul>
<li>owner(u)</li>
<li>group(g)</li>
<li>all other users(a)</li>
</ul>
</li>
<li>permission type<ul>
<li>read (r - 4)</li>
<li>write(w - 2)</li>
<li>execute(x - 1)</li>
</ul>
</li>
</ol>
<h2 id="permission-presentation"><a href="#permission-presentation" class="headerlink" title="permission presentation"></a>permission presentation</h2><p>The permission in the command line is displayed as <em><strong>_rwxrwxrwx 1 owner:group</strong></em></p>
<ul>
<li>the first character (underscore <strong>_</strong>  here) is the <strong>special permission flag</strong> that can vary.</li>
<li>the following three groups of <em><strong>rwx</strong></em> represent <strong>permission of owner, group and all other users</strong> respectively. If the owner and all users has no read permission, it is <em><strong>__wxrwx_wx</strong></em></li>
<li>follwing that grouping since the integer displays <strong>the number of hardlinks to the file</strong></li>
<li>the last piece is the owner and group assignment.</li>
</ul>
<h3 id="special-permission-flag"><a href="#special-permission-flag" class="headerlink" title="special permission flag"></a>special permission flag</h3><p>The special permission flag can be:</p>
<ul>
<li><strong>_</strong>: no special permissions</li>
<li><em><strong>d</strong></em>: directory</li>
<li><em><strong>l</strong></em>: the file or dir is a symbolic link</li>
<li><em><strong>s</strong></em>: This indicated the setuid&#x2F;setgid permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a <strong>s</strong> in the read portion of the owner or group permissions.</li>
<li><em><strong>t</strong></em>: This indicates the sticky bit permissions. This is not set displayed in the special permission part of the permissions display, but is represented as a <strong>t</strong> in the executable portion of the all users permissions</li>
</ul>
<h2 id="permission-modification"><a href="#permission-modification" class="headerlink" title="permission modification"></a>permission modification</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># grant read and write permissions to the user and group</span></span><br><span class="line">$ <span class="built_in">chmod</span> ug+rw file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove read and write permissions to the user and group</span></span><br><span class="line">$ <span class="built_in">chmod</span> ug-rw file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># set permission using binary references (owner: rwx = 4+2+1, group: rx = 4+1, all users: rx = 4+1)</span></span><br><span class="line">$ <span class="built_in">chmod</span> 755 file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># change the file permission recursively in the file/dir instead of just the files themselves</span></span><br><span class="line">$ <span class="built_in">chmod</span> -R 755 dir1</span><br></pre></td></tr></table></figure>

<h2 id="change-owner-group-assignments"><a href="#change-owner-group-assignments" class="headerlink" title="change owner:group assignments"></a>change owner:group assignments</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># change the owner of file1 to user1 and group to family</span></span><br><span class="line">$ <span class="built_in">chown</span> user1:family file1</span><br></pre></td></tr></table></figure>

<h1 id="find"><a href="#find" class="headerlink" title="find"></a>find</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find all xml files from the current dir</span></span><br><span class="line">$ find ./* -name <span class="string">&#x27;*.xml&#x27;</span></span><br></pre></td></tr></table></figure>

<p>To find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ find /directory_path -mtime -1 -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<p>Should be to your liking</p>
<p>The <code>-</code> before <code>1</code> is important - it means anything changed one day or less ago. A <code>+</code> before <code>1</code> would instead mean anything changed at least one day ago, while having nothing before the <code>1</code> would have meant it was changed exacted one day ago, no more, no less.</p>
<p>Another, more humane way:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">&quot;-24 hours&quot;</span> -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<p>or:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">&quot;1 day ago&quot;</span> -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<p>or:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find /&lt;directory&gt; -newermt <span class="string">&quot;yesterday&quot;</span> -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<h1 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h1><p>找到文件并删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">find /home/raven -name abc.txt | xargs rm -rf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">不使用 xargs</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list file with creation date and sort by it</span></span><br><span class="line">$ <span class="built_in">ls</span> -lct</span><br></pre></td></tr></table></figure>

<h1 id="du"><a href="#du" class="headerlink" title="du"></a>du</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">du</span> -sh -- * | <span class="built_in">sort</span> -hr</span><br></pre></td></tr></table></figure>

<h1 id="List-users"><a href="#List-users" class="headerlink" title="List users"></a>List users</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /etc/passwd | <span class="built_in">cut</span> -d: -f1</span><br></pre></td></tr></table></figure>

<h1 id="pbcopy"><a href="#pbcopy" class="headerlink" title="pbcopy"></a>pbcopy</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># copy file content to clipboard</span></span><br><span class="line">$ pbcopy &lt; test.txt</span><br></pre></td></tr></table></figure>

<h1 id="dstat"><a href="#dstat" class="headerlink" title="dstat"></a>dstat</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dstat -t -a --tcp --output network.log</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/17/lombok/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/17/lombok/" class="post-title-link" itemprop="url">lombok</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-17 10:24:55" itemprop="dateCreated datePublished" datetime="2019-01-17T10:24:55+08:00">2019-01-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://projectlombok.org/">lombok</a> is a library to help your <strong>write java cleaner and more efficiently</strong>. It’s plugged into the editor and build tool, which works <strong>at compile time</strong>. </p>
<p>Essentially, it modifies the byte-codes by operating AST (abstract semantic tree) at compile time, which is allowed by javac. This is, in fact, a way to modify java grammar.</p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>To use it,</p>
<ol>
<li>install lombok plugin in intellij</li>
<li>add package dependency in project (to use its annotations)</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.16.18<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>See <a href="https://projectlombok.org/features/all">all the annotations</a>. Give some example here:</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="@Data"></a>@Data</h3><p><a href="https://projectlombok.org/features/Data"><code>@Data</code></a> bundles the features of   <a href="https://projectlombok.org/features/ToString"><code>@ToString</code></a>, <a href="https://projectlombok.org/features/EqualsAndHashCode"><code>@EqualsAndHashCode</code></a>, <a href="https://projectlombok.org/features/GetterSetter"><code>@Getter</code> &#x2F; <code>@Setter</code></a> and <a href="https://projectlombok.org/features/constructor"><code>@RequiredArgsConstructor</code></a> together.</p>
<p><strong>With lombok:</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lombok.AccessLevel;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"><span class="keyword">import</span> lombok.ToString;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span> <span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataExample</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">  <span class="meta">@Setter(AccessLevel.PACKAGE)</span> <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">double</span> score;</span><br><span class="line">  <span class="keyword">private</span> String[] tags;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@ToString(includeFieldNames=true)</span></span><br><span class="line">  <span class="meta">@Data(staticConstructor=&quot;of&quot;)</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Exercise</span>&lt;T&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> T value;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Vanila java:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataExample</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">double</span> score;</span><br><span class="line">  <span class="keyword">private</span> String[] tags;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">DataExample</span><span class="params">(String name)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.name;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">setAge</span><span class="params">(<span class="type">int</span> age)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getAge</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.age;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setScore</span><span class="params">(<span class="type">double</span> score)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.score = score;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="type">double</span> <span class="title function_">getScore</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.score;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> String[] getTags() &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.tags;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTags</span><span class="params">(String[] tags)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.tags = tags;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="meta">@Override</span> <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;DataExample(&quot;</span> + <span class="built_in">this</span>.getName() + <span class="string">&quot;, &quot;</span> + <span class="built_in">this</span>.getAge() + <span class="string">&quot;, &quot;</span> + <span class="built_in">this</span>.getScore() + <span class="string">&quot;, &quot;</span> + Arrays.deepToString(<span class="built_in">this</span>.getTags()) + <span class="string">&quot;)&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">protected</span> <span class="type">boolean</span> <span class="title function_">canEqual</span><span class="params">(Object other)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> other <span class="keyword">instanceof</span> DataExample;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span> <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="built_in">this</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (!(o <span class="keyword">instanceof</span> DataExample)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="type">DataExample</span> <span class="variable">other</span> <span class="operator">=</span> (DataExample) o;</span><br><span class="line">    <span class="keyword">if</span> (!other.canEqual((Object)<span class="built_in">this</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.getName() == <span class="literal">null</span> ? other.getName() != <span class="literal">null</span> : !<span class="built_in">this</span>.getName().equals(other.getName())) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.getAge() != other.getAge()) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (Double.compare(<span class="built_in">this</span>.getScore(), other.getScore()) != <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (!Arrays.deepEquals(<span class="built_in">this</span>.getTags(), other.getTags())) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span> <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">PRIME</span> <span class="operator">=</span> <span class="number">59</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">long</span> <span class="variable">temp1</span> <span class="operator">=</span> Double.doubleToLongBits(<span class="built_in">this</span>.getScore());</span><br><span class="line">    result = (result*PRIME) + (<span class="built_in">this</span>.getName() == <span class="literal">null</span> ? <span class="number">43</span> : <span class="built_in">this</span>.getName().hashCode());</span><br><span class="line">    result = (result*PRIME) + <span class="built_in">this</span>.getAge();</span><br><span class="line">    result = (result*PRIME) + (<span class="type">int</span>)(temp1 ^ (temp1 &gt;&gt;&gt; <span class="number">32</span>));</span><br><span class="line">    result = (result*PRIME) + Arrays.deepHashCode(<span class="built_in">this</span>.getTags());</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Exercise</span>&lt;T&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> T value;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Exercise</span><span class="params">(String name, T value)</span> &#123;</span><br><span class="line">      <span class="built_in">this</span>.name = name;</span><br><span class="line">      <span class="built_in">this</span>.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; Exercise&lt;T&gt; <span class="title function_">of</span><span class="params">(String name, T value)</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Exercise</span>&lt;T&gt;(name, value);</span><br><span class="line">      &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getName</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> T <span class="title function_">getValue</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">this</span>.value;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="string">&quot;Exercise(name=&quot;</span> + <span class="built_in">this</span>.getName() + <span class="string">&quot;, value=&quot;</span> + <span class="built_in">this</span>.getValue() + <span class="string">&quot;)&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">protected</span> <span class="type">boolean</span> <span class="title function_">canEqual</span><span class="params">(Object other)</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> other <span class="keyword">instanceof</span> Exercise;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (o == <span class="built_in">this</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">if</span> (!(o <span class="keyword">instanceof</span> Exercise)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      Exercise&lt;?&gt; other = (Exercise&lt;?&gt;) o;</span><br><span class="line">      <span class="keyword">if</span> (!other.canEqual((Object)<span class="built_in">this</span>)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">this</span>.getName() == <span class="literal">null</span> ? other.getValue() != <span class="literal">null</span> : !<span class="built_in">this</span>.getName().equals(other.getName())) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">this</span>.getValue() == <span class="literal">null</span> ? other.getValue() != <span class="literal">null</span> : !<span class="built_in">this</span>.getValue().equals(other.getValue())) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span> <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">int</span> <span class="variable">PRIME</span> <span class="operator">=</span> <span class="number">59</span>;</span><br><span class="line">      <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">      result = (result*PRIME) + (<span class="built_in">this</span>.getName() == <span class="literal">null</span> ? <span class="number">43</span> : <span class="built_in">this</span>.getName().hashCode());</span><br><span class="line">      result = (result*PRIME) + (<span class="built_in">this</span>.getValue() == <span class="literal">null</span> ? <span class="number">43</span> : <span class="built_in">this</span>.getValue().hashCode());</span><br><span class="line">      <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/15/automatic-drive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/15/automatic-drive/" class="post-title-link" itemprop="url">automatic drive</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-15 09:57:10" itemprop="dateCreated datePublished" datetime="2019-01-15T09:57:10+08:00">2019-01-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>reference:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/29393415">coco</a>: one format for data labelling</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5e6217ea2a76689a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="auto-drive system"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-6a7eac9f5586b04a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="labelling system"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5e82ce0148c621a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="training system.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/10/spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/10/spark/" class="post-title-link" itemprop="url">spark</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-10 10:17:01" itemprop="dateCreated datePublished" datetime="2019-01-10T10:17:01+08:00">2019-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-05-10 10:57:43" itemprop="dateModified" datetime="2023-05-10T10:57:43+08:00">2023-05-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h1><p><a href="https://spark.apache.org/docs/latest/">spark</a> is a fast and general-purpose cluster computing system like Hadoop Map-reduce.  It runs on the clusters.</p>
<h2 id="Spark-Ecosystem"><a href="#Spark-Ecosystem" class="headerlink" title="Spark Ecosystem"></a>Spark Ecosystem</h2><p><strong><a href="http://data-flair.training/blogs/apache-spark-ecosystem-components/">The components of Apache Spark Ecosystem</a></strong></p>
<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/07/apache-spark-ecosystem-components.jpg" alt="ecosystem"></p>
<ul>
<li>spark core: <strong>cluster computing system</strong>. Provide API to  write computing functions.</li>
<li><a href="https://spark.apache.org/docs/2.4.0/sql-programming-guide.html">Spark SQL</a>. SQL for data processing, like hive?</li>
<li><a href="https://spark.apache.org/docs/2.4.0/ml-guide.html">MLlib</a> for machine learning.</li>
<li><a href="https://spark.apache.org/docs/2.4.0/graphx-programming-guide.html">GraphX</a> for graph processing</li>
<li><a href="https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html">Spark Streaming</a>.</li>
</ul>
<h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p>Spark Core is the fundamental unit of the whole Spark project. Its key features are:</p>
<ul>
<li>It is in charge of essential I&#x2F;O functionalities.</li>
<li>Provide API to defines and manipulate the RDDs. Significant in programming and observing the role of the <strong><a href="http://data-flair.training/blogs/install-apache-spark-multi-node-cluster/">Spark cluster</a></strong>.</li>
<li>Task dispatching, scheduling</li>
<li>Fault recovery.</li>
<li>It overcomes the snag of**<a href="http://data-flair.training/blogs/hadoop-mapreduce-introduction-tutorial-comprehensive-guide/"> MapReduce</a>** by using in-memory computation.</li>
</ul>
<p>Spark makes use of Special data structure known as <strong><a href="http://data-flair.training/blogs/rdd-in-apache-spark/">RDD (Resilient Distributed Dataset)</a></strong>. Spark Core is distributed execution engine with all the functionality attached on its top. For example, MLlib, <strong><a href="http://data-flair.training/blogs/spark-sql-tutorial/">SparkSQL</a></strong>, GraphX, <strong><a href="http://data-flair.training/blogs/apache-spark-streaming-comprehensive-guide/">Spark Streaming</a></strong>. Thus, allows diverse workload on single platform. All the basic functionality of Apache Spark Like <strong><a href="http://data-flair.training/blogs/apache-spark-in-memory-computing/">in-memory computation</a>, <a href="http://data-flair.training/blogs/apache-spark-streaming-fault-tolerance/">fault tolerance</a></strong>, memory management, monitoring, task scheduling is provided by Spark Core.<br>Apart from this Spark also provides the basic connectivity with the data sources. For example, <strong><a href="http://data-flair.training/blogs/category/hbase/">HBase</a></strong>, Amazon S3, **<a href="http://data-flair.training/blogs/comprehensive-hdfs-guide-introduction-architecture-data-read-write-tutorial/">HDFS </a>**etc.</p>
<h4 id="Action-Job-Stage-Task"><a href="#Action-Job-Stage-Task" class="headerlink" title="Action, Job, Stage, Task"></a>Action, Job, Stage, Task<a name="action-job-stage-task" /></h4><p><strong>Actions</strong> are RDD’s operation. <em>reduce, collect, takeSample, take, first, saveAsTextfile, saveAsSequenceFile, countByKey, foreach</em> are common actions in Apache spark.</p>
<p>In a Spark application, when you invoke an action on RDD, a <strong>job</strong> is created. Jobs are the main function that has to be done and is submitted to Spark. The jobs are divided into <strong>stages</strong> depending on how they can be separately carried out (mainly on shuffle boundaries). Then, these stages are divided into <strong>tasks</strong>. Tasks are the smallest unit of work that has to be done the executor.</p>
<p>When you call <code>collect()</code> on an RDD or Dataset, the whole data is sent to the <strong>Driver</strong>. This is why you should be careful when calling <code>collect()</code>.</p>
<p> <strong>An example:</strong></p>
<p><a href="https://stackoverflow.com/questions/28973112/what-is-spark-job">What is Spark Job ?</a></p>
<blockquote>
<p>let’s say you need to do the following:</p>
<ol>
<li>Load a file with people names and addresses into RDD1</li>
<li>Load a file with people names and phones into RDD2</li>
<li>Join RDD1 and RDD2 by name, to get RDD3</li>
<li>Map on RDD3 to get a nice HTML presentation card for each person as RDD4</li>
<li>Save RDD4 to file.</li>
<li>Map RDD1 to extract zipcodes from the addresses to get RDD5</li>
<li>Aggregate on RDD5 to get a count of how many people live on each zipcode as RDD6</li>
<li>Collect RDD6 and prints these stats to the stdout.</li>
</ol>
<p>So,</p>
<ol>
<li>The *<strong>driver program*</strong> is this entire piece of code, running all 8 steps.</li>
<li>Producing the entire HTML card set on step 5 is a *<strong>job*</strong> (clear because we are using the <em>save</em> action, not a transformation). Same with the <em>collect</em> on step 8</li>
<li>Other steps will be organized into *<strong>stages*</strong>, with each job being the result of a sequence of stages. For simple things a job can have a single stage, but the need to repartition data (for instance, the join on step 3) or anything that breaks the locality of the data usually causes more stages to appear. You can think of stages as computations that produce intermediate results, which can in fact be persisted. For instance, we can persist RDD1 since we’ll be using it more than once, avoiding recomputation.</li>
<li>All 3 above basically talk about how the <em>logic</em> of a given algorithm will be broken. In contrast, a *<strong>task*</strong> is a particular <em>piece of data</em> that will go through a given stage, on a given executor.</li>
</ol>
</blockquote>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD 数据模型</p>
<table>
<thead>
<tr>
<th align="center">属性名</th>
<th align="center">成员类型</th>
<th align="center">属性含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">dependencies</td>
<td align="center">变量</td>
<td align="center">生成该RDD所依赖的父RDD</td>
</tr>
<tr>
<td align="center">compute</td>
<td align="center">方法</td>
<td align="center">生成该RDD的计算接口</td>
</tr>
<tr>
<td align="center">partitions</td>
<td align="center">变量</td>
<td align="center">该RDD的所有数据分片实体</td>
</tr>
<tr>
<td align="center">partitioner</td>
<td align="center">方法</td>
<td align="center">划分数据分片的规则</td>
</tr>
<tr>
<td align="center">preferredLocations</td>
<td align="center">变量</td>
<td align="center">数据分片的物理位置偏好</td>
</tr>
</tbody></table>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><p><a href="https://data-flair.training/blogs/learn-apache-spark-sparkcontext/">SparkContext</a> is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. <strong>It allows your Spark Application to access Spark Cluster</strong> with the help of Resource Manager. </p>
<p>If you want to create SparkContext, first <strong>SparkConf</strong> should be made. The SparkConf has a configuration parameter that our Spark driver application will pass to SparkContext. Some of these parameter defines properties of Spark driver application. While some are used by Spark to allocate resources on the cluster, like the number, memory size, and cores used by executor running on the worker nodes.<br>In short, <strong>it guides how to access the Spark cluster</strong>. After the creation of a SparkContext object, we can invoke functions such as <strong>textFile, sequenceFile, parallelize</strong> etc.<br>Once the SparkContext is created, it can be used to <strong><a href="http://data-flair.training/blogs/how-to-create-rdds-in-apache-spark/">create RDDs</a></strong>, broadcast variable, and accumulator, ingress Spark service and run jobs. All these things can be carried out until SparkContext is stopped.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Usage: wordcount &lt;file&gt;&quot;</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the builder here defines the sparkConf, and then create a sparkSession with an underlying SparkContext `spark.sparkContext`</span></span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;PythonWordCount&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># here by `spark.read.text(&#x27;some.txt&#x27;)`, we use SparkContext create an DataFrame</span></span><br><span class="line">    lines = spark.read.text(sys.argv[<span class="number">1</span>]).rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> r: r[<span class="number">0</span>])</span><br><span class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)) \</span><br><span class="line">                  .<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">                  .reduceByKey(add)</span><br><span class="line">    <span class="comment"># this is the spark action `collect`</span></span><br><span class="line">    output = counts.collect()</span><br><span class="line">    <span class="keyword">for</span> (word, count) <span class="keyword">in</span> output:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s: %i&quot;</span> % (word, count))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># this in fact stop the sparkContext</span></span><br><span class="line">    spark.stop()</span><br></pre></td></tr></table></figure>

<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/08/functions-of-sparkcontext-in-apache-spark.jpg" alt="10 Important Functions of SparkContext in Apache Spark"></p>
<h1 id="How-does-it-run"><a href="#How-does-it-run" class="headerlink" title="How does it run"></a>How does it run</h1><p>Spark core contains the main api, driver engine, scheduler… to support the cluster computing. The real computing is completed on the cluster. Spark can connect to many cluster managers(spark’s own standalone cluster manager, mesos, yarn) to complete the jobs. Typically, the process is like this:</p>
<ol>
<li>The user submits a spark application using the <code>spark-submit</code> command.</li>
<li>Spark-submit launches the driver program on the same node in (client mode) or on the cluster (cluster mode) and invokes the main method specified by the user.</li>
<li>The driver program contacts the cluster manager to ask for resources to launch executor JVMs based on the configuration parameters supplied.</li>
<li>The cluster manager launches executor JVMs on worker nodes.</li>
<li>The driver process scans through the user application. Based on the RDD actions and transformations in the program, Spark creates an operator graph.</li>
<li>When an action (such as collect) is called, the graph is submitted to a DAG scheduler. The DAG scheduler divides the operator graph into stages.</li>
<li>A stage comprises tasks based on partitions of the input data. The driver sends work to executors in the form of tasks.</li>
<li>The executors process the task and the result sends back to the driver through the cluster manager.</li>
</ol>
<p><img src="https://spark.apache.org/docs/latest/img/cluster-overview.png" alt="cluster mode overview"></p>
<p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/08/Internals-of-job-execution-in-spark.jpg" alt="Complete Picture of Apache Spark Job Execution Flow."></p>
<h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><p>You can write spark function (eg. map function, reduce funciton) using Java&#x2F;scala&#x2F;python&#x2F;R API. See <a href="https://spark.apache.org/docs/latest/">api docs</a>.</p>
<h1 id="Installation-On-Yarn"><a href="#Installation-On-Yarn" class="headerlink" title="Installation On Yarn"></a>Installation On Yarn</h1><p>See <a href="https://spark.apache.org/docs/2.4.0/running-on-yarn.html">run spark on Yarn</a>, <a href="https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/">Install, Configure, and Run Spark on Top of a Hadoop YARN Cluster</a></p>
<ol>
<li><p><a href="https://spark.apache.org/downloads.html">downloads page</a> download the spark</p>
</li>
<li><p><code>tar -xvf spark-xxx.tgz</code></p>
</li>
<li><p>configuration</p>
</li>
</ol>
<ul>
<li>config in <code>/conf/spark-env.sh</code></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config this to specify the installed HADOOP path</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure>

<ul>
<li>config in <code>/conf/spark-default.conf</code>. (<a href="https://spark.apache.org/docs/2.4.0/configuration.html#spark-properties">all configuration properties</a>)</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config the spark master</span></span><br><span class="line">spark.master                     yarn</span><br><span class="line">spark.driver.memory    512m</span><br><span class="line">spark.yarn.am.memory    512m</span><br><span class="line">spark.executor.memory          512m</span><br></pre></td></tr></table></figure>

<h2 id="history-server-config"><a href="#history-server-config" class="headerlink" title="history server config"></a>history server config</h2><p>When the spark job is running, you can access the job log by <code>localhost:4040</code>. When the job is finished, by default, the log is not persisted which means you can’t access it. To access the logs later, need to config the following: (see <a href="https://spark.apache.org/docs/latest/monitoring.html">spark Monitoring and Instrumentation</a> and <a href="https://spark.apache.org/docs/2.4.0/running-on-yarn.html#using-the-spark-history-server-to-replace-the-spark-web-ui">using history server to replace the spark web ui</a>)</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in /conf/spark-default.conf</span></span><br><span class="line"><span class="comment"># config history server</span></span><br><span class="line">spark.ui.filters         org.apache.spark.deploy.yarn.YarnProxyRedirectFilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># tell spark use history server url as the trackint url</span></span><br><span class="line">spark.yarn.historyServer.allowTracking  <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># enable log persistence</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># log write dir. Here use the hdfs dir and you must create the dir in hdfs first</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>               hdfs://localhost:9000/spark-logs</span><br><span class="line"></span><br><span class="line"><span class="comment"># log read dir. Sometimes logs are transfered.</span></span><br><span class="line">spark.history.fs.logDirectory     hdfs://localhost:9000/spark-logs</span><br></pre></td></tr></table></figure>

<h2 id="example-execution"><a href="#example-execution" class="headerlink" title="example execution"></a>example execution</h2><ul>
<li>Start histroy server: <code>sbin/start-history-server.sh</code></li>
<li>execute spark job:</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>

<p>Then you can:</p>
<ul>
<li><p>Check the job&#x2F;application info in yarn: <code>http://localhost:8088/cluster/apps</code></p>
</li>
<li><p>Check the job&#x2F;application using Spark history server: <code>http://localhost:18080/</code></p>
</li>
</ul>
<h1 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h1><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html#glossary">glossary</a></p>
<p>NoteThe following table summarizes terms you’ll see used to refer to cluster concepts:</p>
<table>
<thead>
<tr>
<th align="left">Term</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Application</td>
<td align="left">User program built on Spark. Consists of a <em>driver program</em> and <em>executors</em> on the cluster.</td>
</tr>
<tr>
<td align="left">Application jar</td>
<td align="left">A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies**. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.**</td>
</tr>
<tr>
<td align="left">Driver program</td>
<td align="left">The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td align="left">Cluster manager</td>
<td align="left">An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</td>
</tr>
<tr>
<td align="left">Deploy mode</td>
<td align="left">Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</td>
</tr>
<tr>
<td align="left">Worker node</td>
<td align="left">Any node that can run application code in the cluster</td>
</tr>
<tr>
<td align="left">Executor</td>
<td align="left">A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</td>
</tr>
<tr>
<td align="left">Task</td>
<td align="left">A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td align="left">Job</td>
<td align="left">A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. <code>save</code>, <code>collect</code>); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td align="left">Stage</td>
<td align="left">Each job gets divided into smaller sets of tasks called <em>stages</em> that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody></table>
<h1 id="Deploy-mode"><a href="#Deploy-mode" class="headerlink" title="Deploy mode"></a>Deploy mode</h1><h2 id="yarn-client-vs-yarn-cluster"><a href="#yarn-client-vs-yarn-cluster" class="headerlink" title="yarn-client vs yarn-cluster"></a>yarn-client vs yarn-cluster</h2><p><a href="https://www.cnblogs.com/ittangtang/p/7967386.html">yarn-client vs yarn-cluster 深度剖析</a></p>
<p><a href="https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use">stackoverflow</a></p>
<h1 id="spark-shell-vs-spark-submit"><a href="#spark-shell-vs-spark-submit" class="headerlink" title="spark-shell vs spark-submit"></a>spark-shell vs spark-submit</h1><p>Spark shell is only intended to be use for testing and perhaps development of small applications - is only an interactive shell and should not be use to run production spark applications. For production application deployment you should use spark-submit. The last one will also allow you to run applications in yarn-cluster mode</p>
<h1 id="Spark-DataFrame"><a href="#Spark-DataFrame" class="headerlink" title="Spark DataFrame"></a>Spark DataFrame</h1><h2 id="auto-increment-id"><a href="#auto-increment-id" class="headerlink" title="auto increment id"></a>auto increment id</h2><p><a href="https://blog.csdn.net/k_wzzc/article/details/84996172">two ways for auto increment id</a></p>
<h3 id="Row-number"><a href="#Row-number" class="headerlink" title="Row_number"></a>Row_number</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 设置窗口函数的分区以及排序，因为是全局排序而不是分组排序，所有分区依据为空</span></span><br><span class="line"><span class="comment">  * 排序规则没有特殊要求也可以随意填写</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> spec = <span class="type">Window</span>.partitionBy().orderBy($<span class="string">&quot;lon&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df1 = dataframe.withColumn(<span class="string">&quot;id&quot;</span>, row_number().over(spec))</span><br><span class="line"></span><br><span class="line">df1.show()</span><br></pre></td></tr></table></figure>

<h3 id="rdd-zipWithIndex"><a href="#rdd-zipWithIndex" class="headerlink" title="rdd.zipWithIndex"></a>rdd.zipWithIndex</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在原Schema信息的基础上添加一列 “id”信息</span></span><br><span class="line"> <span class="keyword">val</span> schema: <span class="type">StructType</span> = dataframe.schema.add(<span class="type">StructField</span>(<span class="string">&quot;id&quot;</span>, <span class="type">LongType</span>))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// DataFrame转RDD 然后调用 zipWithIndex</span></span><br><span class="line"> <span class="keyword">val</span> dfRDD: <span class="type">RDD</span>[(<span class="type">Row</span>, <span class="type">Long</span>)] = dataframe.rdd.zipWithIndex()</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = dfRDD.map(tp =&gt; <span class="type">Row</span>.merge(tp._1, <span class="type">Row</span>(tp._2)))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 将添加了索引的RDD 转化为DataFrame</span></span><br><span class="line"> <span class="keyword">val</span> df2 = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"> df2.show()</span><br></pre></td></tr></table></figure>

<h2 id="Add-constant-column"><a href="#Add-constant-column" class="headerlink" title="Add constant column"></a>Add constant column</h2><p><a href="https://stackoverflow.com/questions/32788322/how-to-add-a-constant-column-in-a-spark-dataframe">add constant column</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.typedLit</span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">&quot;some_array&quot;</span>, typedLit(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">df.withColumn(<span class="string">&quot;some_struct&quot;</span>, typedLit((<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>, <span class="number">0.3</span>)))</span><br><span class="line">df.withColumn(<span class="string">&quot;some_map&quot;</span>, typedLit(<span class="type">Map</span>(<span class="string">&quot;key1&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;key2&quot;</span> -&gt; <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">from pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df.withColumn(&#x27;new_column&#x27;, lit(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h2 id="select-latest-record"><a href="#select-latest-record" class="headerlink" title="select latest record"></a>select latest record</h2><p><a href="https://stackoverflow.com/questions/55615716/select-latest-record-from-spark-dataframe">stackoverflow</a></p>
<h1 id="Hive-Hints"><a href="#Hive-Hints" class="headerlink" title="Hive Hints"></a>Hive Hints</h1><p><a href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#partitioning-hints">hive hints</a></p>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><p><a href="https://www.youtube.com/watch?v=daXEp4HmS-E">deep dive - spark optimization</a></p>
<p><a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">performance tuning</a></p>
<h2 id="Get-Baseline"><a href="#Get-Baseline" class="headerlink" title="Get Baseline"></a>Get Baseline</h2><ol>
<li>利用 spark-ui 观察任务运行情况（long stages，spill，laggard tasks, etc.）</li>
<li>利用 yarn 等观察资源利用情况（CPU 利用率 etc.）</li>
</ol>
<h2 id="Memory-spill"><a href="#Memory-spill" class="headerlink" title="Memory spill"></a>Memory spill<a name = "memory-spill" /></h2><p>Spark 运行时会分配一定的 memory（可以<a href="#specify-resources">指定资源需求</a>)， 分 storage 和 working memory。</p>
<ul>
<li>storage memory 是 persist 会用的 memory。当调用 persist（或 cache，一种使用 <code>StorageLevel.MemoryAndDisk</code> 的 persist）时，如果指定的 storage_level 有 memory，那么就会将数据存到 memory。</li>
<li>working memory 是 spark 运算所需要的 memory，这个大小是动态变化的。当 storage memory 占用过多内存时，working memory 就不够了。然后就会有 spill，就会慢。</li>
</ul>
<p>memory spill 表示 working memory 不够，spark 开始使用 disk。而 disk 的 I&#x2F;O 效率是极低的。所以一旦出现 spill，性能就会大大降低。</p>
<p>working memory 不够有很多原因：</p>
<ol>
<li>Memory 资源申请的太少了，就是不够 &#x3D;&#x3D;&#x3D;&#x3D;》 增加 <code>spark.executor.memory</code><ol>
<li>数据在 memory&#x2F;disk 的存储一般是 serialized，以节省空间。但数据 load 到 working memory 时，一般都是 deserialized 的，处理更快，但是更占空间。</li>
</ol>
</li>
<li>资源可以了，partition 太少，每个 partition 处理的数据太多，所以 spill 了 &#x3D;&#x3D;&#x3D;&#x3D;》 增加 <a href="shuffle-partition">shuffle partition</a></li>
<li>有不均衡出现，导致某些 task 处理的数据尤其多 &#x3D;&#x3D;&#x3D;&#x3D;》see <a href="#balance">balance</a></li>
<li>有太多 persist，持久化了太多东西，占用过多的 storage memory &#x3D;&#x3D;&#x3D;&#x3D;》see <a href="#persistence">persistence</a></li>
</ol>
<p><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-storage-hierachy.png" alt="image-20210319124829765"></p>
<h2 id="指定资源需求"><a href="#指定资源需求" class="headerlink" title="指定资源需求"></a>指定资源需求<a name="specify-resources" /></h2><p>Spark-submit 运行时，可通过指定以下参数来定义运行所需的资源：</p>
<ul>
<li><code>--conf spark.num.executors=xx</code> (或 <code>--num-executors xx</code>)：指定运行时需要几个 executor（也可以通过 <a href="#dynamic-allocation">dynamic allocation</a> 来根据运算动态分配 executors）</li>
<li><code>--conf spark.executor.memory=xxG</code>（或 <code>--executor-memory xxG</code>）：指定每个 executor 所需要的内存</li>
<li><code>--conf spark.executor.cores=xx</code>（或 <code>--executor-cores xx</code>）：指定每个 executor 所需要的 cores</li>
<li><code>--conf spark.driver.memory=xxG</code>（或 <code>--driver-memory xxG</code>）：指定每个 driver 所需要的内存。当执行 <code>df.collect()</code>时，会将数据 collect 到 driver，此时就需要 driver 有很多的 memory</li>
<li><code>--conf spark.driver.cores=xx</code>（或 <code>--driver-cores xx</code>）：指定每个 driver 所需要的 cores</li>
</ul>
<h2 id="Some-issues"><a href="#Some-issues" class="headerlink" title="Some issues"></a>Some issues</h2><h3 id="–executor-cores-settinng-not-working"><a href="#–executor-cores-settinng-not-working" class="headerlink" title="–executor-cores settinng not working"></a>–executor-cores settinng not working</h3><p>需要配置 <code>yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</code>，因为默认的使用的是 <a href="https://apache.googlesource.com/hadoop-common/+/e0c9f893b684246feb5b4adbb95a05a436cdb790/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/DefaultResourceCalculator.java">DefaultResourceCalculator</a>，它只看 memory(–executor-memory)，DominantResourceCalculator 则同时考虑 cpu 和 memory</p>
<p>see <a href="https://stackoverflow.com/questions/33248108/spark-executor-on-yarn-client-does-not-take-executor-core-count-configuration">stackoverflow</a></p>
<h3 id="–spark-dynamicAllocation-maxExecutors-not-working"><a href="#–spark-dynamicAllocation-maxExecutors-not-working" class="headerlink" title="–spark.dynamicAllocation.maxExecutors not working"></a>–spark.dynamicAllocation.maxExecutors not working<a name="dynamic-allocation" /></h3><p>这个需要和其他配置配合使用</p>
<blockquote>
<p>spark.dynamicAllocation.enabled &#x3D; true<br>This requires <code>spark.shuffle.service.enabled</code> or <code>spark.dynamicAllocation.shuffleTracking.enabled</code> to be set. The following configurations are also relevant: <code>spark.dynamicAllocation.minExecutors</code>, <code>spark.dynamicAllocation.maxExecutors</code>, and <code>spark.dynamicAllocation.initialExecutors</code> <code>spark.dynamicAllocation.executorAllocationRatio</code></p>
</blockquote>
<p>如果还不工作，可能要按 [spark dynamic allocation not working](https://community.cloudera.com/t5/Support-Questions/Spark-dynamic-allocation-dont-work/td-p/140227 设置各 nodemanager 并重启</p>
<p>See <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">spark dynamic allocation</a></p>
<h2 id="Partitions"><a href="#Partitions" class="headerlink" title="Partitions"></a>Partitions</h2><p>接下来从输入、运行、输出三个阶段的 partition 优化来看</p>
<p>一般 1 partition -&gt; 1 task，分多少个 partition，就拆多少个 task 来运行。</p>
<ol>
<li><strong>Avoid the spills</strong></li>
<li><strong>Maximize parallelism</strong><ol>
<li>utilize all cores</li>
<li>provision only the cores you need</li>
</ol>
</li>
</ol>
<h3 id="输入（input）"><a href="#输入（input）" class="headerlink" title="输入（input）"></a>输入（input）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.default.parallelism (don&#x27;t use)</span><br><span class="line">spark.sql.files.maxPartitionBytes (mutable，控制每个 partition 读的文件大小)</span><br></pre></td></tr></table></figure>

<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle<a name="shuffle-partition" /></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.shuffle.partitions（控制使用多少个 partition 来 shuffle）</span><br><span class="line">spark.default.parallelism（控制 rdd 的 partition 数目？？？？？？）</span><br></pre></td></tr></table></figure>

<p>如果配置了 <code>spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#39;true&#39;)</code> 或 <code>spark.sql.adaptive.coalescePartitions.enabled</code> ，它会动态控制 parition count （参见 <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions">coalescing post shuffle partitions</a>），根据 shuffle 数据大小来动态设置 partition 数目。但是这个设置可能不合理，因为 shuffle 过程中，最终操作的数据可能远大于 shuffle read 的大小，这个过程中存在 deserialize 等。如果配置了动态控制，依然出现了 shuffle spill，那么可以先关掉这个配置，手动控制 shuffle partitions 大小。</p>
<h3 id="输出（output）"><a href="#输出（output）" class="headerlink" title="输出（output）"></a>输出（output）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">coalesce</span><br><span class="line">repartition</span><br><span class="line">repartition(range) ===&gt; range partitioner???</span><br><span class="line">df.localCheckPoint().repartition().... ==&gt; how to use tis</span><br></pre></td></tr></table></figure>

<h2 id="Balance"><a href="#Balance" class="headerlink" title="Balance"></a>Balance<a name="balance" /></h2><p>When some partitions are significantly larger than most, there is skew.</p>
<p>Balance 体现在很多方面：网络、GC、数据，当然最常见的问题是数据的不均匀。</p>
<p>通过查看 spark ui 可以看到不均匀的任务（这个时候需要停掉重跑）：</p>
<ol>
<li>查看 staggling tasks<ol>
<li>查看 stage 执行进度：stage 里剩余几个 task 执行特别慢，这个时候各个 task 处理的数据肯定存在不均匀，导致那几个 task 处理的尤其慢<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-straggling-task1.png" alt="image-20210317130715804"></li>
</ol>
</li>
<li>查看 stage 执行 metric：大部分时候没有 spill，但是 max 的时候有 spill；或者大部分的时候 read size 和 max read size 有很大差别<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-straggling-task2.png" alt="image-20210317130618356"></li>
</ol>
</li>
</ol>
</li>
<li>查看 stage 里各个节点的 GC time，GC time 分布不均匀，也是有问题的（什么问题？？）<ol>
<li><img src="/Users/zhenzheng/code/hfcherish.github.io/source/images/spark-gc-skew.png" alt="image-20210317130839506"></li>
</ol>
</li>
<li></li>
</ol>
<h2 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence<a name="persistence" /></h2><p>当 execution plan 中，有些 superset 被多个 subset 所使用，superset 计算复杂、耗时久，这个时候就可以选择将 superset persist，从而避免重复运算。</p>
<blockquote>
<p><a href="#action-job-stage-task">spark core</a> 中有几个概念，其中只有 action 会触发一次 dag 的运行。同一段代码，可能会生成不同的 dag，每次都需要执行。所以如果被多次使用的 superset，最好将它 cache，避免后续的重复运算。</p>
</blockquote>
<p>persist&#x2F;cache 要慎用，因为：</p>
<ol>
<li>占资源。当 persist 消耗了太多的 storage memory 时，就会出现 <a href="#memory-spill">memory spill</a></li>
<li>也有时间损耗（serialize, deserialize, I&#x2F;O)。persist 一般都以 serialized 的形式存储，节省空间，而 load 到 working memory 时，又需要 deserialiize</li>
</ol>
<blockquote>
<p>In Python, stored objects will always be serialized with the <a href="https://docs.python.org/3/library/pickle.html">Pickle</a> library, so it does not matter whether you choose a serialized level. The available storage levels in Python include <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>DISK_ONLY</code>, <code>DISK_ONLY_2</code>, and <code>DISK_ONLY_3</code>.*</p>
</blockquote>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>Cache 是选择 default 的 persist。persist 可以选择不同的 <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence">persistence storage level</a> </p>
<p>With <code>cache()</code>, you use only the default storage level :</p>
<ul>
<li><code>MEMORY_ONLY</code> for <strong>RDD</strong></li>
<li><code>MEMORY_AND_DISK</code> for <strong>Dataset</strong></li>
</ul>
<p>With <code>persist()</code>, you can specify which storage level you want for both <strong>RDD</strong> and <strong>Dataset</strong>.</p>
<p>From the official docs:</p>
<blockquote>
<ul>
<li>You can mark an <code>RDD</code> to be persisted using the <code>persist</code>() or <code>cache</code>() methods on it.</li>
<li>each persisted <code>RDD</code> can be stored using a different <code>storage level</code></li>
<li>The <code>cache</code>() method is a shorthand for using the default storage level, which is <code>StorageLevel.MEMORY_ONLY</code> (store deserialized objects in memory).</li>
</ul>
</blockquote>
<p>Use <code>persist()</code> if you want to assign a storage level other than :</p>
<ul>
<li><code>MEMORY_ONLY</code> to the <strong>RDD</strong></li>
<li>or <code>MEMORY_AND_DISK</code> for <strong>Dataset</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(<span class="string">&quot;tableName&quot;</span>)</span><br><span class="line">spark.catalog.uncacheTable(<span class="string">&quot;tableName&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataFrame.cache()</span><br></pre></td></tr></table></figure>

<h2 id="broadcast-join"><a href="#broadcast-join" class="headerlink" title="broadcast join"></a>broadcast join</h2><p><a href="https://zhuanlan.zhihu.com/p/58765338">spark 执行 map-join 优化</a></p>
<p><a href="https://www.jianshu.com/p/2c7689294a73">spark broadcast join</a></p>
<p>几种方式：</p>
<h3 id="1-spark-自动识别小表-broadcast"><a href="#1-spark-自动识别小表-broadcast" class="headerlink" title="1. spark 自动识别小表 broadcast"></a>1. spark 自动识别小表 broadcast</h3><p><code>spark.sql.statistics.fallBackToHdfs=True</code>, 这样它会直接分析文件的大小，而不是 metastore 数据</p>
<h3 id="2-使用-hint"><a href="#2-使用-hint" class="headerlink" title="2. 使用 hint"></a>2. 使用 hint</h3><p><a href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html#partitioning-hints">hive hints</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ BROADCAST (b) */</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">where</span> id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b)</span><br></pre></td></tr></table></figure>

<h3 id="3-使用-dataframe-api"><a href="#3-使用-dataframe-api" class="headerlink" title="3. 使用 dataframe api"></a>3. 使用 dataframe api</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> broadcast</span><br><span class="line">broadcast(spark.table(<span class="string">&quot;b&quot;</span>)).join(spark.table(<span class="string">&quot;a&quot;</span>), <span class="string">&quot;id&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h2 id="cache-vs-broadcast"><a href="#cache-vs-broadcast" class="headerlink" title="cache vs broadcast"></a>cache vs broadcast</h2><p><a href="https://stackoverflow.com/questions/38056774/spark-cache-vs-broadcast">cache vs broadcast</a></p>
<blockquote>
<p>RDDs are divided into <em>partitions</em>. These partitions themselves act as an immutable subset of the entire RDD. When Spark executes each stage of the graph, each partition gets sent to a worker which operates on the subset of the data. In turn, each worker can <em>cache</em> the data if the RDD needs to be re-iterated.</p>
<p>Broadcast variables are used to send some immutable state <em>once</em> to each worker. You use them when you want a local copy of a variable.</p>
<p>These two operations are quite different from each other, and each one represents a solution to a different problem.</p>
</blockquote>
<h2 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h2><p><a href="https://blog.csdn.net/lhxsir/article/details/87882128">spark-sql 优化小文件过多</a></p>
<p><a href="https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908">On Spark, Hive, and Small Files: An In-Depth Look at Spark Partitioning Strategies</a></p>
<h4 id="为什么会有小文件？"><a href="#为什么会有小文件？" class="headerlink" title="为什么会有小文件？"></a>为什么会有小文件？</h4><p>当 spark 要 write 到 hive 表时，这实际也是一个 shuffle stage，就会分很多个 sPartition (spark partition)。每个 sPartition 在处理时，都会生成一个文件（如果是动态分区，则更严重，因为每个 sPartition 的数据分布式均匀的，每个 sPartition 可能包含很多个 hive paritition key，spark 每遇到一个 partition key 就生成一个文件），那么 sPartition 数目越多（动态分区的情况下，会更不可控），文件数就会越多。</p>
<p>简单来说，就是 spark 的一个 stage 分成了很多个 task（shuffle partitions 控制这个数量），即 sPartition，每个 sPartition 可能对应多个 hPartitiion（hive partition）key，多个 sPartition 也对应一个 hPartition key。而每个 sPartition 里对应的每个 hPartition key，都会生成一个文件。</p>
<p>那么，如果一个 sPartition 和 hPartition 只是一个 <strong>多（可控数目，对应最后每个 hPartitiion 的文件数）对一</strong> 的情况，那么文件数就是可控的。</p>
<blockquote>
<p>使用 hive 时，不会有小文件问题。hive 里只需要设置下边的这些参数，就</p>
<p>In pure Hive pipelines, there are configurations provided to automatically collect results into reasonably sized files, nearly transparently from the perspective of the developer, such as <em>hive.merge.smallfiles.avgsize</em>, or <em>hive.merge.size.per.task</em>.</p>
</blockquote>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ol>
<li>coalesce</li>
<li>repartition</li>
<li>Distribute by</li>
<li>adaptive execution</li>
</ol>
<p><a href="http://www.jasongj.com/spark/adaptive_execution/">Adaptive Execution 让 Spark SQL 更高效更智能</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 启用 Adaptive Execution ，从而启用自动设置 Shuffle Reducer 特性</span><br><span class="line">spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, &#x27;true&#x27;)</span><br><span class="line"># 设置每个 Reducer 读取的目标数据量，单位为字节。默认64M，一般改成集群块大小</span><br><span class="line">spark.conf.set(&quot;spark.sql.adaptive.shuffle.targetPostShuffleInputSize&quot;, &#x27;134217728&#x27;)</span><br></pre></td></tr></table></figure>

<h2 id="python-udf-vs-scala-udf"><a href="#python-udf-vs-scala-udf" class="headerlink" title="python udf vs scala udf"></a>python udf vs scala udf</h2><p><a href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62">python udf vs scala udf</a></p>
<p><img src="https://miro.medium.com/max/1570/1*ddtDqMvoDGhxsw0CDEnfag.png" alt="img"></p>
<p><img src="https://miro.medium.com/max/1415/1*SMlxTZJBsAPKmpdRH5VFVw.png" alt="img"></p>
<p><img src="https://miro.medium.com/max/1500/1*FFi8Yk6mwSc6AvI-avWcYw.png" alt="img"></p>
<h1 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h1><h2 id="Null-aware-predicate-sub-queries-cannot-be-used-in-nested-conditions"><a href="#Null-aware-predicate-sub-queries-cannot-be-used-in-nested-conditions" class="headerlink" title="Null-aware predicate sub-queries cannot be used in nested conditions"></a>Null-aware predicate sub-queries cannot be used in nested conditions</h2><p><code>not in</code> 不能和 <code>or</code> 之类的 condition 一块用。现在好像还没有修复，参见：<a href="https://github.com/apache/spark/pull/22141">SPARK-25154</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/09/yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/09/yarn/" class="post-title-link" itemprop="url">yarn</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-09 17:27:05" itemprop="dateCreated datePublished" datetime="2019-01-09T17:27:05+08:00">2019-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="yarn-architecture"><a href="#yarn-architecture" class="headerlink" title="yarn architecture"></a><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">yarn architecture</a></h1><p>Yarn is used to <strong>manage&#x2F;allocate cluster resource</strong> &amp; <strong>schedule&#x2F;moniter jobs</strong>. These parts – resource manager – are split up from hadoop framework.</p>
<p><img src="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="yarn architecture"></p>
<p>Yarn has two main components:</p>
<ul>
<li><strong>Schedular</strong>: manage resources (cpu, memory, network, disk, etc.) and allocate it the applications.<ul>
<li>node manager will tell Schedular the node resource info (node status)</li>
<li>application master will ask Schedular for resources.</li>
<li>When partitioning resources among various queues, applications, Schedular supports pluggable policies. For example:<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">CapacityScheduler</a> allocate resources by tenant request. It’s used especially for <strong>multi-tenant scenario</strong>, designed to allow sharing a large cluster while giving each <strong>organization</strong> capacity guarantees. Each client&#x2F;tenant can request any resources that are not used by others. And there’s strict ACLs to ensure the <strong>security</strong> of resources between tenants. The primary abstraction is queue. Different tenant use different queue to utilize the resources. And <strong>hierachical queues</strong> are provided to support data separation in one tenant.</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">FairScheduler</a> assigning resources to appliations such that all apps get, <strong>on average</strong>, <strong>an equal shares of resources over time</strong>. It’s mainly designed to share cluster between <strong>a number of users</strong>. It lets short apps are completed in a reasonable time while not starving long-lived apps. (resources might free up when new apps are submitted).</li>
</ul>
</li>
</ul>
</li>
<li><strong>ApplicationManager</strong>: accept job-submisons, negotiate to exeuct application masters, and moniter reboot app master when failure.<ul>
<li>AppMaster are the one who <ul>
<li>apply to Schedular for resources</li>
<li>boot up job execution</li>
<li>moniter the job execution status</li>
<li>tell app manager if the job fails or succeeds.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Other components:</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">ReservationSystem</a>: reserve some resources to ensure the predictable execution of important jobs.</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>: join clusters to scale and allow multiple independent clusters.</li>
</ul>
<h2 id="an-example"><a href="#an-example" class="headerlink" title="an example"></a>an example</h2><p>Take hive as example:</p>
<h3 id="load-data-–-non-distributed-computing-jobs"><a href="#load-data-–-non-distributed-computing-jobs" class="headerlink" title="load data – non-distributed-computing jobs"></a>load data – non-distributed-computing jobs</h3><ol>
<li>user uses hive command to load data into hive table. (eg. <code>LOAD DATA LOCAL INPATH &#39;/path/to/datafile/&#39; OVERWRITE INTO TABLE table_name;</code>)</li>
<li>hive calls hdfs to write data.</li>
<li>node inform schedular the new node status.</li>
</ol>
<h3 id="query-data-–-distributed-computing-jobs"><a href="#query-data-–-distributed-computing-jobs" class="headerlink" title="query data – distributed computing jobs"></a>query data – distributed computing jobs</h3><ol>
<li>user uses hive command to query data (eg. <code>select count(*) from xxx</code>)</li>
<li>hive submits a map-reduce job to appliction manager</li>
<li>application manager applies to Schedular (??? not sure) for a container to execute application master and boots it.</li>
<li>application master applies to Schedular for resoures to excute map-reduce job and boots the job.</li>
<li>the map-reduce job get input data from hdfs, and write output data into hdfs</li>
<li>the map-reduce job informs the application master the status of the job.</li>
<li>application manager will restart application master on failure (application failure&#x2F;hardware failure). (when application failed, the job informs the app master, and app manager knows it, and then reboot it)</li>
</ol>
<h1 id="JobHistoryServer"><a href="#JobHistoryServer" class="headerlink" title="JobHistoryServer"></a>JobHistoryServer</h1><p>On YARN all applications page, here’s a link to job history. However, you must config to make it take effect.</p>
<p>Follow the instructions <a href="https://blog.csdn.net/xiaoduan_/article/details/79689882">config of johhistory in hadoop</a>. Also, see <a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop Cluster Setup</a> to get info about starting log and jobhistory server. See <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server Rest API</a>, <a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/apidocs/index.html?org/apache/hadoop/mapreduce/v2/hs/package-tree.html">JobHistoryServer javadoc</a></p>
<blockquote>
<p>Notes:</p>
<p>The host of <code>mapreduce.jobhistory.webapp.address</code> and <code>mapreduce.jobhistory.address</code> may need to be set as the real ip (get from <code>ipconfig getifaddr en0</code>) or some other host (eg. cncherish.local) instead of <code>localhost</code>.</p>
<p>When start history server, you can see the start host in the log. It may look like this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">STARTUP_MSG: Starting JobHistoryServer</span><br><span class="line">STARTUP_MSG:   host = CNcherish.local/192.168.xx.xxx</span><br><span class="line">STARTUP_MSG:   args = []</span><br><span class="line">STARTUP_MSG:   version = 3.1.1</span><br></pre></td></tr></table></figure>

<p>This might be because the JobHistoryServer told <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html#Configurations">yarn web proxy</a> that its host is ‘cncherish.local&#x2F;192.168.xx.xxx’ (mapping host ‘cncherish.local’ to the real ip ‘192.168.xx.xxx’), while yarn knows that history host for map-reduce job is ‘localhost’ from <code>mapred-site.xml</code> — the config for map-reduced jobs. The incompatible info reduce the jobhistory link is unreachable.</p>
</blockquote>
<p>1.add the following properties into the <code>mapred-site.xml</code> (config the map-reduce framework)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- config to persist the jobhistory logs in hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.cleaner.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置jobhistoryserver 没有配置的话 history入口不可用 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.x.xxx:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置web端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.x.xxx:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置正在运行中的日志在hdfs上的存放路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done_intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置运行过的日志存放在hdfs上的存放路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2.add the following properties into the <code>yarn-site.xml</code> (config the yarn — resource manager)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.start the historyserver</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The following command will run server as a background daemon</span></span><br><span class="line">$ mapred --daemon start historyserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># The following command will run server on the current terminal.</span></span><br><span class="line"><span class="comment"># In this way, you can know how the server is started, stopped and what it does.</span></span><br><span class="line"><span class="comment"># Also you can know the real server host from the log, which should be aligned by the mapred-site.xml</span></span><br><span class="line">$ mapred historyserver</span><br></pre></td></tr></table></figure>

<h1 id="rest-api"><a href="#rest-api" class="headerlink" title="rest api"></a>rest api</h1><ul>
<li>yarn rest api:  throught postman <a href="http://localhost:8088/ws/v1/cluster/apps">http://localhost:8088/ws/v1/cluster/apps</a> (get all the apps)</li>
<li>history rest api: <a href="http://cnpzzheng.local:19888/ws/v1/history">http://cnpzzheng.local:19888/ws/v1/history</a> (get server info)<ul>
<li>use 19888 instead of 10020</li>
</ul>
</li>
</ul>
<h1 id="CheatSheet"><a href="#CheatSheet" class="headerlink" title="CheatSheet"></a>CheatSheet</h1><p><a href="https://www.jianshu.com/p/f510a1f8e5f0">link</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看正在运行的程序资源使用情况</span></span><br><span class="line">$ yarn top</span><br><span class="line">$ yarn node -all -list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看指定queue使用情况</span></span><br><span class="line">$ yarn queue -status root.users.xxx</span><br><span class="line">$ yarn application -movetoqueue application_1528080031923_0067 -queue root.users.xxx</span><br><span class="line"></span><br><span class="line">$ yarn application -list -appStates [ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING,FINISHED,FAILED,KILLED]</span><br><span class="line">$ yarn application -list -appTypes [SUBMITTED, ACCEPTED, RUNNING]</span><br><span class="line">$ yarn applicationattempt -list application_1528080031923_0064</span><br><span class="line"></span><br><span class="line">$ yarn application -<span class="built_in">kill</span> application_1528080031923_0067</span><br><span class="line"></span><br><span class="line">$ yarn logs -applicationId application_1528080031923_0064</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/07/hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/07/hadoop/" class="post-title-link" itemprop="url">hadoop</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-07 14:46:44" itemprop="dateCreated datePublished" datetime="2019-01-07T14:46:44+08:00">2019-01-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Hadoop is a framework of distributed storage &amp; computing.</p>
<ul>
<li><strong>distributed storage</strong>: hadoop use <strong>HDFS</strong> to save large amount of data in cluster.</li>
<li><strong>distributed computing</strong>: hadoop use <strong>map-reduce</strong> framework to conduct fast data analysis (query &amp; writing) over data in HDFS.</li>
<li><strong>resource manager &amp; job schedular</strong>: hadoop use <strong>yarn</strong> to manage&#x2F;allocate cluster resources (memory, cpu, etc.) and to schedule  and moniter job executing.</li>
</ul>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="cluster-architecture"><a href="#cluster-architecture" class="headerlink" title="cluster architecture"></a>cluster architecture</h2><p><img src="/images/hadoop-20201026164010368.png" alt="image-20201026164010368"></p>
<p><img src="/images/hadoop-20201026164402103.png" alt="image-20201026164402103"></p>
<h2 id="request-processing"><a href="#request-processing" class="headerlink" title="request processing"></a>request processing</h2><p><img src="/images/hadoop-20201026164147039.png" alt="image-20201026164147039"></p>
<h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><p>Use <strong>rack aware</strong> so that your replicas will be saved into different racks, which can solve the rack failure issue.</p>
<p>Each data node will send heartbeat and block report to the namenode. Thus when data node fails, the name node knows it and will re-replicated to 3.</p>
<p><img src="/images/hadoop-20201026164613209.png" alt="image-20201026164613209"></p>
<h2 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h2><p><strong>High Availability: Percentage of Uptime of the system</strong>. Fault Tolerance, on the other hand, mainly focus on the data loss &#x2F; system un-recovered damage tolerance. For example, a name-node failure can be processed by reboot from the aspect of fault-tolerance, while there must be a <strong>quick</strong> working solution from the aspect of high availability.</p>
<h3 id="Name-Node-Failure"><a href="#Name-Node-Failure" class="headerlink" title="Name Node Failure"></a>Name Node Failure</h3><p>For a name node failure, we want to switch to a standby name node with all the informations quickly. How?</p>
<p>A name node saves the file namespaces in memory, besides, it also saved editlog for each change into the disk. A name node failure will lose the in-memory fsImage, but we can reproduce the fsImage from the editlogs</p>
<p><img src="/images/hadoop-20201026165814244.png" alt="y"></p>
<p>A common solution is to use QJM to save the editlogs. And the standby name node will read from the editlogs to rebuild the fsImage. Besides, there’s  two failover controllers on each name node and a zookeeper. ZooKeeper keeps a lock, and both name nodes are requesting the lock. When the active name node fails, it lost the lock, and the standby nn will acquire the lock.</p>
<p><img src="/images/hadoop-20201026170521244.png" alt="image-20201026170521244"></p>
<h3 id="Name-Node-Reboot"><a href="#Name-Node-Reboot" class="headerlink" title="Name Node Reboot"></a>Name Node Reboot</h3><p>What if you just want to reboot the name node, and since the fsImage is in memory, it will be gone at once and it takes a long time to rebuild from the editlogs?</p>
<p>The main issue here is that the fsImage is in memory. Thus to reboot quickly, we need to save the fsImage into disk. The secondary name node is for this. It periodically merge the old fsImage with the editlogs, and replace the old fsImage in the disk, and then truncate the logs.</p>
<p>Secondary Name Node is not necessary. If needed, you can build it on the standby nn.</p>
<p><img src="/images/hadoop-20201026171217319.png" alt="image-20201026171217319"></p>
<h3 id="install-hadoop-on-mac"><a href="#install-hadoop-on-mac" class="headerlink" title="install hadoop on mac"></a><a href="http://www.cnblogs.com/micrari/p/5716851.html">install hadoop on mac</a></h3><p>see <a href="http://kontext.tech/docs/DataAndBusinessIntelligence/p/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">default ports used by hadoop services 3.1.0</a></p>
<blockquote>
<p>when config password-free login by ssh, it may only work when generate key into id_rsa&#x2F;id_dsa. The other user defind key file name won’t work.</p>
</blockquote>
<ul>
<li><strong>access hdfs</strong></li>
</ul>
<p><a href="https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2_1.html">hdfs default ports</a> are changed. see <a href="https://issues.apache.org/jira/browse/HDFS-9427">hdfs issue</a>, or check the <code>dfs.namenode.http-address</code> property in <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a> for the newest setting.</p>
<blockquote>
<p>Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820<br>Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868<br>Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 – &gt;9864</p>
</blockquote>
<p>When running the example, it seems that jar can only search files. Thus you need to ensure there’s no sub-dirs in search dir.</p>
<ul>
<li><strong>access yarn</strong></li>
</ul>
<p>Access resource manager through <code>localhost:8088</code>. Or check the property <code>yarn.resourcemanager.webapp.address</code> in  <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a> for the newest configuration</p>
<h4 id="start-hadoop-locally"><a href="#start-hadoop-locally" class="headerlink" title="start hadoop locally"></a>start hadoop locally</h4><p><a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">map reduce</a> is a framework to write applications which process vast amounts of data in-parallel on large clusters. </p>
<p>A map-reduce job usually splits the input data into independent chunks, and <strong>map</strong> in a parallel manner. Then the frameworks sorts the output and then <strong>reduce</strong> to the integrate output.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize the namenode</span></span><br><span class="line">$ hdfs namenode -format</span><br><span class="line"><span class="comment"># start namenode and datanode daemon (access namenode at localhost:9870)</span></span><br><span class="line">$ start-dfs.sh</span><br><span class="line"><span class="comment"># start ResourceManager &amp; NodeManager daemon (access yarn at localhost:8088)</span></span><br><span class="line">$ start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># stop namenode and datanode daemon</span></span><br><span class="line">$ stop-dfs.sh</span><br><span class="line"><span class="comment"># stop ResourceManager &amp; NodeManager daemon</span></span><br><span class="line">$ stop-yarn.sh</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Note:</p>
<p>The <code>hdfs namenode -format</code> command must be executed everytime you restarted your computer. And it’s initialized again. Need to figure out other ways to avoid this.</p>
</blockquote>
<p>There are other commands used to start these daemon:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># deprecated to use the above</span></span><br><span class="line">$ start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># used on specific node (eg. when a new node is added into the cluster, execute on that node)</span></span><br><span class="line">$ hadoop-daemon.sh start datanode/namenode</span><br><span class="line">$ yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/07/hdfs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/07/hdfs/" class="post-title-link" itemprop="url">hdfs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-07 14:05:16" itemprop="dateCreated datePublished" datetime="2019-01-07T14:05:16+08:00">2019-01-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="hdfs-architecture"><a href="#hdfs-architecture" class="headerlink" title="hdfs architecture"></a><a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#The+File+System+Namespace">hdfs architecture</a></h1><p>HDFS 集群以 master-slave 模型运行。其中有两种节点：</p>
<ul>
<li>namenode: master node. know where the files are to find in hdfs</li>
<li>datanode: slave node: have the data of the files</li>
</ul>
<p><img src="https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif" alt="architecture"></p>
<h1 id="namenode"><a href="#namenode" class="headerlink" title="namenode"></a>namenode</h1><p>参见 <a href="https://www.cnblogs.com/shitouer/archive/2013/01/07/2837683.html">namenode and datanode</a></p>
<p>Namenode 管理着文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块 (block) 所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/721960-5d86c88472cd002a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="namenode.png"></p>
<p>每个 file 有多个 block 构成，这些 block 分散的存储在各个 datanode 上（并且根据 replication factor，有冗余副本），而 namenode 知道如何一个 file 有哪些 block (file 的元数据信息)，根据 datanode 发送给它的 block 列表，namenode 就可以构建每个文件中各个 block 的位置信息。即根据文件元数据 + datanode block 列表，可以重建文件 block 位置信息，因此不需要持久化。</p>
<h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><p>由于 datanode 只是分布式地存储 block，不知道这些 block 是怎么组织成文件，以及文件是怎么组织成文件树的。因此 namenode 一旦当掉，整个文件系统就挂了（没办法写和查文件）。因此 namenode 的容错机制很重要。常见的方式：</p>
<ol>
<li>同步备份。即 namenode 中需要持久化存储的镜像文件和log，同步地持久化存储到其他文件系统中。</li>
<li>secondary namenode (异步)。secondary namenode 一般定期地去同步本地 namenode 的镜像和 log。但除此之外，secondary namenode 还有其他用途，比如合并镜像和log（避免文件过大），这个合并过程很占用 cpu 和内存，所以正好在 secondary namenode 上做。合并完后，在 secondary namenode 上也保存一份。不过这种备份恢复会丢掉一部分数据。</li>
</ol>
<h1 id="datanode"><a href="#datanode" class="headerlink" title="datanode"></a>datanode</h1><p>datanode 根据客户端或者 namenode 调度存储&#x2F;检索数据，并定期向 namenode 发送它们所存储的 block 列表。</p>
<h1 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h1><h2 id="hdfs-namenode-format"><a href="#hdfs-namenode-format" class="headerlink" title="hdfs namenode -format"></a>hdfs namenode -format</h2><p><a href="https://stackoverflow.com/questions/27143409/what-the-command-hadoop-namenode-format-will-do">stackoverflow</a></p>
<p>Remove all metadata in namenode. Initialize the namenode. However, the data in datanode is not removed.</p>
<h2 id="hdfs-dfs-mkdir-xxx"><a href="#hdfs-dfs-mkdir-xxx" class="headerlink" title="hdfs dfs -mkdir xxx"></a>hdfs dfs -mkdir xxx</h2><p>Create a directory. To see the data location, see <code>local storage</code></p>
<h2 id="hdfs-dfs-put-source-dest"><a href="#hdfs-dfs-put-source-dest" class="headerlink" title="hdfs dfs -put source dest"></a>hdfs dfs -put source dest</h2><p>copy content in source to dest</p>
<h2 id="hdfs-dfs-get"><a href="#hdfs-dfs-get" class="headerlink" title="hdfs dfs -get"></a>hdfs dfs -get</h2><h2 id="hdfs-dfs-du-h-v"><a href="#hdfs-dfs-du-h-v" class="headerlink" title="hdfs dfs -du -h -v"></a>hdfs dfs -du -h -v</h2><p>It displays sizes of files and directories contained in the given directory or the length of a file in case it’s just a file.</p>
<ul>
<li>The <strong>-s</strong> option will result in an <strong>aggregate summary of file lengths</strong> being displayed, rather than the individual files. Without the -s option, the calculation is done by going 1-level deep from the given path.</li>
<li>The <strong>-h</strong> option will format file sizes in a <strong>human-readable</strong> fashion (e.g 64.0m instead of 67108864)</li>
<li>The <strong>-v</strong> option will display <strong>the names of columns</strong> as a header line.</li>
<li>The <strong>-x</strong> option will <strong>exclude snapshots</strong> from the result calculation. Without the -x option (default), the result is always calculated from all INodes, including all snapshots under the given path.</li>
</ul>
<h2 id="hadoop-fs-count-h-x2F-dir-x2F"><a href="#hadoop-fs-count-h-x2F-dir-x2F" class="headerlink" title="hadoop fs -count -h &#x2F;dir&#x2F;*"></a>hadoop fs -count -h &#x2F;dir&#x2F;*</h2><p>显示文件夹下的所有文件数、大小</p>
<h1 id="web-ui"><a href="#web-ui" class="headerlink" title="web ui"></a>web ui</h1><p><a href="https://ambari.apache.org/1.2.3/installing-hadoop-using-ambari/content/reference_chap2_1.html">hdfs default ports</a> are changed. see <a href="https://issues.apache.org/jira/browse/HDFS-9427">here</a></p>
<blockquote>
<p>Namenode ports: 50470 –&gt; 9871, 50070 –&gt; 9870, 8020 –&gt; 9820<br>Secondary NN ports: 50091 –&gt; 9869, 50090 –&gt; 9868<br>Datanode ports: 50020 –&gt; 9867, 50010 –&gt; 9866, 50475 –&gt; 9865, 50075 –&gt; 9864</p>
</blockquote>
<h2 id="local-storage"><a href="#local-storage" class="headerlink" title="local storage"></a>local storage</h2><p>From localhost:9870, you can get the namenode information. To see the data you created locally:</p>
<ol>
<li>Login localhost:9870, <strong>get the ‘<em>configuration</em>‘ from the ‘<em>utilities</em>‘</strong></li>
<li>Find <code>dfs.datanode.data.dir</code>  to get the data location</li>
</ol>
<h2 id="Issue-Permission-denied-user-x3D-dr-who"><a href="#Issue-Permission-denied-user-x3D-dr-who" class="headerlink" title="Issue: Permission denied: user&#x3D;dr.who"></a>Issue: Permission denied: user&#x3D;dr.who</h2><p>When ‘<em>browse the file system</em>‘ from ‘<em>utilities</em>‘, there are some dirs (e.g. <code>/tmp</code>) you have no permission to access. It may show:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Permission denied: user=dr.who, access=READ_EXECUTE, inode=&quot;/tmp&quot;:cherish:supergroup:drwx------</span><br></pre></td></tr></table></figure>

<p>The ‘<em>dr.who</em>‘ is just a configured static user in <code>core-default.xml</code>:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop.http.staticuser.user</span>=<span class="string">dr.who</span></span><br></pre></td></tr></table></figure>

<p>And there is permission check because it’s set to check by default in <code>hdfs-default.xml</code>:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dfs.permissions.enabled</span>=<span class="string">true </span></span><br></pre></td></tr></table></figure>

<p>There are three ways to solve it:</p>
<h3 id="solutions"><a href="#solutions" class="headerlink" title="solutions"></a>solutions</h3><h4 id="disable-the-permission-check"><a href="#disable-the-permission-check" class="headerlink" title="disable the permission check"></a>disable the permission check</h4><blockquote>
<p>This is not recommended in the prod mode.</p>
</blockquote>
<p>Add the following property in <code>hdfs-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="change-the-staticuser"><a href="#change-the-staticuser" class="headerlink" title="change the staticuser"></a>change the staticuser</h4><p>Add the following property in <code>core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cherish<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="modify-the-file-permission"><a href="#modify-the-file-permission" class="headerlink" title="modify the file permission"></a>modify the file permission</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -<span class="built_in">chmod</span> -R 755 /tmp</span><br></pre></td></tr></table></figure>

<h1 id="Replica"><a href="#Replica" class="headerlink" title="Replica"></a>Replica</h1>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hfcherish.github.io/2019/01/03/hive-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cherish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cherish's Blog">
      <meta itemprop="description" content="从心所欲不逾矩">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Cherish's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/01/03/hive-introduction/" class="post-title-link" itemprop="url">hive introduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-03 14:50:02" itemprop="dateCreated datePublished" datetime="2019-01-03T14:50:02+08:00">2019-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-10-22 14:22:52" itemprop="dateModified" datetime="2022-10-22T14:22:52+08:00">2022-10-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a href="https://cwiki.apache.org/confluence/display/Hive/Home#Home-HiveDocumentation">apache hive</a> 是一个 data warehouse 应用。支持分布式存储的大数据读、写和管理，并且支持使用标准的 SQL 语法查询。Hive is not a database.  This is to make use of SQL capabilities by defining a metadata to the files in HDFS.  Long story short, it brings the possibility to query the hdfs file.</p>
<p>hive 并没有固定的数据存储方式。自带的是 csv（comma-separated value）和 tsv (tab-separated values) connectors，也可以使用 connector for other formats。</p>
<h2 id="database-v-s-warehouse"><a href="#database-v-s-warehouse" class="headerlink" title="database v.s. warehouse"></a>database v.s. warehouse</h2><p>参见 <a href="https://panoply.io/data-warehouse-guide/the-difference-between-a-database-and-a-data-warehouse/">the difference between database and data warehouse</a></p>
<h3 id="database："><a href="#database：" class="headerlink" title="database："></a>database：</h3><p>存储具体的业务数据，完善支持 concurrent transaction 操作（CRUD）。</p>
<p>database contains highly detailed data as well as a detailed relational views. Tables are normalized to achieve efficient storage, concurrent transaction processing, as well as return quick query results.</p>
<ul>
<li>**主要用于 OLTP (online trancaction processing)**。</li>
<li><strong>use a normalized structure</strong>. 即通常会组织成 table、row、column，冗余信息很少（比如三张表 product、color、product-color），所以节省空间。在查询时就需要通过复杂的 join 来实现，所以分析性的查询会比较耗时</li>
<li><strong>no historical data</strong>. 主要处理 transaction 数据，只保存现在的数据，进行的查询和分析也是基于现有数据。即它的分析是 static one-time reports</li>
<li><strong>optimization 主要是优化写速度、读速度</strong>。复杂分析因为涉及很多 join，其性能提升也是一个主要的问题。</li>
<li><strong>经常需要满足关系型数据库的 ACID 原则</strong>（atomicity, consistency, isolation, and durability）。所以它需要支持并发操作下的数据完整性。对 concurrent transaction 的支持要求比较高。</li>
</ul>
<h3 id="data-warehouse"><a href="#data-warehouse" class="headerlink" title="data warehouse"></a>data warehouse</h3><p>将企业中的各种数据收集起来，重新组织，对这些数据做高效 <em><strong>分析</strong></em></p>
<blockquote>
<p>A <a href="https://panoply.io/data-warehouse-guide">data warehouse</a> is a system that pulls together data from many different sources within an organization for reporting and analysis. The reports created from complex queries within a data warehouse are used to make business decisions.</p>
<p>The primary focus of a data warehouse is to provide a correlation between data from existing systems, i.e., product inventory stored in one system, purchase orders for a specific customer, stored in another system. Data warehouses are used for online analytical processing (OLAP), which uses complex queries to analyze rather than process transactions.</p>
</blockquote>
<ul>
<li><strong>主要用于 OLAP (online analysis processing)</strong>. 它收集企业内各个数据源的数据，建立数据关联，对这些数据做复杂的查询分析，以辅佐业务决策。</li>
<li><strong>use a denormalized structure</strong>. 它收集多个相关数据源的数据，将这些 table <a href="https://searchdatamanagement.techtarget.com/definition/denormalization">denormailize</a>、transform，获得 summarized data、multidimentional views，并基于这些数据实现快速分析和查询。它不在乎冗余，相反，很多时候正是通过冗余重新组织数据，使得查询更方便。</li>
<li><strong>store historical data</strong>. data warehouse 主要是用于分析的，所以通常会存储历史数据，以实现对历史数据和现有数据的对比分析。</li>
<li><strong>optimization 主要是查询响应速度</strong>。它对大数据做分析，响应速度是主要的衡量标准。</li>
<li><strong>一般不支持高并发操作</strong>。支持一定并发，但支持程度远不如 database</li>
</ul>
<h1 id="installation"><a href="#installation" class="headerlink" title="installation"></a>installation</h1><p>See <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">hadoop: setting up a single-node cluster</a>, <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">GettingStarted</a></p>
<p>Hive relies on hadoop. And we need a db (eg. mysql) to store hive metadata. So the prerequisites are:</p>
<ul>
<li><strong>hadoop installed</strong></li>
<li><strong>mysql installed</strong>: to store hive metadata</li>
<li><strong>java installed</strong>: ??</li>
<li><strong>ssh installed and sshd running</strong>: when running hadoop scripts and managing remote hadoop daemons, it use ssh to authenticate.</li>
</ul>
<h3 id="install-hadoop-on-mac"><a href="#install-hadoop-on-mac" class="headerlink" title="install hadoop on mac"></a><a href="https://hfcherish.github.io/2019/01/07/hadoop/">install hadoop on mac</a></h3><h3 id="install-hive-on-mac"><a href="#install-hive-on-mac" class="headerlink" title="install hive on mac"></a><a href="https://www.cnblogs.com/micrari/p/7067968.html">install hive on mac</a></h3><blockquote>
<p>After init mysql, you may find that you can’t connect mysql using ‘-uhive -pxxx’. Then try to grant privileges to <code>&#39;hive&#39;@&#39;%&#39;</code> instead of <code>&#39;hive&#39;@&#39;localhost&#39;</code>. Use wildcard <code>%</code> to match all hosts.</p>
</blockquote>
<p>After installation, can try the <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SimpleExampleUseCases">simple example</a> to see how to conduct analysis on hive.</p>
<h3 id="set-env"><a href="#set-env" class="headerlink" title="set env"></a>set env</h3><p>To use hadoop and hive conveniently, set the bin in Path. Just add the follow config into <code>~/.zshrc</code>, and then source it <code>source ~/.zshrc</code>.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/Cellar/hadoop/3.1.1/libexec</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/local/Cellar/hive/3.1.1/libexec</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure>



<h3 id="running-using-beeline"><a href="#running-using-beeline" class="headerlink" title="running using beeline"></a><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHiveServer2andBeeline.1">running using beeline</a></h3><p>beeline is a new hive client to replace the deprecated HiveCli. With beeline, you can execute write, load, query, etc. on hive.</p>
<p>To connect simply, type the following:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hiveserver2</span><br><span class="line">$ beeline -u jdbc:hive2://</span><br></pre></td></tr></table></figure>

<p>To create, alter database&#x2F;table&#x2F;column&#x2F;etc. on hive, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">Hive Data Definition Language</a>.</p>
<p>To get the query commands, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">LanguageManual Select</a></p>
<p>To load data from file, insert, delete, merge, update data, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML">DML (data manipulation language)</a></p>
<p>Other non-sql commands to use in HiveQL or beeline, see <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Commands">LanguageManual Commands</a>. The <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources">Hive Resources</a> related commands are non-sql commands.</p>
<h1 id="Configure-Hive"><a href="#Configure-Hive" class="headerlink" title="Configure Hive"></a>Configure Hive</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration#AdminManualConfiguration-ConfiguringHive">how to configure hive properties</a></p>
<p>To show hive config in hive cli: (<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ShowConf">show conf</a>)</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to show current: `set confName`</span></span><br><span class="line">0: jdbc:hive2://slave1:2181,slave2:2181,maste&gt; <span class="built_in">set</span> hive.fetch.task.conversion;</span><br></pre></td></tr></table></figure>



<p>There are two hive-site.xml files. See <a href="https://community.cloudera.com/t5/Support-Questions/Why-do-I-have-two-hive-site-xml-config-files-on-my-HDP-host/td-p/209500">two hive-site.xml config files on HDP</a></p>
<ul>
<li><p>&#x2F;etc&#x2F;hive&#x2F;conf&#x2F;hive-site.xml is the config for Hive service itself and is managed via Ambari through the Hive service config page.</p>
</li>
<li><p>&#x2F;usr&#x2F;hdp&#x2F;current&#x2F;spark-client&#x2F;conf&#x2F;hive-site.xml actually points to &#x2F;etc&#x2F;spark&#x2F;conf&#x2F;hive-site.xml . This is the minimal hive config that Spark needs to access Hive. This is managed via Ambari through the Spark service config page. Ambari correctly configures this hive site for Kerberos. Depending upon your version of HDP you may not have the correct support in Ambari for configuring Livy.  The hive-site.xml in Spark doesn’t have the same template as Hive’s. Ambari will notice the hive-site.xml and overwrite it in the Spark directory whenever Spark is restarted.</p>
</li>
</ul>
<h1 id="analysis-on-hive"><a href="#analysis-on-hive" class="headerlink" title="analysis on hive"></a>analysis on hive</h1><p>When you start a sql function (eg. <code>select count(*) from xxx</code>), it in fact  starts an map-reduce job based on hadoop to search among all datanodes. Such functions are simple analysis implemented by hive.</p>
<blockquote>
<p>Hive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable:</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapred.job.tracker</span><br></pre></td></tr></table></figure>

<p>For complex analysis, you may need to write custom mappers (map data) &amp; reducers (collect data) scripts. Use<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform"><code>TRANSFORM</code></a> keyword in hive to achieve this.</p>
<p>For example, the <code>weekday_mapper.py</code> to convert <code>unixtime</code> to <code>weekday</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">  line = line.strip()</span><br><span class="line">  userid, movieid, rating, unixtime = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">  weekday = datetime.datetime.fromtimestamp(<span class="built_in">float</span>(unixtime)).isoweekday()</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;\t&#x27;</span>.join([userid, movieid, rating, <span class="built_in">str</span>(weekday)])</span><br></pre></td></tr></table></figure>

<p>And then use the script:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> u_data_new (</span><br><span class="line">  userid <span class="type">INT</span>,</span><br><span class="line">  movieid <span class="type">INT</span>,</span><br><span class="line">  rating <span class="type">INT</span>,</span><br><span class="line">  weekday <span class="type">INT</span>)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">add</span> FILE weekday_mapper.py;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> u_data_new</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  TRANSFORM (userid, movieid, rating, unixtime)</span><br><span class="line">  <span class="keyword">USING</span> <span class="string">&#x27;python weekday_mapper.py&#x27;</span></span><br><span class="line">  <span class="keyword">AS</span> (userid, movieid, rating, weekday)</span><br><span class="line"><span class="keyword">FROM</span> u_data;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> weekday, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> u_data_new</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> weekday;</span><br></pre></td></tr></table></figure>

<h1 id="storage-on-hive"><a href="#storage-on-hive" class="headerlink" title="storage on hive"></a>storage on hive</h1><p>Hive relies on Hadoop. The data in hive is saved in hdfs in fact. And the metadata is saved in mysql (the db can be configured). When check <code>localhost:9870</code>, you can see a new folder <code>/user/hive/warehouse</code>. All tables in hive are dirs in <code>/user/hive/warehouse</code>.</p>
<h2 id="Hive-Partition"><a href="#Hive-Partition" class="headerlink" title="Hive Partition"></a>Hive Partition</h2><p><a href="https://blog.csdn.net/helloxiaozhe/article/details/78445276">hive中简单介绍分区表(partition table)，含动态分区(dynamic partition)与静态分区(static partition)</a></p>
<blockquote>
<p>Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.<br>Tables or partitions are sub-divided into <strong>buckets,</strong> to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table.<br>For example, a table named <strong>Tab1</strong> contains employee data such as id, name, dept, and yoj (i.e., year of joining). Suppose you need to retrieve the details of all employees who joined in 2012. A query searches the whole table for the required information. However, if you partition the employee data with the year and store it in a separate file, it reduces the query processing time. The following example shows how to partition a file and its data:</p>
</blockquote>
<h2 id="Hive-bucket"><a href="#Hive-bucket" class="headerlink" title="Hive bucket"></a>Hive bucket</h2><p><a href="https://sparkbyexamples.com/apache-hive/hive-partitioning-vs-bucketing-with-examples/">hive partitioning vs bucket with examples</a></p>
<p><a href="https://sparkbyexamples.com/apache-hive/hive-partitioning-vs-bucketing-with-examples/">stack-overflow: hive partition vs bucket</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> zipcodes(</span><br><span class="line">RecordNumber <span class="type">int</span>,</span><br><span class="line">Country string,</span><br><span class="line">City string,</span><br><span class="line">Zipcode <span class="type">int</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(state string)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> Zipcode <span class="keyword">INTO</span> <span class="number">10</span> BUCKETS</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="left">PARTITIONING</th>
<th align="left">BUCKETING</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Directory is created on HDFS for each partition.</td>
<td align="left">File is created on HDFS for each bucket.</td>
</tr>
<tr>
<td align="left">You can have one or more Partition columns</td>
<td align="left">You can have only one Bucketing column</td>
</tr>
<tr>
<td align="left">You can’t manage the number of partitions to create</td>
<td align="left">You can manage the number of buckets to create by specifying the count</td>
</tr>
<tr>
<td align="left">NA</td>
<td align="left">Bucketing can be created on a partitioned table</td>
</tr>
<tr>
<td align="left">Uses PARTITIONED BY</td>
<td align="left">Uses CLUSTERED BY</td>
</tr>
</tbody></table>
<p>partition  和 bucket 都是将大数据集拆成更小的数据集，加速查询处理的方式。比如按日期拆分区，很多分析只拿当天的分区，处理的数据量、读取的 hdfs 文件很少，就快。</p>
<p>最大的区别是 partition 拆数据就是按 column 值拆，bucket 拆数据是按 column hash 值拆，所以 bucket 最终的桶的数目是固定的，同时一个桶里可能有多个 column 值（parition 每个分区只会存一种 column 的值）</p>
<p>相对来讲，bucket 粒度可能更细。比如一个场景，我们将 order 按 date 分区，分区后每天的数据量还是特别大，如果我们很多查询&#x2F;join是基于 employee，此时可以基于 employe_id 再分成更多的小集合，即按 employe_id 字段 hash 到 n 个桶里，这种拆桶方式特别有利于宏宇今天说的 map-side join，而且相比 partition，可以控制文件数量（有时想用的 partition 字段可能会分成特别特别多小分区，这个时候 bucket 就更合适些）</p>
<p>上边那个例子，假如 order 按 date+employee_id partition，分区就会特别多（对 hdfs namenode 造成大压力，hive metadata 也有压力），所以按 date partition, 按 employee_id bucket 就比较合适</p>
<h2 id="ORC-vs-Parquet"><a href="#ORC-vs-Parquet" class="headerlink" title="ORC vs Parquet"></a>ORC vs Parquet</h2><p><a href="https://community.cloudera.com/t5/Support-Questions/ORC-vs-Parquet-When-to-use-one-over-the-other/td-p/95942">orc vs Parquet</a></p>
<p><a href="https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/">ORCFile in HDP 2: Better Compression, Better Performance</a></p>
<h1 id="Hive-Transactional"><a href="#Hive-Transactional" class="headerlink" title="Hive Transactional"></a>Hive Transactional</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-NewConfigurationParametersforTransactions">hive transaction</a></p>
<p>Close:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set hive.support.concurrency = false;</span><br><span class="line">set hive.optimize.index.filter = false;</span><br><span class="line">set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;</span><br><span class="line">set hive.compactor.initiator.on = false;</span><br><span class="line">set hive.compactor.worker.threads = 0;</span><br><span class="line">set hive.strict.managed.tables = false;</span><br><span class="line"></span><br><span class="line">TBLPROPERTIES (&#x27;transactional&#x27;=&#x27;false&#x27;)</span><br></pre></td></tr></table></figure>

<h1 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a><a href="https://cwiki.apache.org/confluence/display/Hive/Design">architecture</a></h1><p><img src="https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&modificationDate=1414560669000&api=v2" alt="hive architecture"></p>
<h1 id="Hive-Data-Types"><a href="#Hive-Data-Types" class="headerlink" title="Hive Data Types"></a>Hive Data Types</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-decimal">hive data types</a></p>
<h1 id="Common-used-commands"><a href="#Common-used-commands" class="headerlink" title="Common used commands"></a>Common used commands</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://slave1:2181&gt; use dbname;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; show tables;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; describe formatted tablename;</span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; describe extended tableName</span><br></pre></td></tr></table></figure>

<h2 id="auto-increment-id"><a href="#auto-increment-id" class="headerlink" title="auto increment id"></a>auto increment id</h2><p><a href="https://cloud.tencent.com/developer/article/1433240">two ways hive auto increment id</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">## use row_number</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl_dim  </span><br><span class="line"><span class="keyword">select</span> <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> tbl_stg.id) <span class="operator">+</span> t2.sk_max, tbl_stg.<span class="operator">*</span>  </span><br><span class="line"><span class="keyword">from</span> tbl_stg </span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> (<span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="built_in">max</span>(sk),<span class="number">0</span>) sk_max <span class="keyword">from</span> tbl_dim) t2; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## use UDFRowSequence</span><br><span class="line"><span class="keyword">add</span> jar hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">-</span>contrib<span class="number">-2.0</span><span class="number">.0</span>.jar;  </span><br><span class="line"><span class="keyword">create</span> temporary <span class="keyword">function</span> row_sequence <span class="keyword">as</span> <span class="string">&#x27;org.apache.hadoop.hive.contrib.udf.udfrowsequence&#x27;</span>; </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl_dim  </span><br><span class="line"><span class="keyword">select</span> row_sequence() <span class="operator">+</span> t2.sk_max, tbl_stg.<span class="operator">*</span>  </span><br><span class="line"><span class="keyword">from</span> tbl_stg </span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> (<span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="built_in">max</span>(sk),<span class="number">0</span>) sk_max <span class="keyword">from</span> tbl_dim) t2;</span><br></pre></td></tr></table></figure>

<h2 id="get-latest-partition"><a href="#get-latest-partition" class="headerlink" title="get latest partition"></a>get latest partition</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## will <span class="keyword">only</span> scan <span class="number">2</span><span class="number">-3</span> partitions</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">max</span>(ingest_date) <span class="keyword">from</span> db.table_name</span><br><span class="line"><span class="keyword">where</span> ingest_date<span class="operator">&gt;</span>date_add(<span class="built_in">current_date</span>,<span class="number">-3</span>)</span><br></pre></td></tr></table></figure>



<h2 id="create-table-from-another-table"><a href="#create-table-from-another-table" class="headerlink" title="create table from another table"></a>create table from another table</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> new_test </span><br><span class="line">    <span class="type">row</span> format delimited </span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;|&#x27;</span> </span><br><span class="line">    STORED <span class="keyword">AS</span> RCFile </span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> source <span class="keyword">where</span> col<span class="operator">=</span><span class="number">1</span></span><br></pre></td></tr></table></figure>

<h2 id="select-all-without-some-columns"><a href="#select-all-without-some-columns" class="headerlink" title="select all without some columns"></a>select all without some columns</h2><p><a href="https://blog.csdn.net/Kikitious_Du/article/details/84754240">blog</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.support.quoted.identifiers<span class="operator">=</span><span class="keyword">none</span>;</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">`(num<span class="operator">|</span>uid)?<span class="operator">+</span>.<span class="operator">+</span>`</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> </span><br><span class="line">    <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uid <span class="keyword">order</span> <span class="keyword">by</span> pay_time <span class="keyword">asc</span>) <span class="keyword">as</span> num</span><br><span class="line">    ,<span class="operator">*</span></span><br><span class="line">    <span class="keyword">from</span> <span class="keyword">order</span>) first_order</span><br><span class="line"><span class="keyword">where</span> num <span class="operator">=</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h2 id="select-latest-in-group"><a href="#select-latest-in-group" class="headerlink" title="select latest in group"></a>select latest in group</h2><p><a href="https://stackoverflow.com/questions/35520193/how-to-find-most-recent-records-for-every-group-in-hive">link</a></p>
<p>use rank</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> (</span><br><span class="line">  <span class="keyword">select</span> id, name, starttime, <span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> unix_timestamp(starttime, <span class="string">&#x27;EEE, dd MMM yyyy hh:mm:ss z&#x27;</span>) <span class="keyword">desc</span>) <span class="keyword">as</span> rnk <span class="keyword">from</span> hive_table) a </span><br><span class="line"> <span class="keyword">where</span> a.rnk<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure>



<h2 id="hive-cli-pretty"><a href="#hive-cli-pretty" class="headerlink" title="hive cli pretty"></a>hive cli pretty</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cli.print.header<span class="operator">=</span><span class="literal">true</span>; <span class="operator">/</span><span class="operator">/</span> 打印列名</span><br><span class="line"><span class="keyword">set</span> hive.cli.print.row.to.vertical<span class="operator">=</span><span class="literal">true</span>; <span class="operator">/</span><span class="operator">/</span> 开启行转列功能, 前提必须开启打印列名功能</span><br><span class="line"><span class="keyword">set</span> hive.cli.print.row.to.vertical.num<span class="operator">=</span><span class="number">1</span>; <span class="operator">/</span><span class="operator">/</span> 设置每行显示的列数</span><br></pre></td></tr></table></figure>



<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><h2 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h2><p>和 spark 的小文件问题一样，hive 的运算引擎（mapreduce 或 Tez），为了提高性能，最后都会采用多个 reducer 来写数据，这个时候就会有小文件。不同于 Spark，Hive 本身提供了多种措施来优化小文件存储，我们只需要设置就行</p>
<h3 id="1-使用-concatenate"><a href="#1-使用-concatenate" class="headerlink" title="1. 使用 concatenate"></a>1. 使用 concatenate</h3><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTable/PartitionConcatenate">hive concatenate</a> 主要针对 orc 和 rcfile 文件格式存储的文件，特别是 orc ，可以直接执行 stripe level 的 merge，省掉 deserialize 和 decode 的开销，很高效。（concatenate 可以执行多次，最终文件数量不会变化）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> (partition_key <span class="operator">=</span> <span class="string">&#x27;partition_value&#x27;</span> [, ...])] CONCATENATE;</span><br></pre></td></tr></table></figure>

<h3 id="2-使用一些配置，在写文件时，自动-merge"><a href="#2-使用一些配置，在写文件时，自动-merge" class="headerlink" title="2. 使用一些配置，在写文件时，自动 merge"></a>2. 使用一些配置，在写文件时，自动 merge</h3><p>输入时合并：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--  每个Map最大输入大小，决定合并后的文件数</span></span><br><span class="line"><span class="keyword">set</span>  mapred. max .split.size<span class="operator">=</span><span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并</span></span><br><span class="line"><span class="keyword">set</span>  mapred. min .split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并</span></span><br><span class="line"><span class="keyword">set</span>  mapred. min .split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 执行Map前进行小文件合并</span></span><br><span class="line"><span class="keyword">set</span>  hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; </span><br></pre></td></tr></table></figure>



<p>输出时合并：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- hive 输出时合并的配置参数</span></span><br><span class="line"><span class="comment">-- 在Map-only的任务结束时合并小文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 在Map-Reduce的任务结束时合并小文件, 默认 false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.tezfiles<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 合并文件的大小, 默认 256000000</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task<span class="operator">=</span><span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 当输出文件的平均大小小于该值时, 启动一个独立的map-reduce任务进行文件merge， 默认 16000000</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 当这个参数设置为true,orc文件进行stripe Level级别的合并,当设置为false,orc文件进行文件级别的合并。默认 true</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.orcfile.stripe.level<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>Hive在对结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值，触发合并的条件是：</p>
<p>根据查询类型不同，相应的mapfiles&#x2F;mapredfiles参数需要打开；</p>
<p>结果文件的平均大小需要大于avgsize参数的值。</p>
<h1 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h1><h2 id="count-return-0"><a href="#count-return-0" class="headerlink" title="count(*) return 0"></a>count(*) return 0</h2><p><a href="https://community.cloudera.com/t5/Support-Questions/hive-count-not-working/td-p/216889">hive count(*) not working</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-ExistingTables%E2%80%93ANALYZE">hive analyze</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以设，但不好</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; <span class="built_in">set</span> hive.fetch.task.conversion=none;</span><br><span class="line"><span class="comment"># 或者设</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; <span class="built_in">set</span> hive.compute.query.using.stats=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 推荐</span></span><br><span class="line">0: jdbc:hive2://slave1:2181&gt; analyze table t [partition p] compute statistics <span class="keyword">for</span> [columns c,...];</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Its better not to disturb the properties on the statistics usage like hive.compute.query.using.stats. It impacts the way the statistics are used in your query for performance optimization and execution plans. It has tremendous influence on execution plans, the statistics stored depends on the file format as well. Therefore definitely not a solution to change any property with regards to statistics.<br>The real reason for count not working correctly is the statistics not updated in the hive due to which it returns 0. When a table is created first, the statistics is written with no data rows. Thereafter any data append&#x2F;change happens hive requires to update this statistics in the metadata. Depending on the circumstances hive might not be updating this real time.<br>Therefore running the ANALYZE command recomputes this statistics to make this work correctly.</p>
</blockquote>
<h2 id="hive-not-recognizing-alias-names-in-select-part"><a href="#hive-not-recognizing-alias-names-in-select-part" class="headerlink" title="hive not recognizing alias names in select part"></a>hive not recognizing alias names in select part</h2><p>The where clause is evaluated before the select clause, which is why you can’t refer to select aliases in your where clause.</p>
<p>You can however refer to aliases from a derived table.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * from (</span><br><span class="line">  <span class="keyword">select</span> user as u1, url as u2 from rank_test</span><br><span class="line">) t1 <span class="built_in">where</span> u1 &lt;&gt; <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * from (</span><br><span class="line">  <span class="keyword">select</span> user, count(*) as cnt from rank_test group by user</span><br><span class="line">) t1 <span class="built_in">where</span> cnt &gt;= 2;</span><br></pre></td></tr></table></figure>

<p>Side note: a more efficient way to write the last query would be</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> user, count(*) as cnt from rank_test group by user</span><br><span class="line">having count(*) &gt;= 2</span><br></pre></td></tr></table></figure>

<h2 id="In-not-in-substitution"><a href="#In-not-in-substitution" class="headerlink" title="In, not in substitution"></a>In, not in substitution</h2><p>Hive supports sub-query in <code>in</code> , <code>not in</code> only after 0.13. And <code>in</code> may be slow, so we can replace it with join.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">where</span> id <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b)</span><br><span class="line"><span class="comment">-- in substitutionn</span></span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b) b1 <span class="keyword">on</span> a.id <span class="operator">=</span> b1.id</span><br></pre></td></tr></table></figure>

<h2 id="VERTEX-FAILURE"><a href="#VERTEX-FAILURE" class="headerlink" title="VERTEX_FAILURE"></a>VERTEX_FAILURE</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions=8000;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions.pernode=8000;</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> hive.tez.log.level=DEBUG;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span> <span class="built_in">sum</span>(id) <span class="keyword">from</span> my;</span><br></pre></td></tr></table></figure>

<h2 id="xa0"><a href="#xa0" class="headerlink" title="\xa0"></a>\xa0</h2><p>(<code>SPACE_SEPARATOR</code>, <code>LINE_SEPARATOR</code>, or <code>PARAGRAPH_SEPARATOR</code>) but is not also a non-breaking space (<code>&#39;\u00A0&#39;</code>, <code>&#39;\u2007&#39;</code>, <code>&#39;\u202F&#39;</code>).</p>
<p><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Character.html#isWhitespace-char-">java isWhiteSpace()</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(res.selectExpr(&quot;trim(translate(mobile1, &#x27;\u00A0&#x27;, &#x27; &#x27;))&quot;).collect())</span><br><span class="line">print(res.selectExpr(&quot;trim(regexp_replace(mobile1, &#x27;\u00A0|\u2007|\u202F&#x27;, &#x27; &#x27;))&quot;).collect())</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cherish</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">429k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">6:30</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>
<script class="next-config" data-name="chatra" type="application/json">{"enable":true,"async":true,"id":null}</script>
<script src="/js/third-party/chat/chatra.js"></script>
<script async src="https://call.chatra.io/chatra.js"></script>






  





</body>
</html>
